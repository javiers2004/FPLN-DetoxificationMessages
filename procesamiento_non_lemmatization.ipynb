{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9399e5ee",
   "metadata": {},
   "source": [
    "### **1. Importar las librerias empleadas**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28c3b2f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T18:38:34.731436Z",
     "start_time": "2025-10-28T18:38:28.997727Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\js834\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset # Para cargar los datasets de Hugging Face\n",
    "import nltk # Para importar el WordNetLemmatizer y la función word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string   # Para la eliminación de signos de puntuación en el procesamiento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baec828",
   "metadata": {},
   "source": [
    "### **2. Cargar los datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d251a8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T18:38:42.974257Z",
     "start_time": "2025-10-28T18:38:34.743935Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\js834\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\js834\\.cache\\huggingface\\hub\\datasets--s-nlp--paradetox. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 19744/19744 [00:00<00:00, 296751.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Se cargan cada uno de los datasets de Hugging Face que se van a usar\n",
    "paradetox = load_dataset(\"textdetox/multilingual_paradetox\")\n",
    "multilingual_toxicity = load_dataset(\"textdetox/multilingual_toxicity_dataset\")\n",
    "toxic_keywords = load_dataset(\"textdetox/multilingual_toxic_lexicon\")\n",
    "toxic_spans = load_dataset(\"textdetox/multilingual_toxic_spans\")\n",
    "paradetox_test_set = load_dataset(\"textdetox/multilingual_paradetox_test\")\n",
    "paradetox2 = load_dataset(\"s-nlp/paradetox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "414d21ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T18:38:43.220912Z",
     "start_time": "2025-10-28T18:38:43.217150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    en: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    ru: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    uk: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    de: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    es: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    am: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    zh: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    ar: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    hi: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    en: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    ru: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    uk: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    de: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    es: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    am: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    zh: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    ar: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    hi: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    it: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    fr: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    he: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 2011\n",
      "    })\n",
      "    hin: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 4363\n",
      "    })\n",
      "    tt: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    ja: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    am: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 245\n",
      "    })\n",
      "    es: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1195\n",
      "    })\n",
      "    ru: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 140517\n",
      "    })\n",
      "    uk: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 7356\n",
      "    })\n",
      "    en: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3386\n",
      "    })\n",
      "    zh: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3839\n",
      "    })\n",
      "    ar: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 430\n",
      "    })\n",
      "    hi: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 133\n",
      "    })\n",
      "    de: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 247\n",
      "    })\n",
      "    it: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 815\n",
      "    })\n",
      "    fr: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1287\n",
      "    })\n",
      "    he: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 731\n",
      "    })\n",
      "    hin: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 209\n",
      "    })\n",
      "    tt: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 15629\n",
      "    })\n",
      "    ja: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 328\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    en: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 991\n",
      "    })\n",
      "    es: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 987\n",
      "    })\n",
      "    de: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 970\n",
      "    })\n",
      "    zh: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 921\n",
      "    })\n",
      "    ar: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 990\n",
      "    })\n",
      "    am: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 995\n",
      "    })\n",
      "    ru: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 999\n",
      "    })\n",
      "    uk: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 943\n",
      "    })\n",
      "    hi: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 992\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    uk: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    hi: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    zh: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    ar: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    de: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    en: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    ru: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    am: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    es: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    it: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    fr: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    he: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    hin: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    tt: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    ja: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['en_toxic_comment', 'en_neutral_comment'],\n",
      "        num_rows: 19744\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#Para ver que hay en cada dataset (diferentes idiomas y diferentes columnas)\n",
    "print(paradetox)\n",
    "print(multilingual_toxicity)\n",
    "print(toxic_keywords)\n",
    "print(toxic_spans)\n",
    "print(paradetox_test_set)\n",
    "print(paradetox2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdc7b43",
   "metadata": {},
   "source": [
    "### **3. Extraemos los datos solo en inglés [\"en\"]***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b42a26c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T18:38:43.288372Z",
     "start_time": "2025-10-28T18:38:43.283724Z"
    }
   },
   "outputs": [],
   "source": [
    "# Para seleccionar solo los datos en inglés\n",
    "paradetox_en = paradetox[\"en\"]\n",
    "multilingual_toxicity_en = multilingual_toxicity[\"en\"]\n",
    "toxic_keywords_en = toxic_keywords[\"en\"]\n",
    "toxic_spans_en = toxic_spans[\"en\"]\n",
    "paradetox_test_set_en = paradetox_test_set[\"en\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49cb3fa",
   "metadata": {},
   "source": [
    "Proximos pasos:\n",
    "- Eliminar valores nulos\n",
    "- Aplicar case folding\n",
    "- Aplicar tokenization\n",
    "- Eliminar stop words y signos de puntuación\n",
    "- Aplicar lemmatization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9389d0",
   "metadata": {},
   "source": [
    "### **4. Quitar valores Nulos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68fd4560",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 14617.10 examples/s]\n",
      "Filter: 100%|██████████| 5000/5000 [00:00<00:00, 91433.75 examples/s]\n",
      "Filter: 100%|██████████| 3386/3386 [00:00<00:00, 104248.00 examples/s]\n",
      "Filter: 100%|██████████| 990/990 [00:00<00:00, 38006.49 examples/s]\n",
      "Filter: 100%|██████████| 600/600 [00:00<00:00, 49431.99 examples/s]\n",
      "Filter: 100%|██████████| 19744/19744 [00:00<00:00, 208942.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "paradetox_en = paradetox_en.filter(lambda x: all(v is not None for v in x.values()))\n",
    "multilingual_toxicity_en = multilingual_toxicity_en.filter(lambda x: all(v is not None for v in x.values()))\n",
    "toxic_keywords_en = toxic_keywords_en.filter(lambda x: all(v is not None for v in x.values()))\n",
    "toxic_spans_en = toxic_spans_en.filter(lambda x: all(v is not None for v in x.values()))\n",
    "paradetox_test_set_en = paradetox_test_set_en.filter(lambda x: all(v is not None for v in x.values()))\n",
    "paradetox2_en = paradetox2[\"train\"].filter(lambda x: all(v is not None for v in x.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57b22b0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:51:19.942644Z",
     "start_time": "2025-10-23T15:51:19.901543Z"
    }
   },
   "outputs": [],
   "source": [
    "# Y transformamos los datos a pandas para facilitar el procesamiento\n",
    "english_paradetox_df = paradetox_en.to_pandas()\n",
    "english_multilingual_toxicity_df = multilingual_toxicity_en.to_pandas()\n",
    "english_toxic_keywords_df = toxic_keywords_en.to_pandas()\n",
    "english_toxic_spans_df = toxic_spans_en.to_pandas()\n",
    "english_paradetox_test_set_df = paradetox_test_set_en.to_pandas()\n",
    "english_paradetox2_df = paradetox2_en.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1256670",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:51:21.376303Z",
     "start_time": "2025-10-23T15:51:21.332810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 2 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   toxic_sentence    400 non-null    object\n",
      " 1   neutral_sentence  400 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 6.4+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    5000 non-null   object\n",
      " 1   toxic   5000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 78.3+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3386 entries, 0 to 3385\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    3386 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 26.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 990 entries, 0 to 989\n",
      "Data columns (total 2 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   Sentence               990 non-null    object\n",
      " 1   Negative Connotations  990 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 15.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 600 entries, 0 to 599\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    600 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 4.8+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19744 entries, 0 to 19743\n",
      "Data columns (total 2 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   en_toxic_comment    19744 non-null  object\n",
      " 1   en_neutral_comment  19744 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 308.6+ KB\n"
     ]
    }
   ],
   "source": [
    "english_paradetox_df.info()\n",
    "english_multilingual_toxicity_df.info()\n",
    "english_toxic_keywords_df.info()\n",
    "english_toxic_spans_df.info()\n",
    "english_paradetox_test_set_df.info()\n",
    "english_paradetox2_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20fa6ed",
   "metadata": {},
   "source": [
    "### **5. Descargar recursos necesarios para tokenization, lemmatization y eliminación de stop words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0db1431",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:52:35.626075Z",
     "start_time": "2025-10-23T15:52:35.575421Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\js834\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\js834\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\js834\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\js834\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('wordnet')    # Diccionario empleado para la lemmatization\n",
    "nltk.download('punkt')      # Modelo empleado para la tokenization\n",
    "nltk.download('averaged_perceptron_tagger')     # Modelo empleado para identificar el tipo de palabra\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()    #Se usará el WordNetLemmatizer de NLTK\n",
    "\n",
    "nltk.download('stopwords')  # Para descargar las stopwords en inglés\n",
    "stop_words_english = nltk.corpus.stopwords.words('english')     \n",
    "\n",
    "punctuation = set(string.punctuation)   # Para cargar signos de puntuación de la librería string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7cb7c3",
   "metadata": {},
   "source": [
    "### **6. Aplicar case folding en los datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07a147ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:52:38.360002Z",
     "start_time": "2025-10-23T15:52:38.350127Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1️ English Paradetox\n",
    "cols = ['toxic_sentence', 'neutral_sentence']\n",
    "for col in cols:\n",
    "    english_paradetox_df[col + '_lower'] = english_paradetox_df[col].str.lower()\n",
    "\n",
    "# 2️ English Multilingual Toxicity\n",
    "for col in cols:\n",
    "    english_multilingual_toxicity_df['text_lower'] = english_multilingual_toxicity_df['text'].str.lower()\n",
    "\n",
    "# 3️ English Toxic Keywords\n",
    "english_toxic_keywords_df['text_lower'] = english_toxic_keywords_df['text'].str.lower()\n",
    "\n",
    "# 4️ English Toxic Spans\n",
    "cols = ['Sentence', 'Negative Connotations']\n",
    "for col in cols:\n",
    "    english_toxic_spans_df[col + '_lower'] = english_toxic_spans_df[col].str.lower()\n",
    "\n",
    "# 5️ English Paradetox Test Set\n",
    "english_paradetox_test_set_df['text_lower'] = english_paradetox_test_set_df['text'].str.lower()\n",
    "\n",
    "paradetox2_cols = ['en_toxic_comment', 'en_neutral_comment']\n",
    "for col in paradetox2_cols:\n",
    "    english_paradetox2_df[col + '_lower'] = english_paradetox2_df[col].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165ae87f",
   "metadata": {},
   "source": [
    "### **7. Aplicar tokenization a las columnas de texto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "566b72b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:53:23.587864Z",
     "start_time": "2025-10-23T15:53:21.895794Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\js834\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "\n",
    "cols = ['toxic_sentence_lower', 'neutral_sentence_lower']\n",
    "for col in cols:\n",
    "    english_paradetox_df[col.replace('_lower','_tokens')] = english_paradetox_df[col].apply(word_tokenize)\n",
    "\n",
    "cols = ['text_lower']\n",
    "for col in cols:\n",
    "    english_multilingual_toxicity_df[col.replace('_lower','_tokens')] = english_multilingual_toxicity_df[col].apply(word_tokenize)              \n",
    "\n",
    "cols = ['text_lower']\n",
    "for col in cols:\n",
    "    english_toxic_keywords_df[col.replace('_lower','_tokens')] = english_toxic_keywords_df[col].apply(word_tokenize)\n",
    "\n",
    "cols = ['Sentence_lower', 'Negative Connotations_lower']\n",
    "for col in cols:\n",
    "    english_toxic_spans_df[col.replace('_lower','_tokens')] = english_toxic_spans_df[col].apply(word_tokenize)\n",
    "\n",
    "cols = ['text_lower']\n",
    "for col in cols:\n",
    "    english_paradetox_test_set_df[col.replace('_lower','_tokens')] = english_paradetox_test_set_df[col].apply(word_tokenize)\n",
    "\n",
    "cols = ['en_toxic_comment_lower', 'en_neutral_comment_lower']\n",
    "for col in cols:\n",
    "    english_paradetox2_df[col.replace('_lower','_tokens')] = english_paradetox2_df[col].apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c6825b",
   "metadata": {},
   "source": [
    "### **8. Seleccionar solo las columnas tokenizadas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5d8d71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:53:29.367465Z",
     "start_time": "2025-10-23T15:53:29.324215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 toxic_sentence_tokens  \\\n",
      "0    [then, all, of, a, sudden, i, see, her, ,, she...   \n",
      "1    [my, page, should, be, protected, first, so, t...   \n",
      "2                 [you, made, a, mistake, you, ass, .]   \n",
      "3    [you, know, more, than, these, idiots, ,, stay...   \n",
      "4    [piss, me, off, ,, fuckin, jerk, ,, get, on, m...   \n",
      "..                                                 ...   \n",
      "395  [at, least, one, party, gives, a, crap, about,...   \n",
      "396                 [parliament, is, a, lame, duck, .]   \n",
      "397  [side, -, note, ,, why, the, fuck, does, every...   \n",
      "398  [you, 're, not, being, helpful, ,, either, ,, ...   \n",
      "399  [prove, my, comment, wrong, and, post, a, vide...   \n",
      "\n",
      "                               neutral_sentence_tokens  \n",
      "0    [all, of, a, sudden, i, see, her, ,, she, is, ...  \n",
      "1    [my, page, should, be, protected, first, so, t...  \n",
      "2                           [you, made, a, mistake, .]  \n",
      "3    [you, know, more, than, these, people, ,, stay...  \n",
      "4                                [get, on, my, nerves]  \n",
      "..                                                 ...  \n",
      "395  [at, leas, one, party, pays, attention, about,...  \n",
      "396                             [parliament, is, dull]  \n",
      "397  [side-note, ,, why, does, everything, the, us,...  \n",
      "398      [you, 're, not, being, helpful, ,, either, .]  \n",
      "399  [prove, my, comment, wrong, and, post, a, vide...  \n",
      "\n",
      "[400 rows x 2 columns]\n",
      "                                            text_tokens  toxic\n",
      "0     [the, trans, women, reading, this, tweet, righ...      0\n",
      "1     [9, ), uhhhh, i, like, being, lgbt, a, lot, .,...      0\n",
      "2           [hero, rohit, sharma, love, from, pakistan]      0\n",
      "3     [as, a, slightly, feminine, top, i, appreciate...      0\n",
      "4     [delon, love, turkey, and, brave, turks, from,...      0\n",
      "...                                                 ...    ...\n",
      "4995  [id, rip, that, top, off, you, a, slap, and, p...      1\n",
      "4996  [i, hope, iran, nukes, the, cancer, usa, so, t...      1\n",
      "4997  [keep, those, black, low, lifes, out, our, cou...      1\n",
      "4998  [spank, your, ass, and, fill, your, slutty, pu...      1\n",
      "4999       [go, fuck, yourself, stupid, white, fuck, .]      1\n",
      "\n",
      "[5000 rows x 2 columns]\n",
      "         text_tokens\n",
      "0           [testes]\n",
      "1            [slopy]\n",
      "2        [upthebutt]\n",
      "3       [cuntsucker]\n",
      "4     [jelly, donut]\n",
      "...              ...\n",
      "3381     [bit, *, h]\n",
      "3382         [argie]\n",
      "3383      [pollocks]\n",
      "3384     [fagfucker]\n",
      "3385       [fagtard]\n",
      "\n",
      "[3386 rows x 1 columns]\n",
      "                                       Sentence_tokens  \\\n",
      "0    [mandated, and, right, fucking, now, would, be...   \n",
      "1    [>, i, wouldn, 't, care, how, ignorant, you, a...   \n",
      "2                   [6., are, you, fucking, insane, ?]   \n",
      "3    [after, all, its, hard, to, get, a, job, if, y...   \n",
      "4    [after, all, most, of, these, dorks, are, mill...   \n",
      "..                                                 ...   \n",
      "985  [at, least, one, party, gives, a, crap, about,...   \n",
      "986                 [parliament, is, a, lame, duck, .]   \n",
      "987  [side, -, note, ,, why, the, fuck, does, every...   \n",
      "988  [you, 're, not, being, helpful, ,, either, ,, ...   \n",
      "989  [prove, my, comment, wrong, and, post, a, vide...   \n",
      "\n",
      "           Negative Connotations_tokens  \n",
      "0                             [fucking]  \n",
      "1    [ignorant, ,, pretending, ,, shit]  \n",
      "2                  [fucking, ,, insane]  \n",
      "3                              [stupid]  \n",
      "4                               [dorks]  \n",
      "..                                  ...  \n",
      "985                              [crap]  \n",
      "986                        [lame, duck]  \n",
      "987                              [fuck]  \n",
      "988                         [tiny-dick]  \n",
      "989                           [fucking]  \n",
      "\n",
      "[990 rows x 2 columns]\n",
      "                                           text_tokens\n",
      "0    [mandated, and, ``, right, fucking, now, ``, w...\n",
      "1    [&, gt, i, wouldn, 't, care, how, ignorant, yo...\n",
      "2                   [6., are, you, fucking, insane, ?]\n",
      "3    [after, all, its, hard, to, get, a, job, if, y...\n",
      "4    [after, all, most, of, these, dorks, are, mill...\n",
      "..                                                 ...\n",
      "595  [what, do, you, mean, why, do, n't, you, keep,...\n",
      "596  [lets, not, edit, user, pages, and, look, like...\n",
      "597  [stalking, you, little, shit, you, need, to, s...\n",
      "598  [for, fuck, 's, sake, when, are, they, gon, na...\n",
      "599  [he, deserves, every, beating, and, rape, he, ...\n",
      "\n",
      "[600 rows x 1 columns]\n",
      "                                 en_toxic_comment_tokens  \\\n",
      "0                        [he, had, steel, balls, too, !]   \n",
      "1      [dude, should, have, been, taken, to, api, ,, ...   \n",
      "2      [im, not, gon, na, sell, the, fucking, picture...   \n",
      "3      [the, garbage, that, is, being, created, by, c...   \n",
      "4      [the, reason, they, dont, exist, is, because, ...   \n",
      "...                                                  ...   \n",
      "19739              [when, they, do, shit, like, this, .]   \n",
      "19740  [but, if, saying, ``, fuck, that, group, ``, i...   \n",
      "19741  [it, hurts, how, judgemental, assholes, view, ...   \n",
      "19742  [shit, we, probably, literally, blow, that, up...   \n",
      "19743  [if, anyone, deserved, it, ,, it, was, this, s...   \n",
      "\n",
      "                               en_neutral_comment_tokens  \n",
      "0                               [he, was, brave, too, !]  \n",
      "1      [it, would, have, been, good, if, he, went, to...  \n",
      "2      [i, 'm, not, gon, na, sell, the, picture, ,, i...  \n",
      "3      [the, news, that, is, being, created, by, cnn,...  \n",
      "4      [the, reason, they, do, n't, exist, is, becaus...  \n",
      "...                                                  ...  \n",
      "19739                [when, they, do, stuff, like, this]  \n",
      "19740  [but, if, saying, '', that, group, is, bad, ''...  \n",
      "19741  [it, hurts, how, judgemental, that, people, vi...  \n",
      "19742  [we, probably, litteralky, blow, that, up, in,...  \n",
      "19743  [if, anyone, deserved, it, ,, it, was, this, b...  \n",
      "\n",
      "[19744 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "english_paradetox_df = english_paradetox_df[['toxic_sentence_tokens', 'neutral_sentence_tokens']]\n",
    "english_multilingual_toxicity_df = english_multilingual_toxicity_df[['text_tokens', 'toxic']]\n",
    "english_toxic_keywords_df = english_toxic_keywords_df[['text_tokens']]\n",
    "english_toxic_spans_df = english_toxic_spans_df[['Sentence_tokens', 'Negative Connotations_tokens']]\n",
    "english_paradetox_test_set_df = english_paradetox_test_set_df[['text_tokens']]\n",
    "english_paradetox2_df = english_paradetox2_df[[\n",
    "    'en_toxic_comment',          # <--- Texto original (para T5)\n",
    "    'en_neutral_comment',        # <--- Texto original (para T5)\n",
    "    'en_toxic_comment_tokens',   # <--- Tokens (para modelos antiguos)\n",
    "    'en_neutral_comment_tokens'  # <--- Tokens (para modelos antiguos)\n",
    "]]\n",
    "\n",
    "print(english_paradetox_df)\n",
    "print(english_multilingual_toxicity_df)\n",
    "print(english_toxic_keywords_df)        \n",
    "print(english_toxic_spans_df)\n",
    "print(english_paradetox_test_set_df)\n",
    "print(english_paradetox2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8b64cc",
   "metadata": {},
   "source": [
    "### **9. Eliminar stop words y signos de puntuación**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "782b315c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:53:32.523714Z",
     "start_time": "2025-10-23T15:53:32.359243Z"
    }
   },
   "outputs": [],
   "source": [
    "#Se eliminan las stop words (sacadas de nltk.corpus.stopwords) y signos de puntuación(sacados de string.punctuation))\n",
    "english_paradetox_df['toxic_sentence_tokens'] = english_paradetox_df['toxic_sentence_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")\n",
    "english_paradetox_df['neutral_sentence_tokens'] = english_paradetox_df['neutral_sentence_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")\n",
    "english_multilingual_toxicity_df['text_tokens'] = english_multilingual_toxicity_df['text_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")\n",
    "english_toxic_keywords_df['text_tokens'] = english_toxic_keywords_df['text_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")   \n",
    "english_toxic_spans_df['Sentence_tokens'] = english_toxic_spans_df['Sentence_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")\n",
    "english_toxic_spans_df['Negative Connotations_tokens'] = english_toxic_spans_df['Negative Connotations_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")\n",
    "english_paradetox_test_set_df['text_tokens'] = english_paradetox_test_set_df['text_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")\n",
    "english_paradetox2_df['en_toxic_comment_tokens'] = english_paradetox2_df['en_toxic_comment_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")\n",
    "english_paradetox2_df['en_neutral_comment_tokens'] = english_paradetox2_df['en_neutral_comment_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c40db358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "#Para eliminar signos de puntuación de los bordes de cada token\n",
    "def strip_punctuation_from_list(token_list):\n",
    "\n",
    "    if not isinstance(token_list, list):\n",
    "            return [] \n",
    "\n",
    "    punctuation = string.punctuation\n",
    "    cleaned_list = []\n",
    "    for t in token_list:\n",
    "        clean_t = str(t).strip(punctuation) \n",
    "            \n",
    "        if clean_t:\n",
    "            cleaned_list.append(clean_t)\n",
    "                \n",
    "    return cleaned_list\n",
    "\n",
    "english_paradetox_df['toxic_sentence_tokens'] = english_paradetox_df['toxic_sentence_tokens'].apply(strip_punctuation_from_list)\n",
    "english_paradetox_df['neutral_sentence_tokens'] = english_paradetox_df['neutral_sentence_tokens'].apply(strip_punctuation_from_list)\n",
    "english_multilingual_toxicity_df['text_tokens'] = english_multilingual_toxicity_df['text_tokens'].apply(strip_punctuation_from_list)\n",
    "english_toxic_keywords_df['text_tokens'] = english_toxic_keywords_df['text_tokens'].apply(strip_punctuation_from_list)\n",
    "english_toxic_spans_df['Sentence_tokens'] = english_toxic_spans_df['Sentence_tokens'].apply(strip_punctuation_from_list)\n",
    "english_toxic_spans_df['Negative Connotations_tokens'] = english_toxic_spans_df['Negative Connotations_tokens'].apply(strip_punctuation_from_list)\n",
    "english_paradetox_test_set_df['text_tokens'] = english_paradetox_test_set_df['text_tokens'].apply(strip_punctuation_from_list)\n",
    "english_paradetox2_df['en_toxic_comment_tokens'] = english_paradetox2_df['en_toxic_comment_tokens'].apply(strip_punctuation_from_list)\n",
    "english_paradetox2_df['en_neutral_comment_tokens'] = english_paradetox2_df['en_neutral_comment_tokens'].apply(strip_punctuation_from_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f8fc7",
   "metadata": {},
   "source": [
    "### **10. Guardar los datos procesados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "681055e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:53:42.635850Z",
     "start_time": "2025-10-23T15:53:42.573031Z"
    }
   },
   "outputs": [],
   "source": [
    "#Se guardan en la carpeta /datos/\n",
    "english_paradetox_df.to_csv(\"datos/english_paradetox_preprocessed_non_lemmatized.csv\", index=False)\n",
    "english_multilingual_toxicity_df.to_csv(\"datos/english_multilingual_toxicity_preprocessed_non_lemmatized.csv\", index=False)\n",
    "english_toxic_keywords_df.to_csv(\"datos/english_toxic_keywords_preprocessed_non_lemmatized.csv\", index=False)\n",
    "english_toxic_spans_df.to_csv(\"datos/english_toxic_spans_preprocessed_non_lemmatized.csv\", index=False)\n",
    "english_paradetox_test_set_df.to_csv(\"datos/english_paradetox_test_set_preprocessed_non_lemmatized.csv\", index=False)\n",
    "english_paradetox2_df.to_csv(\"datos/english_paradetox2_preprocessed_non_lemmatized.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
