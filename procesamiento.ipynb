{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9399e5ee",
   "metadata": {},
   "source": [
    "### **1. Importar las librerias empleadas**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28c3b2f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:34:33.928279Z",
     "start_time": "2025-10-23T15:34:33.922749Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset # Para cargar los datasets de Hugging Face\n",
    "import nltk # Para importar el WordNetLemmatizer y la función word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string   # Para la eliminación de signos de puntuación en el procesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baec828",
   "metadata": {},
   "source": [
    "### **2. Cargar los datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d251a8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:36:19.049151Z",
     "start_time": "2025-10-23T15:35:14.611391Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating en split: 100%|██████████| 400/400 [00:00<00:00, 39551.18 examples/s]\n",
      "Generating ru split: 100%|██████████| 400/400 [00:00<00:00, 104759.39 examples/s]\n",
      "Generating uk split: 100%|██████████| 400/400 [00:00<00:00, 83626.84 examples/s]\n",
      "Generating de split: 100%|██████████| 400/400 [00:00<00:00, 140654.06 examples/s]\n",
      "Generating es split: 100%|██████████| 400/400 [00:00<00:00, 122488.25 examples/s]\n",
      "Generating am split: 100%|██████████| 400/400 [00:00<00:00, 177781.24 examples/s]\n",
      "Generating zh split: 100%|██████████| 400/400 [00:00<00:00, 162664.49 examples/s]\n",
      "Generating ar split: 100%|██████████| 400/400 [00:00<00:00, 114975.44 examples/s]\n",
      "Generating hi split: 100%|██████████| 400/400 [00:00<00:00, 76273.94 examples/s]\n",
      "Generating en split: 100%|██████████| 5000/5000 [00:00<00:00, 901884.49 examples/s]\n",
      "Generating ru split: 100%|██████████| 5000/5000 [00:00<00:00, 573008.01 examples/s]\n",
      "Generating uk split: 100%|██████████| 5000/5000 [00:00<00:00, 593169.85 examples/s]\n",
      "Generating de split: 100%|██████████| 5000/5000 [00:00<00:00, 625287.58 examples/s]\n",
      "Generating es split: 100%|██████████| 5000/5000 [00:00<00:00, 508968.06 examples/s]\n",
      "Generating am split: 100%|██████████| 5000/5000 [00:00<00:00, 576457.39 examples/s]\n",
      "Generating zh split: 100%|██████████| 5000/5000 [00:00<00:00, 912241.51 examples/s]\n",
      "Generating ar split: 100%|██████████| 5000/5000 [00:00<00:00, 632892.32 examples/s]\n",
      "Generating hi split: 100%|██████████| 5000/5000 [00:00<00:00, 518481.01 examples/s]\n",
      "Generating it split: 100%|██████████| 5000/5000 [00:00<00:00, 822122.39 examples/s]\n",
      "Generating fr split: 100%|██████████| 5000/5000 [00:00<00:00, 751398.07 examples/s]\n",
      "Generating he split: 100%|██████████| 2011/2011 [00:00<00:00, 454115.72 examples/s]\n",
      "Generating hin split: 100%|██████████| 4363/4363 [00:00<00:00, 711858.57 examples/s]\n",
      "Generating tt split: 100%|██████████| 5000/5000 [00:00<00:00, 591981.03 examples/s]\n",
      "Generating ja split: 100%|██████████| 5000/5000 [00:00<00:00, 840777.77 examples/s]\n",
      "Generating am split: 100%|██████████| 245/245 [00:00<00:00, 122904.49 examples/s]\n",
      "Generating es split: 100%|██████████| 1195/1195 [00:00<00:00, 372986.55 examples/s]\n",
      "Generating ru split: 100%|██████████| 140517/140517 [00:00<00:00, 3965757.26 examples/s]\n",
      "Generating uk split: 100%|██████████| 7356/7356 [00:00<00:00, 1174916.23 examples/s]\n",
      "Generating en split: 100%|██████████| 3386/3386 [00:00<00:00, 1216021.35 examples/s]\n",
      "Generating zh split: 100%|██████████| 3839/3839 [00:00<00:00, 856713.65 examples/s]\n",
      "Generating ar split: 100%|██████████| 430/430 [00:00<00:00, 142640.84 examples/s]\n",
      "Generating hi split: 100%|██████████| 133/133 [00:00<00:00, 45471.34 examples/s]\n",
      "Generating de split: 100%|██████████| 247/247 [00:00<00:00, 88995.20 examples/s]\n",
      "Generating it split: 100%|██████████| 815/815 [00:00<00:00, 367407.33 examples/s]\n",
      "Generating fr split: 100%|██████████| 1287/1287 [00:00<00:00, 498482.71 examples/s]\n",
      "Generating he split: 100%|██████████| 731/731 [00:00<00:00, 259415.87 examples/s]\n",
      "Generating hin split: 100%|██████████| 209/209 [00:00<00:00, 85256.71 examples/s]\n",
      "Generating tt split: 100%|██████████| 15629/15629 [00:00<00:00, 2174509.96 examples/s]\n",
      "Generating ja split: 100%|██████████| 328/328 [00:00<00:00, 145456.94 examples/s]\n",
      "Generating en split: 100%|██████████| 991/991 [00:00<00:00, 293148.69 examples/s]\n",
      "Generating es split: 100%|██████████| 987/987 [00:00<00:00, 218250.64 examples/s]\n",
      "Generating de split: 100%|██████████| 970/970 [00:00<00:00, 274082.11 examples/s]\n",
      "Generating zh split: 100%|██████████| 921/921 [00:00<00:00, 266631.28 examples/s]\n",
      "Generating ar split: 100%|██████████| 990/990 [00:00<00:00, 301551.27 examples/s]\n",
      "Generating am split: 100%|██████████| 995/995 [00:00<00:00, 294664.44 examples/s]\n",
      "Generating ru split: 100%|██████████| 999/999 [00:00<00:00, 186243.65 examples/s]\n",
      "Generating uk split: 100%|██████████| 943/943 [00:00<00:00, 320209.58 examples/s]\n",
      "Generating hi split: 100%|██████████| 992/992 [00:00<00:00, 273553.55 examples/s]\n",
      "Generating uk split: 100%|██████████| 600/600 [00:00<00:00, 201972.91 examples/s]\n",
      "Generating hi split: 100%|██████████| 600/600 [00:00<00:00, 206649.89 examples/s]\n",
      "Generating zh split: 100%|██████████| 600/600 [00:00<00:00, 263268.38 examples/s]\n",
      "Generating ar split: 100%|██████████| 600/600 [00:00<00:00, 170998.33 examples/s]\n",
      "Generating de split: 100%|██████████| 600/600 [00:00<00:00, 201989.12 examples/s]\n",
      "Generating en split: 100%|██████████| 600/600 [00:00<00:00, 248674.15 examples/s]\n",
      "Generating ru split: 100%|██████████| 600/600 [00:00<00:00, 129009.20 examples/s]\n",
      "Generating am split: 100%|██████████| 600/600 [00:00<00:00, 152086.93 examples/s]\n",
      "Generating es split: 100%|██████████| 600/600 [00:00<00:00, 181610.91 examples/s]\n",
      "Generating it split: 100%|██████████| 600/600 [00:00<00:00, 120566.40 examples/s]\n",
      "Generating fr split: 100%|██████████| 600/600 [00:00<00:00, 186579.36 examples/s]\n",
      "Generating he split: 100%|██████████| 600/600 [00:00<00:00, 189715.97 examples/s]\n",
      "Generating hin split: 100%|██████████| 600/600 [00:00<00:00, 254895.41 examples/s]\n",
      "Generating tt split: 100%|██████████| 600/600 [00:00<00:00, 148541.05 examples/s]\n",
      "Generating ja split: 100%|██████████| 600/600 [00:00<00:00, 184649.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Se cargan cada uno de los datasets de Hugging Face que se van a usar\n",
    "paradetox = load_dataset(\"textdetox/multilingual_paradetox\")\n",
    "multilingual_toxicity = load_dataset(\"textdetox/multilingual_toxicity_dataset\")\n",
    "toxic_keywords = load_dataset(\"textdetox/multilingual_toxic_lexicon\")\n",
    "toxic_spans = load_dataset(\"textdetox/multilingual_toxic_spans\")\n",
    "paradetox_test_set = load_dataset(\"textdetox/multilingual_paradetox_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "414d21ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:51:07.925565Z",
     "start_time": "2025-10-23T15:51:07.911161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    en: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    ru: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    uk: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    de: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    es: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    am: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    zh: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    ar: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    hi: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    en: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    ru: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    uk: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    de: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    es: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    am: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    zh: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    ar: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    hi: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    it: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    fr: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    he: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 2011\n",
      "    })\n",
      "    hin: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 4363\n",
      "    })\n",
      "    tt: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    ja: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    am: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 245\n",
      "    })\n",
      "    es: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1195\n",
      "    })\n",
      "    ru: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 140517\n",
      "    })\n",
      "    uk: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 7356\n",
      "    })\n",
      "    en: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3386\n",
      "    })\n",
      "    zh: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3839\n",
      "    })\n",
      "    ar: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 430\n",
      "    })\n",
      "    hi: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 133\n",
      "    })\n",
      "    de: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 247\n",
      "    })\n",
      "    it: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 815\n",
      "    })\n",
      "    fr: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1287\n",
      "    })\n",
      "    he: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 731\n",
      "    })\n",
      "    hin: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 209\n",
      "    })\n",
      "    tt: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 15629\n",
      "    })\n",
      "    ja: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 328\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    en: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 991\n",
      "    })\n",
      "    es: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 987\n",
      "    })\n",
      "    de: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 970\n",
      "    })\n",
      "    zh: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 921\n",
      "    })\n",
      "    ar: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 990\n",
      "    })\n",
      "    am: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 995\n",
      "    })\n",
      "    ru: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 999\n",
      "    })\n",
      "    uk: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 943\n",
      "    })\n",
      "    hi: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 992\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    uk: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    hi: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    zh: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    ar: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    de: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    en: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    ru: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    am: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    es: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    it: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    fr: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    he: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    hin: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    tt: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    ja: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#Para ver que hay en cada dataset (diferentes idiomas y diferentes columnas)\n",
    "print(paradetox)\n",
    "print(multilingual_toxicity)\n",
    "print(toxic_keywords)\n",
    "print(toxic_spans)\n",
    "print(paradetox_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdc7b43",
   "metadata": {},
   "source": [
    "### **3. Extraemos los datos solo en inglés [\"en\"]***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b42a26c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:51:14.514924Z",
     "start_time": "2025-10-23T15:51:14.511592Z"
    }
   },
   "outputs": [],
   "source": [
    "# Para seleccionar solo los datos en inglés\n",
    "paradetox_en = paradetox[\"en\"]\n",
    "multilingual_toxicity_en = multilingual_toxicity[\"en\"]\n",
    "toxic_keywords_en = toxic_keywords[\"en\"]\n",
    "toxic_spans_en = toxic_spans[\"en\"]\n",
    "paradetox_test_set_en = paradetox_test_set[\"en\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49cb3fa",
   "metadata": {},
   "source": [
    "Proximos pasos:\n",
    "- Eliminar valores nulos\n",
    "- Aplicar case folding\n",
    "- Aplicar tokenization\n",
    "- Eliminar stop words y signos de puntuación\n",
    "- Aplicar lemmatization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9389d0",
   "metadata": {},
   "source": [
    "### **4. Quitar valores Nulos***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68fd4560",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:51:17.648999Z",
     "start_time": "2025-10-23T15:51:17.557210Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 19721.89 examples/s]\n",
      "Filter: 100%|██████████| 5000/5000 [00:00<00:00, 263028.43 examples/s]\n",
      "Filter: 100%|██████████| 3386/3386 [00:00<00:00, 355982.29 examples/s]\n",
      "Filter: 100%|██████████| 991/991 [00:00<00:00, 158950.49 examples/s]\n",
      "Filter: 100%|██████████| 600/600 [00:00<00:00, 145408.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "paradetox_en = paradetox_en.filter(lambda x: all(v is not None for v in x.values()))\n",
    "multilingual_toxicity_en = multilingual_toxicity_en.filter(lambda x: all(v is not None for v in x.values()))\n",
    "toxic_keywords_en = toxic_keywords_en.filter(lambda x: all(v is not None for v in x.values()))\n",
    "toxic_spans_en = toxic_spans_en.filter(lambda x: all(v is not None for v in x.values()))\n",
    "paradetox_test_set_en = paradetox_test_set_en.filter(lambda x: all(v is not None for v in x.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57b22b0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:51:19.942644Z",
     "start_time": "2025-10-23T15:51:19.901543Z"
    }
   },
   "outputs": [],
   "source": [
    "# Y transformamos los datos a pandas para facilitar el procesamiento\n",
    "english_paradetox_df = paradetox_en.to_pandas()\n",
    "english_multilingual_toxicity_df = multilingual_toxicity_en.to_pandas()\n",
    "english_toxic_keywords_df = toxic_keywords_en.to_pandas()\n",
    "english_toxic_spans_df = toxic_spans_en.to_pandas()\n",
    "english_paradetox_test_set_df = paradetox_test_set_en.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1256670",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:51:21.376303Z",
     "start_time": "2025-10-23T15:51:21.332810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 2 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   toxic_sentence    400 non-null    object\n",
      " 1   neutral_sentence  400 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 6.4+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    5000 non-null   object\n",
      " 1   toxic   5000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 78.3+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3386 entries, 0 to 3385\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    3386 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 26.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 990 entries, 0 to 989\n",
      "Data columns (total 2 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   Sentence               990 non-null    object\n",
      " 1   Negative Connotations  990 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 15.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 600 entries, 0 to 599\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    600 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 4.8+ KB\n"
     ]
    }
   ],
   "source": [
    "english_paradetox_df.info()\n",
    "english_multilingual_toxicity_df.info()\n",
    "english_toxic_keywords_df.info()\n",
    "english_toxic_spans_df.info()\n",
    "english_paradetox_test_set_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20fa6ed",
   "metadata": {},
   "source": [
    "### **5. Descargar recursos necesarios para tokenization, lemmatization y eliminación de stop words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0db1431",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:52:35.626075Z",
     "start_time": "2025-10-23T15:52:35.575421Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/elenaa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/elenaa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/elenaa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/elenaa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('wordnet')    # Diccionario empleado para la lemmatization\n",
    "nltk.download('punkt')      # Modelo empleado para la tokenization\n",
    "nltk.download('averaged_perceptron_tagger')     # Modelo empleado para identificar el tipo de palabra\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()    #Se usará el WordNetLemmatizer de NLTK\n",
    "\n",
    "nltk.download('stopwords')  # Para descargar las stopwords en inglés\n",
    "stop_words_english = nltk.corpus.stopwords.words('english')     \n",
    "\n",
    "punctuation = set(string.punctuation)   # Para cargar signos de puntuación de la librería string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7cb7c3",
   "metadata": {},
   "source": [
    "### **6. Aplicar case folding en los datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07a147ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:52:38.360002Z",
     "start_time": "2025-10-23T15:52:38.350127Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1️ English Paradetox\n",
    "cols = ['toxic_sentence', 'neutral_sentence']\n",
    "for col in cols:\n",
    "    english_paradetox_df[col + '_lower'] = english_paradetox_df[col].str.lower()\n",
    "\n",
    "# 2️ English Multilingual Toxicity\n",
    "for col in cols:\n",
    "    english_multilingual_toxicity_df['text_lower'] = english_multilingual_toxicity_df['text'].str.lower()\n",
    "\n",
    "# 3️ English Toxic Keywords\n",
    "english_toxic_keywords_df['text_lower'] = english_toxic_keywords_df['text'].str.lower()\n",
    "\n",
    "# 4️ English Toxic Spans\n",
    "cols = ['Sentence', 'Negative Connotations']\n",
    "for col in cols:\n",
    "    english_toxic_spans_df[col + '_lower'] = english_toxic_spans_df[col].str.lower()\n",
    "\n",
    "# 5️ English Paradetox Test Set\n",
    "english_paradetox_test_set_df['text_lower'] = english_paradetox_test_set_df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165ae87f",
   "metadata": {},
   "source": [
    "### **7. Aplicar tokenization a las columnas de texto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "566b72b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:53:23.587864Z",
     "start_time": "2025-10-23T15:53:21.895794Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/elenaa/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "\n",
    "cols = ['toxic_sentence_lower', 'neutral_sentence_lower']\n",
    "for col in cols:\n",
    "    english_paradetox_df[col.replace('_lower','_tokens')] = english_paradetox_df[col].apply(word_tokenize)\n",
    "\n",
    "cols = ['text_lower']\n",
    "for col in cols:\n",
    "    english_multilingual_toxicity_df[col.replace('_lower','_tokens')] = english_multilingual_toxicity_df[col].apply(word_tokenize)              \n",
    "\n",
    "cols = ['text_lower']\n",
    "for col in cols:\n",
    "    english_toxic_keywords_df[col.replace('_lower','_tokens')] = english_toxic_keywords_df[col].apply(word_tokenize)\n",
    "\n",
    "cols = ['Sentence_lower', 'Negative Connotations_lower']\n",
    "for col in cols:\n",
    "    english_toxic_spans_df[col.replace('_lower','_tokens')] = english_toxic_spans_df[col].apply(word_tokenize)\n",
    "\n",
    "cols = ['text_lower']\n",
    "for col in cols:\n",
    "    english_paradetox_test_set_df[col.replace('_lower','_tokens')] = english_paradetox_test_set_df[col].apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c6825b",
   "metadata": {},
   "source": [
    "### **8. Seleccionar solo las columnas tokenizadas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a5d8d71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:53:29.367465Z",
     "start_time": "2025-10-23T15:53:29.324215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 toxic_sentence_tokens  \\\n",
      "0    [then, all, of, a, sudden, i, see, her, ,, she...   \n",
      "1    [my, page, should, be, protected, first, so, t...   \n",
      "2                 [you, made, a, mistake, you, ass, .]   \n",
      "3    [you, know, more, than, these, idiots, ,, stay...   \n",
      "4    [piss, me, off, ,, fuckin, jerk, ,, get, on, m...   \n",
      "..                                                 ...   \n",
      "395  [at, least, one, party, gives, a, crap, about,...   \n",
      "396                 [parliament, is, a, lame, duck, .]   \n",
      "397  [side, -, note, ,, why, the, fuck, does, every...   \n",
      "398  [you, 're, not, being, helpful, ,, either, ,, ...   \n",
      "399  [prove, my, comment, wrong, and, post, a, vide...   \n",
      "\n",
      "                               neutral_sentence_tokens  \n",
      "0    [all, of, a, sudden, i, see, her, ,, she, is, ...  \n",
      "1    [my, page, should, be, protected, first, so, t...  \n",
      "2                           [you, made, a, mistake, .]  \n",
      "3    [you, know, more, than, these, people, ,, stay...  \n",
      "4                                [get, on, my, nerves]  \n",
      "..                                                 ...  \n",
      "395  [at, leas, one, party, pays, attention, about,...  \n",
      "396                             [parliament, is, dull]  \n",
      "397  [side-note, ,, why, does, everything, the, us,...  \n",
      "398      [you, 're, not, being, helpful, ,, either, .]  \n",
      "399  [prove, my, comment, wrong, and, post, a, vide...  \n",
      "\n",
      "[400 rows x 2 columns]\n",
      "                                            text_tokens  toxic\n",
      "0     [the, trans, women, reading, this, tweet, righ...      0\n",
      "1     [9, ), uhhhh, i, like, being, lgbt, a, lot, .,...      0\n",
      "2           [hero, rohit, sharma, love, from, pakistan]      0\n",
      "3     [as, a, slightly, feminine, top, i, appreciate...      0\n",
      "4     [delon, love, turkey, and, brave, turks, from,...      0\n",
      "...                                                 ...    ...\n",
      "4995  [id, rip, that, top, off, you, a, slap, and, p...      1\n",
      "4996  [i, hope, iran, nukes, the, cancer, usa, so, t...      1\n",
      "4997  [keep, those, black, low, lifes, out, our, cou...      1\n",
      "4998  [spank, your, ass, and, fill, your, slutty, pu...      1\n",
      "4999       [go, fuck, yourself, stupid, white, fuck, .]      1\n",
      "\n",
      "[5000 rows x 2 columns]\n",
      "         text_tokens\n",
      "0           [testes]\n",
      "1            [slopy]\n",
      "2        [upthebutt]\n",
      "3       [cuntsucker]\n",
      "4     [jelly, donut]\n",
      "...              ...\n",
      "3381     [bit, *, h]\n",
      "3382         [argie]\n",
      "3383      [pollocks]\n",
      "3384     [fagfucker]\n",
      "3385       [fagtard]\n",
      "\n",
      "[3386 rows x 1 columns]\n",
      "                                       Sentence_tokens  \\\n",
      "0    [mandated, and, right, fucking, now, would, be...   \n",
      "1    [>, i, wouldn, 't, care, how, ignorant, you, a...   \n",
      "2                   [6., are, you, fucking, insane, ?]   \n",
      "3    [after, all, its, hard, to, get, a, job, if, y...   \n",
      "4    [after, all, most, of, these, dorks, are, mill...   \n",
      "..                                                 ...   \n",
      "985  [at, least, one, party, gives, a, crap, about,...   \n",
      "986                 [parliament, is, a, lame, duck, .]   \n",
      "987  [side, -, note, ,, why, the, fuck, does, every...   \n",
      "988  [you, 're, not, being, helpful, ,, either, ,, ...   \n",
      "989  [prove, my, comment, wrong, and, post, a, vide...   \n",
      "\n",
      "           Negative Connotations_tokens  \n",
      "0                             [fucking]  \n",
      "1    [ignorant, ,, pretending, ,, shit]  \n",
      "2                  [fucking, ,, insane]  \n",
      "3                              [stupid]  \n",
      "4                               [dorks]  \n",
      "..                                  ...  \n",
      "985                              [crap]  \n",
      "986                        [lame, duck]  \n",
      "987                              [fuck]  \n",
      "988                         [tiny-dick]  \n",
      "989                           [fucking]  \n",
      "\n",
      "[990 rows x 2 columns]\n",
      "                                           text_tokens\n",
      "0    [mandated, and, ``, right, fucking, now, ``, w...\n",
      "1    [&, gt, i, wouldn, 't, care, how, ignorant, yo...\n",
      "2                   [6., are, you, fucking, insane, ?]\n",
      "3    [after, all, its, hard, to, get, a, job, if, y...\n",
      "4    [after, all, most, of, these, dorks, are, mill...\n",
      "..                                                 ...\n",
      "595  [what, do, you, mean, why, do, n't, you, keep,...\n",
      "596  [lets, not, edit, user, pages, and, look, like...\n",
      "597  [stalking, you, little, shit, you, need, to, s...\n",
      "598  [for, fuck, 's, sake, when, are, they, gon, na...\n",
      "599  [he, deserves, every, beating, and, rape, he, ...\n",
      "\n",
      "[600 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "english_paradetox_df = english_paradetox_df[['toxic_sentence_tokens', 'neutral_sentence_tokens']]\n",
    "english_multilingual_toxicity_df = english_multilingual_toxicity_df[['text_tokens', 'toxic']]\n",
    "english_toxic_keywords_df = english_toxic_keywords_df[['text_tokens']]\n",
    "english_toxic_spans_df = english_toxic_spans_df[['Sentence_tokens', 'Negative Connotations_tokens']]\n",
    "english_paradetox_test_set_df = english_paradetox_test_set_df[['text_tokens']]\n",
    "\n",
    "print(english_paradetox_df)\n",
    "print(english_multilingual_toxicity_df)\n",
    "print(english_toxic_keywords_df)        \n",
    "print(english_toxic_spans_df)\n",
    "print(english_paradetox_test_set_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8b64cc",
   "metadata": {},
   "source": [
    "### **9. Eliminar stop words y signos de puntuación**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "782b315c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:53:32.523714Z",
     "start_time": "2025-10-23T15:53:32.359243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 toxic_sentence_tokens  \\\n",
      "0    [sudden, see, shes, got, big, phony, tits, eve...   \n",
      "1    [page, protected, first, worthless, scum, like...   \n",
      "2                                 [made, mistake, ass]   \n",
      "3         [know, idiots, stay, course, truth, prevail]   \n",
      "4                    [piss, fuckin, jerk, get, nerves]   \n",
      "..                                                 ...   \n",
      "395    [least, one, party, gives, crap, humans, birth]   \n",
      "396                           [parliament, lame, duck]   \n",
      "397  [side, note, fuck, everything, us, make, calle...   \n",
      "398                  ['re, helpful, either, tiny-dick]   \n",
      "399  [prove, comment, wrong, post, video, fucking, ...   \n",
      "\n",
      "                               neutral_sentence_tokens  \n",
      "0                                 [sudden, see, grown]  \n",
      "1    [page, protected, first, unpleasant, people, l...  \n",
      "2                                      [made, mistake]  \n",
      "3         [know, people, stay, course, truth, prevail]  \n",
      "4                                        [get, nerves]  \n",
      "..                                                 ...  \n",
      "395  [leas, one, party, pays, attention, humans, bi...  \n",
      "396                                 [parliament, dull]  \n",
      "397  [side-note, everything, us, make, called, ``, ...  \n",
      "398                             ['re, helpful, either]  \n",
      "399        [prove, comment, wrong, post, video, tough]  \n",
      "\n",
      "[400 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Se eliminan las stop words (sacadas de nltk.corpus.stopwords) y signos de puntuación(sacados de string.punctuation))\n",
    "english_paradetox_df['toxic_sentence_tokens'] = english_paradetox_df['toxic_sentence_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")\n",
    "english_paradetox_df['neutral_sentence_tokens'] = english_paradetox_df['neutral_sentence_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")\n",
    "english_multilingual_toxicity_df['text_tokens'] = english_multilingual_toxicity_df['text_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")\n",
    "english_toxic_keywords_df['text_tokens'] = english_toxic_keywords_df['text_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")   \n",
    "english_toxic_spans_df['Sentence_tokens'] = english_toxic_spans_df['Sentence_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")\n",
    "english_toxic_spans_df['Negative Connotations_tokens'] = english_toxic_spans_df['Negative Connotations_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")\n",
    "english_paradetox_test_set_df['text_tokens'] = english_paradetox_test_set_df['text_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")\n",
    "print(english_paradetox_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f527627d",
   "metadata": {},
   "source": [
    "### **10. Lemmatizar cada uno de los tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d9b7fbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:53:37.404451Z",
     "start_time": "2025-10-23T15:53:34.651827Z"
    }
   },
   "outputs": [],
   "source": [
    "# Función para lemmatizar listas de tokens (se usa el WordNetLemmatizer de NLTK importado previamente)\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "# Paradetox\n",
    "english_paradetox_df['toxic_sentence_tokens'] = english_paradetox_df['toxic_sentence_tokens'].apply(lemmatize_tokens)\n",
    "english_paradetox_df['neutral_sentence_tokens'] = english_paradetox_df['neutral_sentence_tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "# Multilingual Toxicity\n",
    "english_multilingual_toxicity_df['text_tokens'] = english_multilingual_toxicity_df['text_tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "# Toxic Keywords\n",
    "english_toxic_keywords_df['text_tokens'] = english_toxic_keywords_df['text_tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "# Toxic Spans\n",
    "english_toxic_spans_df['Sentence_tokens'] = english_toxic_spans_df['Sentence_tokens'].apply(lemmatize_tokens)\n",
    "english_toxic_spans_df['Negative Connotations_tokens'] = english_toxic_spans_df['Negative Connotations_tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "# Paradetox Test Set\n",
    "english_paradetox_test_set_df['text_tokens'] = english_paradetox_test_set_df['text_tokens'].apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f8fc7",
   "metadata": {},
   "source": [
    "### **11. Guardar los datos procesados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "681055e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T15:53:42.635850Z",
     "start_time": "2025-10-23T15:53:42.573031Z"
    }
   },
   "outputs": [],
   "source": [
    "#Se guardan en la carpeta /datos/\n",
    "english_paradetox_df.to_csv(\"datos/english_paradetox_preprocessed.csv\", index=False)\n",
    "english_multilingual_toxicity_df.to_csv(\"datos/english_multilingual_toxicity_preprocessed.csv\", index=False)\n",
    "english_toxic_keywords_df.to_csv(\"datos/english_toxic_keywords_preprocessed.csv\", index=False)\n",
    "english_toxic_spans_df.to_csv(\"datos/english_toxic_spans_preprocessed.csv\", index=False)\n",
    "english_paradetox_test_set_df.to_csv(\"datos/english_paradetox_test_set_preprocessed.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
