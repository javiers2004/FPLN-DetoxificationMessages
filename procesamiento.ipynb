{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9399e5ee",
   "metadata": {},
   "source": [
    "1. Importar las librerias empleadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28c3b2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\js834\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset # Para cargar los datasets de Hugging Face\n",
    "import nltk # Para importar el WordNetLemmatizer y la función word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string   # Para la eliminación de signos de puntuación en el procesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baec828",
   "metadata": {},
   "source": [
    "2. Cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d251a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cargan cada uno de los datasets de Hugging Face que se van a usar\n",
    "paradetox = load_dataset(\"textdetox/multilingual_paradetox\")\n",
    "multilingual_toxicity = load_dataset(\"textdetox/multilingual_toxicity_dataset\")\n",
    "toxic_keywords = load_dataset(\"textdetox/multilingual_toxic_lexicon\")\n",
    "toxic_spans = load_dataset(\"textdetox/multilingual_toxic_spans\")\n",
    "paradetox_test_set = load_dataset(\"textdetox/multilingual_paradetox_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "414d21ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    en: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    ru: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    uk: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    de: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    es: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    am: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    zh: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    ar: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    hi: Dataset({\n",
      "        features: ['toxic_sentence', 'neutral_sentence'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    en: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    ru: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    uk: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    de: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    es: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    am: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    zh: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    ar: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    hi: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    it: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    fr: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    he: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 2011\n",
      "    })\n",
      "    hin: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 4363\n",
      "    })\n",
      "    tt: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    ja: Dataset({\n",
      "        features: ['text', 'toxic'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    am: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 245\n",
      "    })\n",
      "    es: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1195\n",
      "    })\n",
      "    ru: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 140517\n",
      "    })\n",
      "    uk: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 7356\n",
      "    })\n",
      "    en: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3386\n",
      "    })\n",
      "    zh: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3839\n",
      "    })\n",
      "    ar: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 430\n",
      "    })\n",
      "    hi: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 133\n",
      "    })\n",
      "    de: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 247\n",
      "    })\n",
      "    it: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 815\n",
      "    })\n",
      "    fr: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1287\n",
      "    })\n",
      "    he: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 731\n",
      "    })\n",
      "    hin: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 209\n",
      "    })\n",
      "    tt: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 15629\n",
      "    })\n",
      "    ja: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 328\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    en: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 991\n",
      "    })\n",
      "    es: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 987\n",
      "    })\n",
      "    de: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 970\n",
      "    })\n",
      "    zh: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 921\n",
      "    })\n",
      "    ar: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 990\n",
      "    })\n",
      "    am: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 995\n",
      "    })\n",
      "    ru: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 999\n",
      "    })\n",
      "    uk: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 943\n",
      "    })\n",
      "    hi: Dataset({\n",
      "        features: ['Sentence', 'Negative Connotations'],\n",
      "        num_rows: 992\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    uk: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    hi: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    zh: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    ar: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    de: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    en: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    ru: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    am: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    es: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    it: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    fr: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    he: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    hin: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    tt: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "    ja: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#Para ver que hay en cada dataset (diferentes idiomas y diferentes columnas)\n",
    "print(paradetox)\n",
    "print(multilingual_toxicity)\n",
    "print(toxic_keywords)\n",
    "print(toxic_spans)\n",
    "print(paradetox_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdc7b43",
   "metadata": {},
   "source": [
    "3. Extraemos los datos solo en inglés [\"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b42a26c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para seleccionar solo los datos en inglés\n",
    "paradetox_en = paradetox[\"en\"]\n",
    "multilingual_toxicity_en = multilingual_toxicity[\"en\"]\n",
    "toxic_keywords_en = toxic_keywords[\"en\"]\n",
    "toxic_spans_en = toxic_spans[\"en\"]\n",
    "paradetox_test_set_en = paradetox_test_set[\"en\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49cb3fa",
   "metadata": {},
   "source": [
    "Proximos pasos:\n",
    "- Eliminar valores nulos\n",
    "- Aplicar case folding\n",
    "- Aplicar tokenization\n",
    "- Eliminar stop words y signos de puntuación\n",
    "- Aplicar lemmatization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9389d0",
   "metadata": {},
   "source": [
    "4. Quitar valores Nulos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68fd4560",
   "metadata": {},
   "outputs": [],
   "source": [
    "paradetox_en = paradetox_en.filter(lambda x: all(v is not None for v in x.values()))\n",
    "multilingual_toxicity_en = multilingual_toxicity_en.filter(lambda x: all(v is not None for v in x.values()))\n",
    "toxic_keywords_en = toxic_keywords_en.filter(lambda x: all(v is not None for v in x.values()))\n",
    "toxic_spans_en = toxic_spans_en.filter(lambda x: all(v is not None for v in x.values()))\n",
    "paradetox_test_set_en = paradetox_test_set_en.filter(lambda x: all(v is not None for v in x.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57b22b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y transformamos los datos a pandas para facilitar el procesamiento\n",
    "english_paradetox_df = paradetox_en.to_pandas()\n",
    "english_multilingual_toxicity_df = multilingual_toxicity_en.to_pandas()\n",
    "english_toxic_keywords_df = toxic_keywords_en.to_pandas()\n",
    "english_toxic_spans_df = toxic_spans_en.to_pandas()\n",
    "english_paradetox_test_set_df = paradetox_test_set_en.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1256670",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_paradetox_df.info()\n",
    "english_multilingual_toxicity_df.info()\n",
    "english_toxic_keywords_df.info()\n",
    "english_toxic_spans_df.info()\n",
    "english_paradetox_test_set_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20fa6ed",
   "metadata": {},
   "source": [
    "5. Descargar recursos necesarios para tokenization, lemmatization y eliminación de stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db1431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\js834\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\js834\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\js834\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\js834\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('wordnet')    # Diccionario empleado para la lemmatization\n",
    "nltk.download('punkt')      # Modelo empleado para la tokenization\n",
    "nltk.download('averaged_perceptron_tagger')     # Modelo empleado para identificar el tipo de palabra\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()    #Se usará el WordNetLemmatizer de NLTK\n",
    "\n",
    "nltk.download('stopwords')  # Para descargar las stopwords en inglés\n",
    "stop_words_english = nltk.corpus.stopwords.words('english')     \n",
    "\n",
    "punctuation = set(string.punctuation)   # Para cargar signos de puntuación de la librería string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7cb7c3",
   "metadata": {},
   "source": [
    "6. Aplicar case folding en los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07a147ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️ English Paradetox\n",
    "cols = ['toxic_sentence', 'neutral_sentence']\n",
    "for col in cols:\n",
    "    english_paradetox_df[col + '_lower'] = english_paradetox_df[col].str.lower()\n",
    "\n",
    "# 2️ English Multilingual Toxicity\n",
    "for col in cols:\n",
    "    english_multilingual_toxicity_df['text_lower'] = english_multilingual_toxicity_df['text'].str.lower()\n",
    "\n",
    "# 3️ English Toxic Keywords\n",
    "english_toxic_keywords_df['text_lower'] = english_toxic_keywords_df['text'].str.lower()\n",
    "\n",
    "# 4️ English Toxic Spans\n",
    "cols = ['Sentence', 'Negative Connotations']\n",
    "for col in cols:\n",
    "    english_toxic_spans_df[col + '_lower'] = english_toxic_spans_df[col].str.lower()\n",
    "\n",
    "# 5️ English Paradetox Test Set\n",
    "english_paradetox_test_set_df['text_lower'] = english_paradetox_test_set_df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165ae87f",
   "metadata": {},
   "source": [
    "7. Aplicar tokenization a las columnas de texto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "566b72b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['toxic_sentence_lower', 'neutral_sentence_lower']\n",
    "for col in cols:\n",
    "    english_paradetox_df[col.replace('_lower','_tokens')] = english_paradetox_df[col].apply(word_tokenize)\n",
    "\n",
    "cols = ['text_lower']\n",
    "for col in cols:\n",
    "    english_multilingual_toxicity_df[col.replace('_lower','_tokens')] = english_multilingual_toxicity_df[col].apply(word_tokenize)              \n",
    "\n",
    "cols = ['text_lower']\n",
    "for col in cols:\n",
    "    english_toxic_keywords_df[col.replace('_lower','_tokens')] = english_toxic_keywords_df[col].apply(word_tokenize)\n",
    "\n",
    "cols = ['Sentence_lower', 'Negative Connotations_lower']\n",
    "for col in cols:\n",
    "    english_toxic_spans_df[col.replace('_lower','_tokens')] = english_toxic_spans_df[col].apply(word_tokenize)\n",
    "\n",
    "cols = ['text_lower']\n",
    "for col in cols:\n",
    "    english_paradetox_test_set_df[col.replace('_lower','_tokens')] = english_paradetox_test_set_df[col].apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c6825b",
   "metadata": {},
   "source": [
    "8. Seleccionar solo las columnas tokenizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a5d8d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 toxic_sentence_tokens  \\\n",
      "0    [then, all, of, a, sudden, i, see, her, ,, she...   \n",
      "1    [my, page, should, be, protected, first, so, t...   \n",
      "2                 [you, made, a, mistake, you, ass, .]   \n",
      "3    [you, know, more, than, these, idiots, ,, stay...   \n",
      "4    [piss, me, off, ,, fuckin, jerk, ,, get, on, m...   \n",
      "..                                                 ...   \n",
      "395  [at, least, one, party, gives, a, crap, about,...   \n",
      "396                 [parliament, is, a, lame, duck, .]   \n",
      "397  [side, -, note, ,, why, the, fuck, does, every...   \n",
      "398  [you, 're, not, being, helpful, ,, either, ,, ...   \n",
      "399  [prove, my, comment, wrong, and, post, a, vide...   \n",
      "\n",
      "                               neutral_sentence_tokens  \n",
      "0    [all, of, a, sudden, i, see, her, ,, she, is, ...  \n",
      "1    [my, page, should, be, protected, first, so, t...  \n",
      "2                           [you, made, a, mistake, .]  \n",
      "3    [you, know, more, than, these, people, ,, stay...  \n",
      "4                                [get, on, my, nerves]  \n",
      "..                                                 ...  \n",
      "395  [at, leas, one, party, pays, attention, about,...  \n",
      "396                             [parliament, is, dull]  \n",
      "397  [side-note, ,, why, does, everything, the, us,...  \n",
      "398      [you, 're, not, being, helpful, ,, either, .]  \n",
      "399  [prove, my, comment, wrong, and, post, a, vide...  \n",
      "\n",
      "[400 rows x 2 columns]\n",
      "                                            text_tokens  toxic\n",
      "0     [the, trans, women, reading, this, tweet, righ...      0\n",
      "1     [9, ), uhhhh, i, like, being, lgbt, a, lot, .,...      0\n",
      "2           [hero, rohit, sharma, love, from, pakistan]      0\n",
      "3     [as, a, slightly, feminine, top, i, appreciate...      0\n",
      "4     [delon, love, turkey, and, brave, turks, from,...      0\n",
      "...                                                 ...    ...\n",
      "4995  [id, rip, that, top, off, you, a, slap, and, p...      1\n",
      "4996  [i, hope, iran, nukes, the, cancer, usa, so, t...      1\n",
      "4997  [keep, those, black, low, lifes, out, our, cou...      1\n",
      "4998  [spank, your, ass, and, fill, your, slutty, pu...      1\n",
      "4999       [go, fuck, yourself, stupid, white, fuck, .]      1\n",
      "\n",
      "[5000 rows x 2 columns]\n",
      "         text_tokens\n",
      "0           [testes]\n",
      "1            [slopy]\n",
      "2        [upthebutt]\n",
      "3       [cuntsucker]\n",
      "4     [jelly, donut]\n",
      "...              ...\n",
      "3381     [bit, *, h]\n",
      "3382         [argie]\n",
      "3383      [pollocks]\n",
      "3384     [fagfucker]\n",
      "3385       [fagtard]\n",
      "\n",
      "[3386 rows x 1 columns]\n",
      "                                       Sentence_tokens  \\\n",
      "0    [mandated, and, right, fucking, now, would, be...   \n",
      "1    [>, i, wouldn, 't, care, how, ignorant, you, a...   \n",
      "2                   [6., are, you, fucking, insane, ?]   \n",
      "3    [after, all, its, hard, to, get, a, job, if, y...   \n",
      "4    [after, all, most, of, these, dorks, are, mill...   \n",
      "..                                                 ...   \n",
      "985  [at, least, one, party, gives, a, crap, about,...   \n",
      "986                 [parliament, is, a, lame, duck, .]   \n",
      "987  [side, -, note, ,, why, the, fuck, does, every...   \n",
      "988  [you, 're, not, being, helpful, ,, either, ,, ...   \n",
      "989  [prove, my, comment, wrong, and, post, a, vide...   \n",
      "\n",
      "           Negative Connotations_tokens  \n",
      "0                             [fucking]  \n",
      "1    [ignorant, ,, pretending, ,, shit]  \n",
      "2                  [fucking, ,, insane]  \n",
      "3                              [stupid]  \n",
      "4                               [dorks]  \n",
      "..                                  ...  \n",
      "985                              [crap]  \n",
      "986                        [lame, duck]  \n",
      "987                              [fuck]  \n",
      "988                         [tiny-dick]  \n",
      "989                           [fucking]  \n",
      "\n",
      "[990 rows x 2 columns]\n",
      "                                           text_tokens\n",
      "0    [mandated, and, ``, right, fucking, now, ``, w...\n",
      "1    [&, gt, i, wouldn, 't, care, how, ignorant, yo...\n",
      "2                   [6., are, you, fucking, insane, ?]\n",
      "3    [after, all, its, hard, to, get, a, job, if, y...\n",
      "4    [after, all, most, of, these, dorks, are, mill...\n",
      "..                                                 ...\n",
      "595  [what, do, you, mean, why, do, n't, you, keep,...\n",
      "596  [lets, not, edit, user, pages, and, look, like...\n",
      "597  [stalking, you, little, shit, you, need, to, s...\n",
      "598  [for, fuck, 's, sake, when, are, they, gon, na...\n",
      "599  [he, deserves, every, beating, and, rape, he, ...\n",
      "\n",
      "[600 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "english_paradetox_df = english_paradetox_df[['toxic_sentence_tokens', 'neutral_sentence_tokens']]\n",
    "english_multilingual_toxicity_df = english_multilingual_toxicity_df[['text_tokens', 'toxic']]\n",
    "english_toxic_keywords_df = english_toxic_keywords_df[['text_tokens']]\n",
    "english_toxic_spans_df = english_toxic_spans_df[['Sentence_tokens', 'Negative Connotations_tokens']]\n",
    "english_paradetox_test_set_df = english_paradetox_test_set_df[['text_tokens']]\n",
    "\n",
    "print(english_paradetox_df)\n",
    "print(english_multilingual_toxicity_df)\n",
    "print(english_toxic_keywords_df)        \n",
    "print(english_toxic_spans_df)\n",
    "print(english_paradetox_test_set_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8b64cc",
   "metadata": {},
   "source": [
    "9. Eliminar stop words y signos de puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "782b315c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 toxic_sentence_tokens  \\\n",
      "0    [sudden, see, shes, got, big, phony, tits, eve...   \n",
      "1    [page, protected, first, worthless, scum, like...   \n",
      "2                                 [made, mistake, ass]   \n",
      "3         [know, idiots, stay, course, truth, prevail]   \n",
      "4                    [piss, fuckin, jerk, get, nerves]   \n",
      "..                                                 ...   \n",
      "395    [least, one, party, gives, crap, humans, birth]   \n",
      "396                           [parliament, lame, duck]   \n",
      "397  [side, note, fuck, everything, us, make, calle...   \n",
      "398                  ['re, helpful, either, tiny-dick]   \n",
      "399  [prove, comment, wrong, post, video, fucking, ...   \n",
      "\n",
      "                               neutral_sentence_tokens  \n",
      "0                                 [sudden, see, grown]  \n",
      "1    [page, protected, first, unpleasant, people, l...  \n",
      "2                                      [made, mistake]  \n",
      "3         [know, people, stay, course, truth, prevail]  \n",
      "4                                        [get, nerves]  \n",
      "..                                                 ...  \n",
      "395  [leas, one, party, pays, attention, humans, bi...  \n",
      "396                                 [parliament, dull]  \n",
      "397  [side-note, everything, us, make, called, ``, ...  \n",
      "398                             ['re, helpful, either]  \n",
      "399        [prove, comment, wrong, post, video, tough]  \n",
      "\n",
      "[400 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Se eliminan las stop words (sacadas de nltk.corpus.stopwords) y signos de puntuación(sacados de string.punctuation))\n",
    "english_paradetox_df['toxic_sentence_tokens'] = english_paradetox_df['toxic_sentence_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")\n",
    "english_paradetox_df['neutral_sentence_tokens'] = english_paradetox_df['neutral_sentence_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")\n",
    "english_multilingual_toxicity_df['text_tokens'] = english_multilingual_toxicity_df['text_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")\n",
    "english_toxic_keywords_df['text_tokens'] = english_toxic_keywords_df['text_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")   \n",
    "english_toxic_spans_df['Sentence_tokens'] = english_toxic_spans_df['Sentence_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")\n",
    "english_toxic_spans_df['Negative Connotations_tokens'] = english_toxic_spans_df['Negative Connotations_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")\n",
    "english_paradetox_test_set_df['text_tokens'] = english_paradetox_test_set_df['text_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t.lower() not in stop_words_english and t not in punctuation]\n",
    ")\n",
    "print(english_paradetox_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f527627d",
   "metadata": {},
   "source": [
    "10. Lemmatizar cada uno de los tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d9b7fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para lemmatizar listas de tokens (se usa el WordNetLemmatizer de NLTK importado previamente)\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "# Paradetox\n",
    "english_paradetox_df['toxic_sentence_tokens'] = english_paradetox_df['toxic_sentence_tokens'].apply(lemmatize_tokens)\n",
    "english_paradetox_df['neutral_sentence_tokens'] = english_paradetox_df['neutral_sentence_tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "# Multilingual Toxicity\n",
    "english_multilingual_toxicity_df['text_tokens'] = english_multilingual_toxicity_df['text_tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "# Toxic Keywords\n",
    "english_toxic_keywords_df['text_tokens'] = english_toxic_keywords_df['text_tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "# Toxic Spans\n",
    "english_toxic_spans_df['Sentence_tokens'] = english_toxic_spans_df['Sentence_tokens'].apply(lemmatize_tokens)\n",
    "english_toxic_spans_df['Negative Connotations_tokens'] = english_toxic_spans_df['Negative Connotations_tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "# Paradetox Test Set\n",
    "english_paradetox_test_set_df['text_tokens'] = english_paradetox_test_set_df['text_tokens'].apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f8fc7",
   "metadata": {},
   "source": [
    "11. Guardar los datos procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "681055e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se guardan en la carpeta /datos/\n",
    "english_paradetox_df.to_csv(\"datos/english_paradetox_preprocessed.csv\", index=False)\n",
    "english_multilingual_toxicity_df.to_csv(\"datos/english_multilingual_toxicity_preprocessed.csv\", index=False)\n",
    "english_toxic_keywords_df.to_csv(\"datos/english_toxic_keywords_preprocessed.csv\", index=False)\n",
    "english_toxic_spans_df.to_csv(\"datos/english_toxic_spans_preprocessed.csv\", index=False)\n",
    "english_paradetox_test_set_df.to_csv(\"datos/english_paradetox_test_set_preprocessed.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
