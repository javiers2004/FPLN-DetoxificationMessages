{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9c0e837",
   "metadata": {},
   "source": [
    "Dividiremos la entrega en 2 tareas principales:\n",
    "- Clasificaci贸n: clasificaci贸n binaria de toxicidad\n",
    "- Detoxificaci贸n de Texto a partir de una frase de entrada (T贸xica) para generar una frase de salida (Neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299b6bea",
   "metadata": {},
   "source": [
    "**Tarea A: Clasificaci贸n de Toxicidad (Binary Classification)**\n",
    "\n",
    "-Datos: english_multilingual_toxicity_df\n",
    "\n",
    "-Input: text_str (o text_tokens)\n",
    "\n",
    "-Output: toxic (0 o 1)\n",
    "\n",
    "-Objetivo: Cumplir los requisitos de Shallow ML (SVM/Regresi贸n Log铆stica) y Deep Learning (CNN/LSTM comparando embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d28b562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total filas finales: 6292\n",
      "Distribuci贸n de clases:\n",
      "toxic\n",
      "1    3400\n",
      "0    2892\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train shape: (5033, 3)\n",
      "Test shape: (1259, 3)\n",
      "                                            text_tokens  \\\n",
      "933              [s, shocking, traitorous, buffoon, wh]   \n",
      "5890                                [gay, decision, ok]   \n",
      "1762  [revert, single, s, cover, original, person, d...   \n",
      "6096   [nothing, done, retarded, rather, retard, bitch]   \n",
      "4984  [dont, see, terminator.i, see, fucking, femini...   \n",
      "...                                                 ...   \n",
      "3627  [funny, runs, screaming, like, bitch, arms, so...   \n",
      "5390  [man, made, cry, bad, jewish, brothers, sister...   \n",
      "5958                               [woman, true, idiot]   \n",
      "1355  [d, love, see, republicans, slowly, flayed, al...   \n",
      "4036  [fuck, wrong, people, america, turning, stockp...   \n",
      "\n",
      "                                               text_str  toxic  \n",
      "933                    s shocking traitorous buffoon wh      1  \n",
      "5890                                    gay decision ok      0  \n",
      "1762  revert single s cover original person decided ...      0  \n",
      "6096          nothing done retarded rather retard bitch      1  \n",
      "4984  dont see terminator.i see fucking feminist mex...      1  \n",
      "...                                                 ...    ...  \n",
      "3627  funny runs screaming like bitch arms society w...      1  \n",
      "5390  man made cry bad jewish brothers sisters stay ...      0  \n",
      "5958                                   woman true idiot      1  \n",
      "1355  d love see republicans slowly flayed alive see...      1  \n",
      "4036  fuck wrong people america turning stockpot shi...      1  \n",
      "\n",
      "[5033 rows x 3 columns]\n",
      "                                            text_tokens  \\\n",
      "1526  [asian, teams, single-handedly, lighting, worl...   \n",
      "6103                 [zenigundam, oh, shit, beta, male]   \n",
      "1227  [inspirational, woman, leader, many, talents, ...   \n",
      "3088  [34, black, women, graduating, west, point, 20...   \n",
      "3127        [n't, believe, jesus, m, going, stab, face]   \n",
      "...                                                 ...   \n",
      "473   [one, stupid, bitch, guessing, s, whole, inbre...   \n",
      "5158  [matter, gender, culture, class, race, creed, ...   \n",
      "1702  [re, fucking, faggot, re, essentially, waste, ...   \n",
      "4791  [may, allah, ease, everything, order, amiiin, ...   \n",
      "299                      [selam, respect, iran, bosnia]   \n",
      "\n",
      "                                               text_str  toxic  \n",
      "1526     asian teams single-handedly lighting world cup      0  \n",
      "6103                       zenigundam oh shit beta male      1  \n",
      "1227  inspirational woman leader many talents 100 br...      0  \n",
      "3088  34 black women graduating west point 2019 maki...      0  \n",
      "3127                n't believe jesus m going stab face      1  \n",
      "...                                                 ...    ...  \n",
      "473   one stupid bitch guessing s whole inbreeding t...      1  \n",
      "5158  matter gender culture class race creed love ne...      0  \n",
      "1702      re fucking faggot re essentially waste oxygen      1  \n",
      "4791         may allah ease everything order amiiin url      0  \n",
      "299                           selam respect iran bosnia      0  \n",
      "\n",
      "[1259 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 1. DATASET PRINCIPAL (English Multilingual Toxicity)\n",
    "df_1 = pd.read_csv(\"datos/english_multilingual_toxicity_preprocessed_non_lemmatized.csv\")\n",
    "df_1['text_tokens'] = df_1['text_tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df_1['text_str'] = df_1['text_tokens'].apply(lambda x: \" \".join(x))\n",
    "df_final_1 = df_1[['text_tokens', 'text_str', 'toxic']].copy()\n",
    "\n",
    "# 2. DATASET PARADETOX - PARTE TXICA\n",
    "df_para_raw = pd.read_csv(\"datos/english_paradetox_preprocessed_non_lemmatized.csv\")\n",
    "df_final_2 = pd.DataFrame()\n",
    "df_final_2['text_tokens'] = df_para_raw['toxic_sentence_tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df_final_2['text_str'] = df_final_2['text_tokens'].apply(lambda x: \" \".join(x))\n",
    "df_final_2['toxic'] = 1  \n",
    "\n",
    "# 3. DATASET PARADETOX - PARTE NEUTRAL\n",
    "df_final_3 = pd.DataFrame()\n",
    "df_final_3['text_tokens'] = df_para_raw['neutral_sentence_tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df_final_3['text_str'] = df_final_3['text_tokens'].apply(lambda x: \" \".join(x))\n",
    "df_final_3['toxic'] = 0  \n",
    "\n",
    "# 4. DATASET TOXIC SPANS (Solo T贸xicos)\n",
    "df_spans_raw = pd.read_csv(\"datos/english_toxic_spans_preprocessed_non_lemmatized.csv\")\n",
    "df_final_4 = pd.DataFrame()\n",
    "df_final_4['text_tokens'] = df_spans_raw['Sentence_tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df_final_4['text_str'] = df_final_4['text_tokens'].apply(lambda x: \" \".join(x))\n",
    "df_final_4['toxic'] = 1  \n",
    "\n",
    "# 5. UNIN FINAL (CONCATENAR)\n",
    "# Ponemos los 4 dataframes en una lista\n",
    "lista_dfs = [df_final_1, df_final_2, df_final_3, df_final_4]\n",
    "# Los pegamos uno debajo de otro\n",
    "df_augmentado = pd.concat(lista_dfs, ignore_index=True)\n",
    "# Eliminar duplicados (si hay frases repetidas entre datasets)\n",
    "df_augmentado.drop_duplicates(subset='text_str', inplace=True)\n",
    "# Mezclar (Shuffle) para que no est茅n ordenados\n",
    "df_augmentado = df_augmentado.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total filas finales: {len(df_augmentado)}\")\n",
    "print(\"Distribuci贸n de clases:\")\n",
    "print(df_augmentado['toxic'].value_counts())\n",
    "\n",
    "# 6. DIVISIN TRAIN / TEST\n",
    "train_df, test_df = train_test_split(\n",
    "    df_augmentado, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df_augmentado['toxic']\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(train_df)\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7050e06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.90       579\n",
      "           1       0.93      0.90      0.92       680\n",
      "\n",
      "    accuracy                           0.91      1259\n",
      "   macro avg       0.91      0.91      0.91      1259\n",
      "weighted avg       0.91      0.91      0.91      1259\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90       579\n",
      "           1       0.93      0.91      0.92       680\n",
      "\n",
      "    accuracy                           0.91      1259\n",
      "   macro avg       0.91      0.91      0.91      1259\n",
      "weighted avg       0.91      0.91      0.91      1259\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Bag of Words + Logistic Regression \n",
    "\n",
    "# A. Vectorizador (Solo aprende vocabulario de TRAIN)\n",
    "vectorizer_bow = CountVectorizer(min_df=2) \n",
    "X_train_bow = vectorizer_bow.fit_transform(train_df['text_str'])\n",
    "X_test_bow = vectorizer_bow.transform(test_df['text_str']) # Transform usa el vocabulario ya aprendido\n",
    "\n",
    "# B. Modelo\n",
    "clf_lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf_lr.fit(X_train_bow, train_df['toxic'])\n",
    "\n",
    "# C. Evaluaci贸n\n",
    "y_pred_lr = clf_lr.predict(X_test_bow)\n",
    "print(classification_report(test_df['toxic'], y_pred_lr))\n",
    "\n",
    "\n",
    "\n",
    "# TF-IDF + SVM \n",
    "\n",
    "# A. Vectorizador\n",
    "vectorizer_tfidf = TfidfVectorizer(min_df=2)\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(train_df['text_str'])\n",
    "X_test_tfidf = vectorizer_tfidf.transform(test_df['text_str'])\n",
    "\n",
    "# B. Modelo (Kernel lineal es r谩pido y efectivo para texto)\n",
    "clf_svm = SVC(kernel='linear', random_state=42)\n",
    "clf_svm.fit(X_train_tfidf, train_df['toxic'])\n",
    "\n",
    "# C. Evaluaci贸n\n",
    "y_pred_svm = clf_svm.predict(X_test_tfidf)\n",
    "print(classification_report(test_df['toxic'], y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f9da3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.90       579\n",
      "           1       0.93      0.90      0.92       680\n",
      "\n",
      "    accuracy                           0.91      1259\n",
      "   macro avg       0.91      0.91      0.91      1259\n",
      "weighted avg       0.91      0.91      0.91      1259\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Bag of Words + Logistic Regression \n",
    "\n",
    "# A. Vectorizador (Solo aprende vocabulario de TRAIN)\n",
    "vectorizer_bow = CountVectorizer(min_df=2) \n",
    "X_train_bow = vectorizer_bow.fit_transform(train_df['text_str'])\n",
    "X_test_bow = vectorizer_bow.transform(test_df['text_str']) # Transform usa el vocabulario ya aprendido\n",
    "\n",
    "# B. Modelo\n",
    "clf_lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf_lr.fit(X_train_bow, train_df['toxic'])\n",
    "\n",
    "# C. Evaluaci贸n\n",
    "y_pred_lr = clf_lr.predict(X_test_bow)\n",
    "print(classification_report(test_df['toxic'], y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "827a753b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90       579\n",
      "           1       0.93      0.91      0.92       680\n",
      "\n",
      "    accuracy                           0.91      1259\n",
      "   macro avg       0.91      0.91      0.91      1259\n",
      "weighted avg       0.91      0.91      0.91      1259\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF + SVM \n",
    "\n",
    "# A. Vectorizador\n",
    "vectorizer_tfidf = TfidfVectorizer(min_df=2)\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(train_df['text_str'])\n",
    "X_test_tfidf = vectorizer_tfidf.transform(test_df['text_str'])\n",
    "\n",
    "# B. Modelo (Kernel lineal es r谩pido y efectivo para texto)\n",
    "clf_svm = SVC(kernel='linear', random_state=42)\n",
    "clf_svm.fit(X_train_tfidf, train_df['toxic'])\n",
    "\n",
    "# C. Evaluaci贸n\n",
    "y_pred_svm = clf_svm.predict(X_test_tfidf)\n",
    "print(classification_report(test_df['toxic'], y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a318c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase m谩s larga: 189 palabras\n",
      "Longitud media: 7.21 palabras\n",
      "El 95% de las frases miden menos de: 13.0 palabras\n",
      "El 99% de las frases miden menos de: 16.0 palabras\n"
     ]
    }
   ],
   "source": [
    "longitudes = train_df['text_tokens'].apply(len)\n",
    "\n",
    "print(f\"Frase m谩s larga: {longitudes.max()} palabras\")\n",
    "print(f\"Longitud media: {longitudes.mean():.2f} palabras\")\n",
    "print(f\"El 95% de las frases miden menos de: {longitudes.quantile(0.95)} palabras\")\n",
    "print(f\"El 99% de las frases miden menos de: {longitudes.quantile(0.99)} palabras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2734b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama帽o del vocabulario (Deep Learning): 3438\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "# 1. Construir Vocabulario usando solo TRAIN\n",
    "word_counts = Counter()\n",
    "for tokens in train_df['text_tokens']:\n",
    "    word_counts.update(tokens)\n",
    "\n",
    "# Mapeo: Palabras -> ndices\n",
    "# <PAD>=0, <UNK>=1\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "next_idx = 2\n",
    "min_freq = 2 # Ignorar palabras que aparecen solo 1 vez para reducir ruido\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count >= min_freq:\n",
    "        vocab[word] = next_idx\n",
    "        next_idx += 1\n",
    "\n",
    "print(f\"Tama帽o del vocabulario (Deep Learning): {len(vocab)}\")\n",
    "\n",
    "# 2. Clase Dataset\n",
    "class ToxicityDataset(Dataset):\n",
    "    def __init__(self, dataframe, vocab, max_len=25):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data.loc[idx, 'text_tokens']\n",
    "        label = self.data.loc[idx, 'toxic']\n",
    "        \n",
    "        # Token a Index\n",
    "        indices = [self.vocab.get(t, self.vocab[\"<UNK>\"]) for t in tokens]\n",
    "        \n",
    "        # Padding / Truncating\n",
    "        if len(indices) < self.max_len:\n",
    "            indices += [self.vocab[\"<PAD>\"]] * (self.max_len - len(indices))\n",
    "        else:\n",
    "            indices = indices[:self.max_len]\n",
    "            \n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "# 3. Crear DataLoaders\n",
    "train_dataset = ToxicityDataset(train_df, vocab)\n",
    "test_dataset = ToxicityDataset(test_df, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad188947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=200, pretrained_weights=None, freeze_embeddings=False):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        \n",
    "        # Capa de Embedding\n",
    "        if pretrained_weights is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_weights, freeze=freeze_embeddings)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "            \n",
    "        # Capas Convolucionales (detectan patrones de 3, 4 y 5 palabras)\n",
    "        self.conv1 = nn.Conv1d(embed_dim, 100, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(embed_dim, 100, kernel_size=4, padding=2)\n",
    "        self.conv3 = nn.Conv1d(embed_dim, 100, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.fc = nn.Linear(300, 1) # 3 filtros * 100 canales\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len]\n",
    "        x_emb = self.embedding(x).permute(0, 2, 1) # -> [batch, embed_dim, seq_len]\n",
    "        \n",
    "        # Aplicar Convoluci贸n + ReLU + Max Pooling\n",
    "        # Nota: usamos max(dim=2) para quedarnos con la caracter铆stica m谩s fuerte de toda la frase\n",
    "        c1 = torch.max(torch.relu(self.conv1(x_emb)), dim=2)[0]\n",
    "        c2 = torch.max(torch.relu(self.conv2(x_emb)), dim=2)[0]\n",
    "        c3 = torch.max(torch.relu(self.conv3(x_emb)), dim=2)[0]\n",
    "        \n",
    "        # Concatenar caracter铆sticas\n",
    "        out = torch.cat((c1, c2, c3), dim=1)\n",
    "        out = self.dropout(out)\n",
    "        return self.sigmoid(self.fc(out)).squeeze()\n",
    "\n",
    "def train_evaluate(model_name, model, train_loader, test_loader, epochs=5):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    print(f\"\\nEntrenando: {model_name}...\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # Validaci贸n\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                y_pred = model(X_batch)\n",
    "                predicted = (y_pred > 0.5).float()\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "        \n",
    "        acc = correct / total\n",
    "        print(f\"Epoca {epoch+1}/{epochs} | Loss: {train_loss/len(train_loader):.4f} | Acc Test: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "450d6bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PREPARANDO EMBEDDINGS REALES (Word2Vec) ---\n",
      "Embeddings cargados: 3436 palabras encontradas. 0 no encontradas.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- PREPARANDO EMBEDDINGS REALES (Word2Vec) ---\")\n",
    "\n",
    "# 1. Entrenar tu propio Word2Vec con tus datos de entrenamiento\n",
    "# Esto har谩 que el modelo aprenda qu茅 significan tus palabras (insultos, slang, etc.)\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=train_df['text_tokens'], # Tus frases tokenizadas\n",
    "    vector_size=200,                   # El tama帽o del vector (embed_dim)\n",
    "    window=5,                          # Mira 5 palabras a los lados\n",
    "    min_count=2,                       # Ignora palabras muy raras\n",
    "    workers=4,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# 2. Crear la \"Matriz de Pesos\" para pas谩rsela a la CNN\n",
    "# Esta funci贸n traduce el diccionario de PyTorch al formato de Word2Vec\n",
    "def create_embedding_matrix(vocab, w2v_model, embed_dim=200):\n",
    "    vocab_size = len(vocab)\n",
    "    # Inicializamos una matriz con n煤meros aleatorios (para las palabras que W2V no conozca)\n",
    "    embedding_matrix = np.random.normal(scale=0.6, size=(vocab_size, embed_dim))\n",
    "    \n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    \n",
    "    # Rellenamos la matriz con los vectores que acabamos de aprender\n",
    "    for word, idx in vocab.items():\n",
    "        if idx < 2: continue # Saltamos <PAD> y <UNK>\n",
    "        \n",
    "        if word in w2v_model.wv:\n",
    "            embedding_matrix[idx] = w2v_model.wv[word]\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "            \n",
    "    print(f\"Embeddings cargados: {hits} palabras encontradas. {misses} no encontradas.\")\n",
    "    return torch.FloatTensor(embedding_matrix)\n",
    "\n",
    "# 3. GENERAR LA VARIABLE REAL\n",
    "# Aqu铆 sustituimos la \"simulated\" por la \"real\"\n",
    "w2v_weights = create_embedding_matrix(vocab, w2v_model, embed_dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28933923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando: CNN From Scratch...\n",
      "Epoca 1/5 | Loss: 0.5251 | Acc Test: 0.8014\n",
      "Epoca 2/5 | Loss: 0.2950 | Acc Test: 0.8403\n",
      "Epoca 3/5 | Loss: 0.1854 | Acc Test: 0.8562\n",
      "Epoca 4/5 | Loss: 0.1132 | Acc Test: 0.8586\n",
      "Epoca 5/5 | Loss: 0.0701 | Acc Test: 0.8610\n",
      "\n",
      "Entrenando: CNN W2V Frozen...\n",
      "Epoca 1/5 | Loss: 0.6998 | Acc Test: 0.5187\n",
      "Epoca 2/5 | Loss: 0.6872 | Acc Test: 0.6005\n",
      "Epoca 3/5 | Loss: 0.6867 | Acc Test: 0.5806\n",
      "Epoca 4/5 | Loss: 0.6739 | Acc Test: 0.5774\n",
      "Epoca 5/5 | Loss: 0.6773 | Acc Test: 0.5814\n",
      "\n",
      "Entrenando: CNN W2V Fine-Tuned...\n",
      "Epoca 1/5 | Loss: 0.6875 | Acc Test: 0.6767\n",
      "Epoca 2/5 | Loss: 0.4302 | Acc Test: 0.8888\n",
      "Epoca 3/5 | Loss: 0.1702 | Acc Test: 0.9182\n",
      "Epoca 4/5 | Loss: 0.0844 | Acc Test: 0.9230\n",
      "Epoca 5/5 | Loss: 0.0516 | Acc Test: 0.9150\n"
     ]
    }
   ],
   "source": [
    "# PREPARACIN DE EMBEDDINGS\n",
    "embed_dim = 200\n",
    "\n",
    "# EXPERIMENTO A: FROM SCRATCH\n",
    "model_scratch = CNNClassifier(len(vocab), embed_dim, pretrained_weights=None)\n",
    "train_evaluate(\"CNN From Scratch\", model_scratch, train_loader, test_loader)\n",
    "\n",
    "# EXPERIMENTO B: W2V FROZEN\n",
    "# Importante: freeze_embeddings=True\n",
    "model_frozen = CNNClassifier(len(vocab), embed_dim, pretrained_weights=w2v_weights, freeze_embeddings=True)\n",
    "train_evaluate(\"CNN W2V Frozen\", model_frozen, train_loader, test_loader)\n",
    "\n",
    "# EXPERIMENTO C: W2V FINE-TUNED\n",
    "# Importante: freeze_embeddings=False\n",
    "model_finetuned = CNNClassifier(len(vocab), embed_dim, pretrained_weights=w2v_weights, freeze_embeddings=False)\n",
    "train_evaluate(\"CNN W2V Fine-Tuned\", model_finetuned, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d2cc744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Probando Modelo CNN ===\n",
      "Frase: 'I love this beautiful sunny day'\n",
      "Predicci贸n: NEUTRAL  (Probabilidad: 0.0018)\n",
      "\n",
      "Frase: 'You are a stupid idiot and I hate you'\n",
      "Predicci贸n: TXICA  (Probabilidad: 1.0000)\n",
      "\n",
      "Frase: 'The book is on the table'\n",
      "Predicci贸n: NEUTRAL  (Probabilidad: 0.1740)\n",
      "\n",
      "Frase: 'Shut up you fool'\n",
      "Predicci贸n: TXICA  (Probabilidad: 0.9996)\n",
      "\n",
      "Frase: 'You look like you belong in a zoo.'\n",
      "Predicci贸n: TXICA  (Probabilidad: 0.5888)\n",
      "\n",
      "Frase: 'Women should not be allowed to vote.'\n",
      "Predicci贸n: NEUTRAL  (Probabilidad: 0.0517)\n",
      "\n",
      "Frase: 'I hope you never wake up.'\n",
      "Predicci贸n: TXICA  (Probabilidad: 0.9071)\n",
      "\n",
      "Frase: 'This movie is fucking amazing!'\n",
      "Predicci贸n: TXICA  (Probabilidad: 0.9999)\n",
      "\n",
      "Frase: 'You are the shit, bro!'\n",
      "Predicci贸n: NEUTRAL  (Probabilidad: 0.2463)\n",
      "\n",
      "Frase: 'Hell yeah, let's go!'\n",
      "Predicci贸n: TXICA  (Probabilidad: 0.9663)\n",
      "\n",
      "Frase: 'You are a genius... at failing.'\n",
      "Predicci贸n: NEUTRAL  (Probabilidad: 0.2463)\n",
      "\n",
      "Frase: 'Nice job breaking everything, hero.'\n",
      "Predicci贸n: NEUTRAL  (Probabilidad: 0.0324)\n",
      "\n",
      "Frase: 'I am a proud gay man.'\n",
      "Predicci贸n: NEUTRAL  (Probabilidad: 0.0304)\n",
      "\n",
      "Frase: 'My best friend is muslim.'\n",
      "Predicci贸n: NEUTRAL  (Probabilidad: 0.0209)\n",
      "\n",
      "Frase: 'Kill the python process.'\n",
      "Predicci贸n: TXICA  (Probabilidad: 0.9610)\n",
      "\n",
      "Frase: 'Execute the command now.'\n",
      "Predicci贸n: NEUTRAL  (Probabilidad: 0.2627)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clasificar_frase_cnn(frase, model, vocab, max_len=50):\n",
    "    \"\"\"\n",
    "    Toma una frase cruda, la convierte a 铆ndices y predice la toxicidad.\n",
    "    \"\"\"\n",
    "    model.eval() # Poner modelo en modo evaluaci贸n\n",
    "    \n",
    "    # 1. Tokenizar (Simulaci贸n simple: min煤sculas y split)\n",
    "    # IMPORTANTE: Lo ideal es usar la misma librer铆a que usaste en el preprocesamiento (NLTK)\n",
    "    # Si no tienes NLTK cargado ahora, esto servir谩 para una prueba r谩pida:\n",
    "    tokens = frase.lower().split() \n",
    "    \n",
    "    # 2. Convertir tokens a 铆ndices usando tu vocabulario (vocab)\n",
    "    indices = [vocab.get(t, vocab[\"<UNK>\"]) for t in tokens]\n",
    "    \n",
    "    # 3. Padding / Recorte (Igual que en el entrenamiento)\n",
    "    if len(indices) < max_len:\n",
    "        indices += [vocab[\"<PAD>\"]] * (max_len - len(indices))\n",
    "    else:\n",
    "        indices = indices[:max_len]\n",
    "        \n",
    "    # 4. Convertir a Tensor de PyTorch y a帽adir dimensi贸n de Batch (1, 50)\n",
    "    tensor_input = torch.tensor(indices, dtype=torch.long).unsqueeze(0)\n",
    "    \n",
    "    # 5. Predicci贸n\n",
    "    with torch.no_grad():\n",
    "        probabilidad = model(tensor_input).item()\n",
    "    \n",
    "    # 6. Resultado legible\n",
    "    etiqueta = \"TXICA \" if probabilidad > 0.5 else \"NEUTRAL \"\n",
    "    print(f\"Frase: '{frase}'\")\n",
    "    print(f\"Predicci贸n: {etiqueta} (Probabilidad: {probabilidad:.4f})\\n\")\n",
    "\n",
    "# --- PRUEBA TU MODELO ---\n",
    "mis_frases = [\n",
    "    \"I love this beautiful sunny day\",\n",
    "    \"You are a stupid idiot and I hate you\",\n",
    "    \"The book is on the table\",\n",
    "    \"Shut up you fool\",\n",
    "    # --- GRUPO 1: Toxicidad sin insultos (Deber铆a ser TXICA) ---\n",
    "    \"You look like you belong in a zoo.\",\n",
    "    \"Women should not be allowed to vote.\",\n",
    "    \"I hope you never wake up.\",\n",
    "    \n",
    "    # --- GRUPO 2: Coloquialismo positivo (Deber铆a ser NEUTRAL) ---\n",
    "    \"This movie is fucking amazing!\",\n",
    "    \"You are the shit, bro!\",\n",
    "    \"Hell yeah, let's go!\",\n",
    "    \n",
    "    # --- GRUPO 3: Sarcasmo (Deber铆a ser TXICA - Muy dif铆cil) ---\n",
    "    \"You are a genius... at failing.\",\n",
    "    \"Nice job breaking everything, hero.\",\n",
    "    \n",
    "    # --- GRUPO 4: Identidad / Sesgo (Deber铆a ser NEUTRAL) ---\n",
    "    \"I am a proud gay man.\",\n",
    "    \"My best friend is muslim.\",\n",
    "    \n",
    "    # --- GRUPO 5: Contexto T茅cnico (Deber铆a ser NEUTRAL) ---\n",
    "    \"Kill the python process.\",\n",
    "    \"Execute the command now.\"\n",
    "]\n",
    "\n",
    "print(\"=== Probando Modelo CNN ===\")\n",
    "for f in mis_frases:\n",
    "    clasificar_frase_cnn(f, model_finetuned, vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
