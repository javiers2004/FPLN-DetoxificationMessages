{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0c58b2b",
   "metadata": {},
   "source": [
    "## MODELO A: Detección de Toxicidad en Textos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b524e3c",
   "metadata": {},
   "source": [
    "Esta primera tarea consistirá en desarrollar un clasificador para evaluar si un comentario es **Tóxico (1)** o **Neutral (0)**.\n",
    "Para ello desarrollaremos diferentes modelos y evaluaremos su rendimiento:\n",
    "\n",
    "En primer lugar, desarrollaremos dos modelos que utilizan algoritmos clásicos de Machine Learning:\n",
    "- Uso de BagOfWords para contar la frecuencia de las palabras y clasificación con Logistic Regression.\n",
    "- Uso de TF-IDF para contar la importancia de las palabras y clasificación con Support Vector Machine.\n",
    "\n",
    "En segundo lugar, implementaremos una CNN, en la que probaremos 4 escenarios de inicialización de embeddings:\n",
    "- A. CNN From Scratch donde la capa de embeddings se inicializa con valores aleatorios (aprendizaje de vocabulario y clasificación de manera simultánea).\n",
    "- B. CNN + Word2Vec Frozen donde cargamos los pesos de un Word2Vec entrenado con los propios datos y se congelan.\n",
    "- C. CNN + Word2Vec Fine-Tuned donde cargamos los pesos de un Word2Vec entrenado con los propios datos pero se ajustan durante el entrenamiento.\n",
    "- D. CNN + GloVe Fine-Tuned donde se inicializa la capa de embeddings con los vectores de un GloVe pre-entrenado (con millones de datos) y sus pesos se fine-tunean.\n",
    "\n",
    "Finalmente, usaremos embeddings contextuales:\n",
    " - E. Fine-Tuning de BERT Multilingual (bert-base-multilingual-cased) ajustando los pesos del modelo preentrenado\n",
    " - F. Fine-Tuning de DistilBERT Uncased (distilbert-base-uncased) ajustando los pesos del modelo preentrenado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbb716e",
   "metadata": {},
   "source": [
    "## 1. Carga y Preprocesamiento de Datos\n",
    "\n",
    "Cargaremos los siguientes conjuntos de datos:\n",
    "* **English Multilingual Toxicity** \n",
    "* **Paradetox** \n",
    "* **Toxic Spans** \n",
    "\n",
    "Convertiremos todos los textos a tokens y concatenaremos los conjuntos unificando los formatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d28b562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas finales: 6292\n",
      "Distribución de los valores en la columna 'toxic':\n",
      "toxic\n",
      "1    3400\n",
      "0    2892\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# En primer lugar, procesaremos todos los datasets que usaremos.\n",
    "# Cargaremos cada conjunto y convertimos las cadenas de texto en listas de Python. \n",
    "\n",
    "# Uniremos los 4 conjuntos en uno solo con la siguiente estructura (text_tokens, text_str, toxic).\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. DATASET PRINCIPAL (English Multilingual Toxicity)\n",
    "df_1 = pd.read_csv(\"datos/english_multilingual_toxicity_preprocessed_non_lemmatized.csv\")\n",
    "df_1['text_tokens'] = df_1['text_tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df_1['text_str'] = df_1['text_tokens'].apply(lambda x: \" \".join(x))\n",
    "df_final_1 = df_1[['text_tokens', 'text_str', 'toxic']].copy()\n",
    "\n",
    "# 2. DATASET PARADETOX - PARTE TÓXICA\n",
    "#Aquí cargamos solo la parte tóxica, por ello asignamos la etiqueta 1 directamente\n",
    "df_para_raw = pd.read_csv(\"datos/english_paradetox_preprocessed_non_lemmatized.csv\")\n",
    "df_final_2 = pd.DataFrame()\n",
    "df_final_2['text_tokens'] = df_para_raw['toxic_sentence_tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df_final_2['text_str'] = df_final_2['text_tokens'].apply(lambda x: \" \".join(x))\n",
    "df_final_2['toxic'] = 1  \n",
    "\n",
    "# 3. DATASET PARADETOX - PARTE NEUTRAL\n",
    "#Aquí cargamos solo la parte neutral, por ello asignamos la etiqueta 0 directamente\n",
    "df_final_3 = pd.DataFrame()\n",
    "df_final_3['text_tokens'] = df_para_raw['neutral_sentence_tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df_final_3['text_str'] = df_final_3['text_tokens'].apply(lambda x: \" \".join(x))\n",
    "df_final_3['toxic'] = 0  \n",
    "\n",
    "# 4. DATASET TOXIC SPANS (Solo Tóxicos)\n",
    "df_spans_raw = pd.read_csv(\"datos/english_toxic_spans_preprocessed_non_lemmatized.csv\")\n",
    "df_final_4 = pd.DataFrame()\n",
    "df_final_4['text_tokens'] = df_spans_raw['Sentence_tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df_final_4['text_str'] = df_final_4['text_tokens'].apply(lambda x: \" \".join(x))\n",
    "df_final_4['toxic'] = 1  \n",
    "\n",
    "# Uniremos los 4 en una lista y los concatenaremos en 'df_augmentado'\n",
    "lista_dfs = [df_final_1, df_final_2, df_final_3, df_final_4]\n",
    "df_augmentado = pd.concat(lista_dfs, ignore_index=True)\n",
    "\n",
    "# Se eliminan los duplicados basándonos en la columna 'text_str'\n",
    "df_augmentado.drop_duplicates(subset='text_str', inplace=True)\n",
    "\n",
    "# Y se mezcla aleatoriamente el DataFrame resultante (se barajan el 100% de las filas y se reinician los índices)\n",
    "df_augmentado = df_augmentado.sample(frac=1, random_state=1234).reset_index(drop=True)\n",
    "\n",
    "# Imprimimos la estructura\n",
    "print(f\"Número de filas finales: {len(df_augmentado)}\") # Número final de filas del conjunto\n",
    "print(\"Distribución de los valores en la columna 'toxic':\") \n",
    "print(df_augmentado['toxic'].value_counts()) # Distribución de clases (tóxico/no tóxico) en la columna 'toxic'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d31fb6",
   "metadata": {},
   "source": [
    "### División del Dataset (Train/Validation/Test)\n",
    "\n",
    "Separamos los datos en:\n",
    "* **Entrenamiento (70%):** Para ajustar los pesos de los modelos.\n",
    "* **Validación (15%):** Para monitorear el rendimiento en cada époch.\n",
    "* **Prueba (15%):** Para evaluar el rendimiento final.\n",
    "\n",
    "La idea es que para los entrenamientos y el ajuste de hiperparámetros se usen el Train y el Validation set y finalmente con el Test set compararemos los resultados de cada uno de ellos para ver cuál es el mejor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ea32ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4404 | Val: 944 | Test: 944\n",
      "Train shape: (4404, 3)\n",
      "Test shape: (944, 3)\n",
      "Validation shape: (944, 3)\n",
      "                                            text_tokens  \\\n",
      "2960  [honor, pride, month, please, donate, gofundme...   \n",
      "1469  [slapping, unconscious, girl, s, real, o-3, pu...   \n",
      "2470  [fucking, loser, n't, cheat, wife, like, rest,...   \n",
      "114   [sort, retard, faggot, goes, life, caring, mea...   \n",
      "5633  [holy, shit, finished, season, good, dark, esp...   \n",
      "\n",
      "                                               text_str  toxic  \n",
      "2960  honor pride month please donate gofundme help ...      0  \n",
      "1469         slapping unconscious girl s real o-3 pussy      1  \n",
      "2470          fucking loser n't cheat wife like rest us      1  \n",
      "114      sort retard faggot goes life caring mean words      1  \n",
      "5633  holy shit finished season good dark especially...      1  \n",
      "                                            text_tokens  \\\n",
      "5238  [people, talk, color, artists, try, capture, p...   \n",
      "3947                           [well, boys, homophobia]   \n",
      "3917                            [girls, retarded, ways]   \n",
      "1384  [zendaya, aura, nuturing, mother, time, want, ...   \n",
      "5187  [move, see, welcoming, pasty, white, faggot, ass]   \n",
      "\n",
      "                                               text_str  toxic  \n",
      "5238  people talk color artists try capture perfect ...      0  \n",
      "3947                               well boys homophobia      0  \n",
      "3917                                girls retarded ways      1  \n",
      "1384        zendaya aura nuturing mother time want wife      0  \n",
      "5187          move see welcoming pasty white faggot ass      1  \n",
      "                                            text_tokens  \\\n",
      "2642                                [get, load, nigger]   \n",
      "5717                          [garbage, idiot, married]   \n",
      "1827  [whoa, buddy, appointed, gatekeeper, christian...   \n",
      "1053  [meier, lots, post-transition, trans, people, ...   \n",
      "2813                  [fuck, even, afghanistan, number]   \n",
      "\n",
      "                                               text_str  toxic  \n",
      "2642                                    get load nigger      1  \n",
      "5717                              garbage idiot married      1  \n",
      "1827  whoa buddy appointed gatekeeper christians m p...      0  \n",
      "1053       meier lots post-transition trans people pass      0  \n",
      "2813                       fuck even afghanistan number      1  \n"
     ]
    }
   ],
   "source": [
    "# Dividiremos en train, test y validation (70%-15%-15%).\n",
    "\n",
    "df_temporal, test_df = train_test_split(df_augmentado, test_size=0.15, stratify=df_augmentado['toxic'], random_state=1234)\n",
    "train_df, val_df = train_test_split(df_temporal, test_size=0.1765, stratify=df_temporal['toxic'], random_state=1234) # 0.1765 es aprox el 15% del total original,\n",
    "\n",
    "print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\") # Tamaño del conjunto de entrenamiento\n",
    "print(f\"Test shape: {test_df.shape}\") # Tamaño del conjunto de prueba\n",
    "print(f\"Validation shape: {val_df.shape}\") # Tamaño del conjunto de validación\n",
    "print(train_df.head())  # Primeras filas del conjunto de entrenamiento\n",
    "print(test_df.head())  # Primeras filas del conjunto de prueba\n",
    "print(val_df.head())  # Primeras filas del conjunto de validación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e71d26f",
   "metadata": {},
   "source": [
    "## 2. Algorítmos de ML clásicos\n",
    "Antes de utilizar Deep Learning, establecemos una línea base con modelos clásicos para tener una referencia de rendimiento.\n",
    "\n",
    "* **Modelo 1 (Bag of Words + Regresión Logística):** Se basa puramente en la frecuencia de aparición de palabras. Es rápido y sencillo.\n",
    "* **Modelo 2 (TF-IDF + SVM):** Pondera las palabras según su importancia. SVM busca el margen máximo de separación entre clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f9da3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.92       434\n",
      "           1       0.94      0.91      0.93       510\n",
      "\n",
      "    accuracy                           0.92       944\n",
      "   macro avg       0.92      0.92      0.92       944\n",
      "weighted avg       0.92      0.92      0.92       944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Bag of Words + Logistic Regression \n",
    "\n",
    "# A. Aplicamos el CountVectorizer, con un umbral mínimo de frecuencia de 2 (ignorar palabras que aparecen en menos de 2 filas), de esta forma evitamos palabras muy raras que solo aparezcan 1 vez.\n",
    "# Lo aplicamos tanto al train como al validation\n",
    "vectorizer_bow = CountVectorizer(min_df=2) \n",
    "X_train_bow = vectorizer_bow.fit_transform(train_df['text_str'])\n",
    "X_val_bow = vectorizer_bow.transform(val_df['text_str']) \n",
    "\n",
    "# B. Entrenamos el modelo LogisticRegression para clasificar la toxicidad sobre el conjunto de entrenamiento (1000 iteraciones)\n",
    "clf_lr = LogisticRegression(max_iter=1000, random_state=1234)\n",
    "clf_lr.fit(X_train_bow, train_df['toxic'])\n",
    "\n",
    "# C. Evaluamos el modelo en el conjunto de validación e imprimimos el informe de clasificación\n",
    "y_pred_lr = clf_lr.predict(X_val_bow)\n",
    "print(classification_report(val_df['toxic'], y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "827a753b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- COMPARATIVA DE KERNELS SVM ---\n",
      "\n",
      "Entrenando SVM Lineal...\n",
      "SVM Lineal -> Tiempo: 1.33s | Accuracy: 91.74%\n",
      "Entrenando SVM No Lineal (RBF)...\n",
      "SVM RBF    -> Tiempo: 1.81s | Accuracy: 89.51%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "\n",
    "# TF-IDF + SVM \n",
    "\n",
    "# A. Aplicamos el TfidfVectorizer, con un umbral mínimo de frecuencia de 2 (ignorar palabras que aparecen en menos de 2 filas)\n",
    "# Lo aplicamos tanto al train como al validation\n",
    "vectorizer_tfidf = TfidfVectorizer(min_df=2)\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(train_df['text_str'])\n",
    "X_val_tfidf = vectorizer_tfidf.transform(val_df['text_str'])\n",
    "\n",
    "print(\"--- COMPARATIVA DE KERNELS SVM ---\\n\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# OPCIÓN 1: KERNEL LINEAL (El que usabas)\n",
    "# -------------------------------------------------------\n",
    "print(\"Entrenando SVM Lineal...\")\n",
    "start_time = time.time()\n",
    "clf_svm_linear = SVC(kernel='linear', random_state=1234)\n",
    "clf_svm_linear.fit(X_train_tfidf, train_df['toxic'])\n",
    "time_linear = time.time() - start_time\n",
    "\n",
    "y_pred_linear = clf_svm_linear.predict(X_val_tfidf)\n",
    "acc_linear = accuracy_score(val_df['toxic'], y_pred_linear)\n",
    "\n",
    "print(f\"SVM Lineal -> Tiempo: {time_linear:.2f}s | Accuracy: {acc_linear:.2%}\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# OPCIÓN 2: KERNEL NO LINEAL (RBF - Radial Basis Function)\n",
    "# -------------------------------------------------------\n",
    "print(\"Entrenando SVM No Lineal (RBF)...\")\n",
    "start_time = time.time()\n",
    "clf_svm_rbf = SVC(kernel='rbf', random_state=1234) # RBF es el estándar no lineal\n",
    "clf_svm_rbf.fit(X_train_tfidf, train_df['toxic'])\n",
    "time_rbf = time.time() - start_time\n",
    "\n",
    "y_pred_rbf = clf_svm_rbf.predict(X_val_tfidf)\n",
    "acc_rbf = accuracy_score(val_df['toxic'], y_pred_rbf)\n",
    "\n",
    "print(f\"SVM RBF    -> Tiempo: {time_rbf:.2f}s | Accuracy: {acc_rbf:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41439d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91       434\n",
      "           1       0.93      0.91      0.92       510\n",
      "\n",
      "    accuracy                           0.92       944\n",
      "   macro avg       0.92      0.92      0.92       944\n",
      "weighted avg       0.92      0.92      0.92       944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Se ha elegido el kernel lineal por su mejor rendimiento y menor tiempo de entrenamiento\n",
    "\n",
    "clf_svm = clf_svm_linear # Guardamos el lineal como el modelo final\n",
    "y_pred_svm = y_pred_linear\n",
    "\n",
    "print(classification_report(val_df['toxic'], y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0746e58",
   "metadata": {},
   "source": [
    "Evaluaremos el modelo usando el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b94a47c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- EVALUACIÓN EN CONJUNTO DE TEST (944 muestras) ---\n",
      "\n",
      "Modelo 1 (BoW + LR) acertó: 845 de 944 (89.51%)\n",
      "Modelo 2 (TF-IDF + SVM) acertó: 845 de 944 (89.51%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f\"--- EVALUACIÓN EN CONJUNTO DE TEST ({len(test_df)} muestras) ---\\n\")\n",
    "\n",
    "# ==========================================\n",
    "# EVALUACIÓN MODELO 1: BoW + LR\n",
    "# ==========================================\n",
    "\n",
    "# 1. Transformar el texto de TEST usando el vocabulario aprendido (Bag of Words)\n",
    "# IMPORTANTE: Usamos .transform(), NUNCA .fit() en test\n",
    "X_test_bow = vectorizer_bow.transform(test_df['text_str'])\n",
    "\n",
    "# 2. Generar predicciones nuevas para el test\n",
    "y_pred_lr_test = clf_lr.predict(X_test_bow)\n",
    "\n",
    "# 3. Comparar predicción vs realidad\n",
    "aciertos_lr = accuracy_score(test_df['toxic'], y_pred_lr_test)\n",
    "num_aciertos_lr = (y_pred_lr_test == test_df['toxic']).sum()\n",
    "\n",
    "print(f\"Modelo 1 (BoW + LR) acertó: {num_aciertos_lr} de {len(test_df)} ({aciertos_lr:.2%})\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# EVALUACIÓN MODELO 2: TF-IDF + SVM \n",
    "# ==========================================\n",
    "\n",
    "# 1. Transformar el texto de TEST usando el vocabulario aprendido (TF-IDF)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(test_df['text_str'])\n",
    "\n",
    "# 2. Generar predicciones nuevas para el test\n",
    "y_pred_svm_test = clf_svm.predict(X_test_tfidf)\n",
    "\n",
    "# 3. Comparar predicción vs realidad\n",
    "aciertos_svm = accuracy_score(test_df['toxic'], y_pred_svm_test)\n",
    "num_aciertos_svm = (y_pred_svm_test == test_df['toxic']).sum()\n",
    "\n",
    "print(f\"Modelo 2 (TF-IDF + SVM) acertó: {num_aciertos_svm} de {len(test_df)} ({aciertos_svm:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7e5634",
   "metadata": {},
   "source": [
    "## 3. Preparación para Deep Learning\n",
    "Las redes neuronales requieren entradas de tamaño fijo. Analizamos la distribución de la longitud de las frases para decidir el tamaño de corte (`max_len`).\n",
    "Buscamos un valor que cubra la gran mayoría de los casos sin introducir demasiado relleno (padding) innecesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a318c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase más larga: 31 palabras\n",
      "Longitud media: 7.24 palabras\n",
      "El 95% de las frases miden menos de: 13.0 palabras\n",
      "El 99% de las frases miden menos de: 16.0 palabras\n",
      "Porcentaje de frases con 25 palabras o menos: 99.93%\n"
     ]
    }
   ],
   "source": [
    "longitudes = train_df['text_tokens'].apply(len)\n",
    "\n",
    "# Imprimiremos los datos básicos de las longitudes\n",
    "print(f\"Frase más larga: {longitudes.max()} palabras\")\n",
    "print(f\"Longitud media: {longitudes.mean():.2f} palabras\")\n",
    "print(f\"El 95% de las frases miden menos de: {longitudes.quantile(0.95)} palabras\")\n",
    "print(f\"El 99% de las frases miden menos de: {longitudes.quantile(0.99)} palabras\")\n",
    "\n",
    "#Verificaremos que 25 es una buena longitud para el padding/truncamiento (engloba más del 99.9% de las frases)\n",
    "porcentaje_englobado = ((longitudes <= 25).sum() / len(longitudes)) * 100\n",
    "print(f\"Porcentaje de frases con 25 palabras o menos: {porcentaje_englobado:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2893b31",
   "metadata": {},
   "source": [
    "Al final, hemos elegido (`max_len` = 25) puesto que contiene la mayor parte de las frases ( > 99.9%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54399eba",
   "metadata": {},
   "source": [
    "### Creación del Vocabulario y DataLoaders\n",
    "Transformamos el texto en secuencias numéricas:\n",
    "1. **Vocabulario:** Mapeamos cada palabra única a un índice entero. Usamos tokens especiales:\n",
    "   * `<PAD>` (0): Para rellenar frases cortas.\n",
    "   * `<UNK>` (1): Para palabras que no existen en el vocabulario de entrenamiento.\n",
    "2. **Dataset Pytorch:** Clase que convierte las frases en tensores y aplica el padding o truncmiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2734b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 3111\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "# Se construye el vocabulario a partir del conjunto de entrenamiento\n",
    "# Conteo de palabras \n",
    "word_counts = Counter()\n",
    "for tokens in train_df['text_tokens']:\n",
    "    word_counts.update(tokens)\n",
    "\n",
    "# <PAD>=0, <UNK>=1\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1} # <PAD> para padding (relleno), <UNK> para palabras desconocidas\n",
    "next_idx = 2\n",
    "min_freq = 2 # Ignorar palabras que aparecen solo 1 vez para reducir ruido\n",
    "\n",
    "# Solo se incluyen en el vocabulario las palabras que aparecen al menos 2 veces\n",
    "for word, count in word_counts.items():\n",
    "    if count >= min_freq:\n",
    "        vocab[word] = next_idx\n",
    "        next_idx += 1\n",
    "print(f\"Tamaño del vocabulario: {len(vocab)}\")\n",
    "\n",
    "# Se define la clase Dataset personalizada\n",
    "class ToxicityDataset(Dataset):\n",
    "    def __init__(self, dataframe, vocab, max_len=25):\n",
    "        self.data = dataframe.reset_index(drop=True)    # Se almacenarán los datos del DataFrame\n",
    "        self.vocab = vocab  # Se almacenará el vocabulario \n",
    "        self.max_len = max_len  # Longitud máxima para padding/truncamiento (25)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)   # Usado por DataLoader para saber el tamaño del dataset\n",
    "    \n",
    "    def __getitem__(self, idx): # Definir transformaciones para cada fila\n",
    "        tokens = self.data.loc[idx, 'text_tokens']\n",
    "        label = self.data.loc[idx, 'toxic']\n",
    "        \n",
    "        indices = [self.vocab.get(t, self.vocab[\"<UNK>\"]) for t in tokens] # Asigna índices, usa <UNK> si la palabra no está en el vocabulario\n",
    "        if len(indices) < self.max_len: # Padding, rellena con <PAD> si es más corto que max_len hasta alcanzar max_len\n",
    "            indices += [self.vocab[\"<PAD>\"]] * (self.max_len - len(indices))\n",
    "        else:   # Truncating, truncar si es más largo que max_len\n",
    "            indices = indices[:self.max_len]\n",
    "            \n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "# Crear Datasets y DataLoaders\n",
    "train_dataset = ToxicityDataset(train_df, vocab)\n",
    "val_dataset = ToxicityDataset(val_df, vocab)\n",
    "test_dataset = ToxicityDataset(test_df, vocab)\n",
    "\n",
    "# Hemos puesto shuffle=True para mezclar los datos en cada época y 128 de batch_size para acelerar el entrenamiento, con ese valor el modelo converge bien.\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True) \n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2c0f3",
   "metadata": {},
   "source": [
    "## 4. Arquitectura de la Red Neuronal (CNN 1D)\n",
    "\n",
    "* **Capa Embedding:** Convierte índices enteros en vectores densos.\n",
    "* **Filtros Convolucionales:** Ventanas de tamaño 3, 4 y 5. Esto permite a la red detectar patrones como trigramas (3 palabras seguidas) o secuencias de 5 palabras que denotan toxicidad.\n",
    "* **Max Pooling:** Extrae la característica más relevante de cada mapa de características.\n",
    "* **Salida:** Una neurona con activación `Sigmoid` (probabilidad entre 0 y 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad188947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "#Definimos la red neuronal CNN\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=200, pretrained_weights=None, freeze_embeddings=False):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        \n",
    "        # Capa de Embedding\n",
    "        if pretrained_weights is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_weights, freeze=freeze_embeddings)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "            \n",
    "        # Capas Convolucionales (detectan patrones de 3, 4 y 5 palabras), de esta forma no solo se consideran palabras individuales sino también combinaciones de 3, 4 y 5 palabras\n",
    "        # El número de filtros y canales se ha ajustado para equilibrar precisión y eficiencia, hemos probado con más pero el modelo memorizaba frases completas que conllevaban al overfitting\n",
    "        self.conv1 = nn.Conv1d(embed_dim, 100, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(embed_dim, 100, kernel_size=4, padding=2)\n",
    "        self.conv3 = nn.Conv1d(embed_dim, 100, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.fc = nn.Linear(300, 1) # 3 filtros * 100 canales\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout para evitar overfitting, 0.5 es el balance óptimo entre retener información y evitar sobreajuste\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_emb = self.embedding(x).permute(0, 2, 1) \n",
    "        \n",
    "        # Aplicar Convolución + ReLU + Max Pooling, permite detección patrones más relevantes para la clasificación, independientemente de dónde se encuentren en la frase\n",
    "        # La dimensión 2 representa la longitud de la frase, es la dimensión que queremos reducir\n",
    "        c1 = torch.max(torch.relu(self.conv1(x_emb)), dim=2)[0]\n",
    "        c2 = torch.max(torch.relu(self.conv2(x_emb)), dim=2)[0]\n",
    "        c3 = torch.max(torch.relu(self.conv3(x_emb)), dim=2)[0]\n",
    "        \n",
    "        # Concatenar características\n",
    "        out = torch.cat((c1, c2, c3), dim=1)\n",
    "        out = self.dropout(out)\n",
    "        return self.sigmoid(self.fc(out)).squeeze()\n",
    "\n",
    "# Función para entrenar y evaluar el modelo. En cada época se entrena y luego se evalúa en el conjunto de prueba. Finalmente, se implementa Early Stopping para evitar overfitting.\n",
    "def train_evaluate(model, train_loader, val_loader, epochs=20, lr=0.001, patience=3):\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device) # Mover modelo a GPU\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Variables para Early Stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    print(f\"Iniciando entrenamiento por {epochs} épocas con Early Stopping (paciencia={patience})...\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # ENTRENAMIENTO\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "            \n",
    "        # EVALUACIÓN (Validación)\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "                y_pred = model(X_batch)\n",
    "                # Calculamos loss de validación para decidir si parar\n",
    "                v_loss = criterion(y_pred, y_batch)\n",
    "                val_loss += v_loss.item()\n",
    "                \n",
    "                predicted = (y_pred > 0.5).float()\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        acc = correct / total\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {acc:.4f}\")\n",
    "        \n",
    "        # LOGICA DE EARLY STOPPING\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0 # Mejoramos, reseteamos contador\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"   > No mejora. Paciencia: {patience_counter}/{patience}\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(\"   > DETENIENDO ENTRENAMIENTO (Early Stopping triggered).\")\n",
    "                break\n",
    "    \n",
    "    # Restauramos el mejor modelo encontrado\n",
    "    print(f\"Restaurando el mejor modelo con Val Loss: {best_val_loss:.4f}\")\n",
    "    model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "450d6bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings cargados: 3109 palabras encontradas. 0 no encontradas.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Entrenamos el modelo Word2Vec con los datos de entrenamiento por 10 epochs\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=train_df['text_tokens'], # Frases\n",
    "    vector_size=200,                   # El tamaño del vector (embed_dim), balance entre precisión y eficiencia dado nuestro conjunto de datos de unas 6000 frases\n",
    "    window=5,                          # Mira 5 palabras a los lados, para análisis de frases cortas (como lo son las de las redes sociales) es el estándar, la mayor parte de las frases rondan las 7 palabras\n",
    "    min_count=2,                       # Ignora palabras que solo salgan 1 vez\n",
    "    workers=4,                         # Número de hilos para entrenamiento paralelo\n",
    "    epochs=10               \n",
    ")\n",
    "\n",
    "# Creamos la \"Matriz de Pesos\" para pasársela a la CNN\n",
    "def create_embedding_matrix(vocab, w2v_model, embed_dim=200):\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = np.random.normal(scale=0.6, size=(vocab_size, embed_dim))\n",
    "    \n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    \n",
    "    for word, idx in vocab.items():\n",
    "        if idx < 2: continue \n",
    "        if word in w2v_model.wv:\n",
    "            embedding_matrix[idx] = w2v_model.wv[word]\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "            \n",
    "    print(f\"Embeddings cargados: {hits} palabras encontradas. {misses} no encontradas.\")\n",
    "    return torch.FloatTensor(embedding_matrix)\n",
    "\n",
    "# Creamos la \"Matriz de Pesos\" para pasársela a la CNN\n",
    "w2v_weights = create_embedding_matrix(vocab, w2v_model, embed_dim=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d3afce",
   "metadata": {},
   "source": [
    "## 5. Experimentos y Entrenamiento\n",
    "Evaluamos las tres estrategias de inicialización para la CNN con el conjunto de Validation:\n",
    "\n",
    "1. **From Scratch:** Embeddings inicializados aleatoriamente. La red aprende vocabulario y clasificación simultáneamente.\n",
    "2. **W2V Frozen:** Usamos los pesos de Word2Vec y los congelamos. La red no puede modificar los embeddings, solo aprende a combinarlos.\n",
    "3. **W2V Fine-Tuned:** Usamos Word2Vec como punto de partida, pero permitimos que la red ajuste (fine-tune) los embeddings durante el entrenamiento para optimizarlos para la tarea de toxicidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28933923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento por 20 épocas con Early Stopping (paciencia=3)...\n",
      "Epoch 1/20 | Train Loss: 0.5781 | Val Loss: 0.4452 | Val Acc: 0.7881\n",
      "Epoch 2/20 | Train Loss: 0.3509 | Val Loss: 0.3579 | Val Acc: 0.8379\n",
      "Epoch 3/20 | Train Loss: 0.2438 | Val Loss: 0.3189 | Val Acc: 0.8453\n",
      "Epoch 4/20 | Train Loss: 0.1661 | Val Loss: 0.2970 | Val Acc: 0.8633\n",
      "Epoch 5/20 | Train Loss: 0.1130 | Val Loss: 0.2848 | Val Acc: 0.8644\n",
      "Epoch 6/20 | Train Loss: 0.0831 | Val Loss: 0.2894 | Val Acc: 0.8676\n",
      "   > No mejora. Paciencia: 1/3\n",
      "Epoch 7/20 | Train Loss: 0.0552 | Val Loss: 0.2966 | Val Acc: 0.8761\n",
      "   > No mejora. Paciencia: 2/3\n",
      "Epoch 8/20 | Train Loss: 0.0394 | Val Loss: 0.3086 | Val Acc: 0.8697\n",
      "   > No mejora. Paciencia: 3/3\n",
      "   > DETENIENDO ENTRENAMIENTO (Early Stopping triggered).\n",
      "Restaurando el mejor modelo con Val Loss: 0.2848\n",
      "Iniciando entrenamiento por 20 épocas con Early Stopping (paciencia=3)...\n",
      "Epoch 1/20 | Train Loss: 0.7160 | Val Loss: 0.6832 | Val Acc: 0.5773\n",
      "Epoch 2/20 | Train Loss: 0.6802 | Val Loss: 0.6732 | Val Acc: 0.5773\n",
      "Epoch 3/20 | Train Loss: 0.6760 | Val Loss: 0.6719 | Val Acc: 0.5932\n",
      "Epoch 4/20 | Train Loss: 0.6756 | Val Loss: 0.6864 | Val Acc: 0.5847\n",
      "   > No mejora. Paciencia: 1/3\n",
      "Epoch 5/20 | Train Loss: 0.6801 | Val Loss: 0.6723 | Val Acc: 0.6049\n",
      "   > No mejora. Paciencia: 2/3\n",
      "Epoch 6/20 | Train Loss: 0.6706 | Val Loss: 0.6716 | Val Acc: 0.6006\n",
      "Epoch 7/20 | Train Loss: 0.6718 | Val Loss: 0.6684 | Val Acc: 0.5869\n",
      "Epoch 8/20 | Train Loss: 0.6700 | Val Loss: 0.6679 | Val Acc: 0.5890\n",
      "Epoch 9/20 | Train Loss: 0.6685 | Val Loss: 0.6713 | Val Acc: 0.5816\n",
      "   > No mejora. Paciencia: 1/3\n",
      "Epoch 10/20 | Train Loss: 0.6677 | Val Loss: 0.6669 | Val Acc: 0.5964\n",
      "Epoch 11/20 | Train Loss: 0.6690 | Val Loss: 0.6733 | Val Acc: 0.5752\n",
      "   > No mejora. Paciencia: 1/3\n",
      "Epoch 12/20 | Train Loss: 0.6631 | Val Loss: 0.6700 | Val Acc: 0.6028\n",
      "   > No mejora. Paciencia: 2/3\n",
      "Epoch 13/20 | Train Loss: 0.6638 | Val Loss: 0.6868 | Val Acc: 0.5879\n",
      "   > No mejora. Paciencia: 3/3\n",
      "   > DETENIENDO ENTRENAMIENTO (Early Stopping triggered).\n",
      "Restaurando el mejor modelo con Val Loss: 0.6669\n",
      "Iniciando entrenamiento por 20 épocas con Early Stopping (paciencia=3)...\n",
      "Epoch 1/20 | Train Loss: 0.6945 | Val Loss: 0.6626 | Val Acc: 0.6377\n",
      "Epoch 2/20 | Train Loss: 0.6298 | Val Loss: 0.5652 | Val Acc: 0.7574\n",
      "Epoch 3/20 | Train Loss: 0.4259 | Val Loss: 0.3436 | Val Acc: 0.8379\n",
      "Epoch 4/20 | Train Loss: 0.2135 | Val Loss: 0.2475 | Val Acc: 0.8888\n",
      "Epoch 5/20 | Train Loss: 0.1185 | Val Loss: 0.1842 | Val Acc: 0.9280\n",
      "Epoch 6/20 | Train Loss: 0.0673 | Val Loss: 0.1696 | Val Acc: 0.9354\n",
      "Epoch 7/20 | Train Loss: 0.0437 | Val Loss: 0.1856 | Val Acc: 0.9206\n",
      "   > No mejora. Paciencia: 1/3\n",
      "Epoch 8/20 | Train Loss: 0.0303 | Val Loss: 0.1901 | Val Acc: 0.9301\n",
      "   > No mejora. Paciencia: 2/3\n",
      "Epoch 9/20 | Train Loss: 0.0222 | Val Loss: 0.2019 | Val Acc: 0.9248\n",
      "   > No mejora. Paciencia: 3/3\n",
      "   > DETENIENDO ENTRENAMIENTO (Early Stopping triggered).\n",
      "Restaurando el mejor modelo con Val Loss: 0.1696\n"
     ]
    }
   ],
   "source": [
    "# PREPARACIÓN DE EMBEDDINGS\n",
    "embed_dim = 200\n",
    "\n",
    "# A: CNN FROM SCRATCH\n",
    "model_scratch = CNNClassifier(len(vocab), embed_dim, pretrained_weights=None)\n",
    "train_evaluate(model_scratch, train_loader, val_loader)\n",
    "\n",
    "# B: CNN W2V FROZEN\n",
    "model_frozen = CNNClassifier(len(vocab), embed_dim, pretrained_weights=w2v_weights, freeze_embeddings=True)\n",
    "train_evaluate(model_frozen, train_loader, val_loader)\n",
    "\n",
    "# C: CNN W2V FINE-TUNED\n",
    "model_finetuned = CNNClassifier(len(vocab), embed_dim, pretrained_weights=w2v_weights, freeze_embeddings=False)\n",
    "train_evaluate(model_finetuned, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d2cc744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM SCRATCH RESULTS:\n",
      "Aciertos: 824 de 944\n",
      "Precisión (Accuracy): 87.29%\n",
      "\n",
      "W2V FROZEN RESULTS:\n",
      "Aciertos: 605 de 944\n",
      "Precisión (Accuracy): 64.09%\n",
      "\n",
      "W2V FINE-TUNED RESULTS:\n",
      "Aciertos: 849 de 944\n",
      "Precisión (Accuracy): 89.94%\n",
      "\n",
      "PRUEBA CON FRASES NUEVAS - W2V FINE-TUNED:\n",
      "\n",
      "Frase: 'I love this beautiful sunny day'\n",
      "Predicción: 0-NEUTRAL (Probabilidad: 0.0070)\n",
      "\n",
      "Frase: 'You are a stupid idiot and I hate you'\n",
      "Predicción: 1-TÓXICA (Probabilidad: 1.0000)\n",
      "\n",
      "Frase: 'Shut up you fool'\n",
      "Predicción: 1-TÓXICA (Probabilidad: 0.9969)\n",
      "\n",
      "Frase: 'You look like you belong in a zoo.'\n",
      "Predicción: 1-TÓXICA (Probabilidad: 0.6936)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creamos una función para clasificar frases nuevas usando el modelo CNN fine-tuned\n",
    "def clasificar_frase_cnn(frase, model, vocab, max_len=25):\n",
    "    model.eval() \n",
    "    tokens = frase.lower().split() # Tokenizamos la frase (simple split por espacios)\n",
    "    \n",
    "    indices = [vocab.get(t, vocab[\"<UNK>\"]) for t in tokens]    # Asigna índices, usa <UNK> si la palabra no está en el vocabulario\n",
    "    if len(indices) < max_len:\n",
    "        indices += [vocab[\"<PAD>\"]] * (max_len - len(indices))  # Padding para que todas las frases tengan la misma longitud (25)\n",
    "    else:\n",
    "        indices = indices[:max_len]\n",
    "        \n",
    "    tensor_input = torch.tensor(indices, dtype=torch.long).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        probabilidad = model(tensor_input).item()   # Obtener la probabilidad de toxicidad\n",
    "    etiqueta = 1 if probabilidad > 0.5 else 0   # Asignar etiqueta basada en un umbral de 0.5 (si probabilidad > 0.5, es tóxico, sino neutral)\n",
    "\n",
    "    return etiqueta, probabilidad   # Se devuelve la etiqueta y la probabilidad\n",
    "\n",
    "# EVALUACIÓN DE LA CNN EN EL CONJUNTO DE TEST (test_df) - FROM SCRATCH\n",
    "model_scratch.eval() \n",
    "\n",
    "aciertos = 0\n",
    "total_muestras = len(val_df)\n",
    "\n",
    "for texto, etiqueta_real in zip(test_df['text_str'], test_df['toxic']):\n",
    "    prediccion_etiqueta, _ = clasificar_frase_cnn(texto, model_scratch, vocab)\n",
    "    if prediccion_etiqueta == etiqueta_real:\n",
    "        aciertos += 1\n",
    "\n",
    "accuracy = aciertos / total_muestras\n",
    "print(\"FROM SCRATCH RESULTS:\")\n",
    "print(f\"Aciertos: {aciertos} de {total_muestras}\")\n",
    "print(f\"Precisión (Accuracy): {accuracy:.2%}\\n\")\n",
    "\n",
    "# EVALUACIÓN DE LA CNN EN EL CONJUNTO DE TEST (test_df) - W2V FROZEN\n",
    "model_frozen.eval() \n",
    "\n",
    "aciertos = 0\n",
    "total_muestras = len(test_df)\n",
    "\n",
    "for texto, etiqueta_real in zip(test_df['text_str'], test_df['toxic']):\n",
    "    prediccion_etiqueta, _ = clasificar_frase_cnn(texto, model_frozen, vocab)\n",
    "    if prediccion_etiqueta == etiqueta_real:\n",
    "        aciertos += 1\n",
    "\n",
    "accuracy = aciertos / total_muestras\n",
    "print(\"W2V FROZEN RESULTS:\")\n",
    "print(f\"Aciertos: {aciertos} de {total_muestras}\")\n",
    "print(f\"Precisión (Accuracy): {accuracy:.2%}\\n\")\n",
    "\n",
    "\n",
    "# EVALUACIÓN DE LA CNN EN EL CONJUNTO DE TEST (val_df) - W2V FINE-TUNED\n",
    "model_finetuned.eval() \n",
    "\n",
    "aciertos = 0\n",
    "total_muestras = len(test_df)\n",
    "\n",
    "for texto, etiqueta_real in zip(test_df['text_str'], test_df['toxic']):\n",
    "    prediccion_etiqueta, _ = clasificar_frase_cnn(texto, model_finetuned, vocab)\n",
    "    if prediccion_etiqueta == etiqueta_real:\n",
    "        aciertos += 1\n",
    "\n",
    "accuracy = aciertos / total_muestras\n",
    "print(\"W2V FINE-TUNED RESULTS:\")\n",
    "print(f\"Aciertos: {aciertos} de {total_muestras}\")\n",
    "print(f\"Precisión (Accuracy): {accuracy:.2%}\\n\")\n",
    "\n",
    "# PRUEBA CON FRASES NUEVAS - W2V FINE-TUNED\n",
    "frases_nuevas = [\n",
    "    \"I love this beautiful sunny day\",\n",
    "    \"You are a stupid idiot and I hate you\",\n",
    "    \"Shut up you fool\",\n",
    "    \"You look like you belong in a zoo.\",\n",
    "]\n",
    "print(\"PRUEBA CON FRASES NUEVAS - W2V FINE-TUNED:\\n\")\n",
    "for f in frases_nuevas:\n",
    "    etiqueta, probabilidad = clasificar_frase_cnn(f, model_finetuned, vocab)\n",
    "    print(f\"Frase: '{f}'\")\n",
    "    if etiqueta == 0:\n",
    "        print(f\"Predicción: {etiqueta}-NEUTRAL (Probabilidad: {probabilidad:.4f})\\n\")\n",
    "    else:\n",
    "        print(f\"Predicción: {etiqueta}-TÓXICA (Probabilidad: {probabilidad:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1c315",
   "metadata": {},
   "source": [
    "## 6. Transfer Learning con GloVe\n",
    "Finalmente, intentamos mejorar el modelo utilizando **GloVe (Global Vectors for Word Representation)**.\n",
    "A diferencia de nuestro Word2Vec (entrenado con ~5000 frases), GloVe ha sido entrenado con millones de textos de Wikipedia y la web.\n",
    "\n",
    "* **Estrategia:** Cargamos los vectores de 100 dimensiones.\n",
    "* **Manejo de desconocidas:** Las palabras que no están en GloVe se inicializan con una distribución normal aleatoria para que la red pueda aprenderlas desde cero durante el Fine-Tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eef121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287614 vectores válidos.\n",
      "Bad lines: 3533 .\n",
      "3059 palabras encontradas. 50 palabras no encontradas.\n",
      "Iniciando entrenamiento por 20 épocas con Early Stopping (paciencia=3)...\n",
      "Epoch 1/20 | Train Loss: 0.4515 | Val Loss: 0.3212 | Val Acc: 0.8633\n",
      "Epoch 2/20 | Train Loss: 0.2288 | Val Loss: 0.2279 | Val Acc: 0.9078\n",
      "Epoch 3/20 | Train Loss: 0.1654 | Val Loss: 0.1993 | Val Acc: 0.9227\n",
      "Epoch 4/20 | Train Loss: 0.1274 | Val Loss: 0.1871 | Val Acc: 0.9227\n",
      "Epoch 5/20 | Train Loss: 0.0946 | Val Loss: 0.1834 | Val Acc: 0.9227\n",
      "Epoch 6/20 | Train Loss: 0.0749 | Val Loss: 0.1796 | Val Acc: 0.9290\n",
      "Epoch 7/20 | Train Loss: 0.0588 | Val Loss: 0.1821 | Val Acc: 0.9227\n",
      "   > No mejora. Paciencia: 1/3\n",
      "Epoch 8/20 | Train Loss: 0.0474 | Val Loss: 0.1914 | Val Acc: 0.9237\n",
      "   > No mejora. Paciencia: 2/3\n",
      "Epoch 9/20 | Train Loss: 0.0369 | Val Loss: 0.1814 | Val Acc: 0.9375\n",
      "   > No mejora. Paciencia: 3/3\n",
      "   > DETENIENDO ENTRENAMIENTO (Early Stopping triggered).\n",
      "Restaurando el mejor modelo con Val Loss: 0.1796\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# CONFIGURACIÓN\n",
    "GLOVE_FILE = \"glove100b.txt\" \n",
    "GLOVE_DIM = 100  \n",
    "\n",
    "# Función para crear la matriz de embeddings a partir del archivo GloVe\n",
    "def create_glove_matrix(vocab, glove_path, embed_dim=100):\n",
    "\n",
    "    embeddings_index = {}\n",
    "    bad_lines = 0  # Contador para líneas corruptas\n",
    "    \n",
    "    # Cargar el archivo GloVe\n",
    "    with open(glove_path, encoding=\"utf8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            values = line.split()\n",
    "            if len(values) != embed_dim + 1:   # Línea corrupta\n",
    "                bad_lines += 1\n",
    "                continue\n",
    "            word = values[0]\n",
    "            # Extraer el vector de embedding\n",
    "            try:\n",
    "                coefs = np.asarray(values[1:], dtype='float32') # Convertir a array numpy\n",
    "                embeddings_index[word] = coefs  # Guardar en el diccionario\n",
    "            except ValueError:\n",
    "                bad_lines += 1\n",
    "                continue\n",
    "\n",
    "    print(f\"{len(embeddings_index)} vectores válidos.\")\n",
    "    print(f\"Bad lines: {bad_lines} .\")\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = np.random.normal(scale=0.6, size=(vocab_size, embed_dim))\n",
    "    \n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    # Rellenar la matriz de embeddings\n",
    "    for word, idx in vocab.items():\n",
    "        if idx < 2: continue  # Saltar índices especiales\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector    # Rellenar la matriz\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "            \n",
    "    print(f\"{hits} palabras encontradas. {misses} palabras no encontradas.\")\n",
    "    return torch.FloatTensor(embedding_matrix)\n",
    "\n",
    "# Usamos la función \n",
    "glove_weights = create_glove_matrix(vocab, GLOVE_FILE, embed_dim=GLOVE_DIM)\n",
    "model_glove_finetuned = CNNClassifier(\n",
    "    vocab_size=len(vocab), \n",
    "    embed_dim=GLOVE_DIM, \n",
    "     pretrained_weights=glove_weights, \n",
    "    freeze_embeddings=False  \n",
    ")   \n",
    "train_evaluate(model_glove_finetuned, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25e99378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOVE FINE-TUNED RESULTS:\n",
      "Aciertos: 879 de 944\n",
      "Precisión (Accuracy): 93.11%\n",
      "\n",
      "Frase: 'I love this beautiful sunny day'\n",
      "Predicción: 0-NEUTRAL (Probabilidad: 0.0059)\n",
      "\n",
      "Frase: 'You are a stupid idiot and I hate you'\n",
      "Predicción: 1-TÓXICA (Probabilidad: 1.0000)\n",
      "\n",
      "Frase: 'Shut up you fool'\n",
      "Predicción: 1-TÓXICA (Probabilidad: 0.9960)\n",
      "\n",
      "Frase: 'You look like you belong in a zoo.'\n",
      "Predicción: 1-TÓXICA (Probabilidad: 0.5041)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUACIÓN DE LA CNN EN EL CONJUNTO DE TEST (test_df) - W2V FINE-TUNED\n",
    "model_glove_finetuned.eval() \n",
    "\n",
    "aciertos = 0\n",
    "total_muestras = len(test_df)\n",
    "\n",
    "for texto, etiqueta_real in zip(test_df['text_str'], test_df['toxic']):\n",
    "    prediccion_etiqueta, _ = clasificar_frase_cnn(texto, model_glove_finetuned, vocab)\n",
    "    if prediccion_etiqueta == etiqueta_real:\n",
    "        aciertos += 1\n",
    "\n",
    "accuracy = aciertos / total_muestras\n",
    "print(\"GLOVE FINE-TUNED RESULTS:\")\n",
    "print(f\"Aciertos: {aciertos} de {total_muestras}\")\n",
    "print(f\"Precisión (Accuracy): {accuracy:.2%}\\n\")\n",
    "\n",
    "# PRUEBA CON FRASES NUEVAS - GLOVE FINE-TUNED\n",
    "frases_nuevas = [\n",
    "    \"I love this beautiful sunny day\",\n",
    "    \"You are a stupid idiot and I hate you\",\n",
    "    \"Shut up you fool\",\n",
    "    \"You look like you belong in a zoo.\",\n",
    "]\n",
    "for f in frases_nuevas:\n",
    "    etiqueta, probabilidad = clasificar_frase_cnn(f, model_glove_finetuned, vocab)\n",
    "    print(f\"Frase: '{f}'\")\n",
    "    if etiqueta == 0:\n",
    "        print(f\"Predicción: {etiqueta}-NEUTRAL (Probabilidad: {probabilidad:.4f})\\n\")\n",
    "    else:\n",
    "        print(f\"Predicción: {etiqueta}-TÓXICA (Probabilidad: {probabilidad:.4f})\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfa9c7f",
   "metadata": {},
   "source": [
    "## 7. Embeddings Contextuales\n",
    "\n",
    "- bert-base-multilingual-cased\n",
    "- distilbert-base-uncased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f7393c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\js834\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "# Adapta los outputs del tokenizer de Hugging Face al DataLoader de PyTorch (2 elementos).\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx].astype(np.float32)) \n",
    "        \n",
    "        # Devolvemos solo INPUT_IDS y LABELS\n",
    "        return item['input_ids'], item['labels'] \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "# Esta clase intercepta la salida del modelo de Hugging Face (SequenceClassifierOutput) y la convierte en un tensor simple, para poder pasarsela a train_evaluate.\n",
    "class BertClassifierWrapper(nn.Module):\n",
    "    def __init__(self, hf_model):\n",
    "        super().__init__()\n",
    "        self.hf_model = hf_model\n",
    "\n",
    "        # Adaptamos la capa final del clasificador para incluir la Sigmoid (para BCELoss)\n",
    "        original_classifier = self.hf_model.classifier \n",
    "        self.hf_model.classifier = torch.nn.Sequential(\n",
    "            original_classifier,\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, input_ids):\n",
    "        attention_mask = (input_ids != 0).long()\n",
    "        output = self.hf_model(input_ids=input_ids, attention_mask=attention_mask) \n",
    "        return output.logits.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1845c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128 \n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2 # Hemos usado solo 2 epochs, por que sino se llegaba al overfit muy rapido y la prueba en el Test set salia incorrecta (seleccionaba al azar una de las dos clases)\n",
    "\n",
    "# Función para ejecutar el entrenamiento y evaluación usando un modelo de Hugging Face con PyTorch\n",
    "def execute_and_evaluate_pytorch_hf(model_name_hf, description, X_train, y_train, X_test, y_test, train_evaluate_func):\n",
    "    \n",
    "    # 1. Tokenización \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_hf)\n",
    "    X_train_encoded = tokenizer(\n",
    "        X_train.tolist(), max_length=MAX_LEN, truncation=True, padding='max_length', return_tensors='pt'\n",
    "    )\n",
    "    X_test_encoded = tokenizer(\n",
    "        X_test.tolist(), max_length=MAX_LEN, truncation=True, padding='max_length', return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # 2. Preparación de DataLoaders (2 elementos) \n",
    "    train_dataset = TextClassificationDataset(X_train_encoded, y_train)\n",
    "    test_dataset = TextClassificationDataset(X_test_encoded, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # 3. Carga y Envoltura del Modelo (The Fix) \n",
    "    hf_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name_hf, \n",
    "        num_labels=1\n",
    "    )\n",
    "    model = BertClassifierWrapper(hf_model)\n",
    "\n",
    "    # 4. ENTRENAMIENTO (Llama a tu función sin modificar) \n",
    "    print(f\"Iniciando Fine-Tuning de {description} usando la función 'train_evaluate'...\")\n",
    "    train_evaluate_func(model, train_loader, val_loader, epochs=EPOCHS, lr=2e-6) # Con mayor lr, overfiteaba muy rapido los modelos\n",
    "\n",
    "    # 5. EVALUACIÓN FINAL \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader: # Solo desempaquetamos 2 valores\n",
    "            # La llamada a model(X_batch) ejecuta BertClassifierWrapper.forward()\n",
    "            y_pred = model(X_batch) \n",
    "            \n",
    "            predicted = (y_pred > 0.5).float() \n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    aciertos = correct\n",
    "    total_muestras = total\n",
    "    accuracy = aciertos / total_muestras\n",
    "    \n",
    "    print(f\"\\n{description} RESULTS:\")\n",
    "    print(f\"Aciertos: {aciertos} de {total_muestras}\")\n",
    "    print(f\"Precisión (Accuracy): {accuracy:.2%}\\n\")\n",
    "    \n",
    "    return model, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34dd95d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Fine-Tuning de BERT_Multilingual usando la función 'train_evaluate'...\n",
      "Iniciando entrenamiento por 2 épocas con Early Stopping (paciencia=3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\js834\\AppData\\Local\\Temp\\ipykernel_26108\\1674711031.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 | Train Loss: 0.5384 | Val Loss: 0.3415 | Val Acc: 0.8665\n",
      "Epoch 2/2 | Train Loss: 0.3069 | Val Loss: 0.2590 | Val Acc: 0.8962\n",
      "Restaurando el mejor modelo con Val Loss: 0.2590\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m model_bert_base_cased \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m desc, model_hf \u001b[38;5;129;01min\u001b[39;00m MODELOS_CONTEXTUALES\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 20\u001b[0m     current_model, acc \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_and_evaluate_pytorch_hf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_hf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_evaluate\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Para guardar los modelos\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m desc \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistilBERT_Uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[1;32mIn[16], line 44\u001b[0m, in \u001b[0;36mexecute_and_evaluate_pytorch_hf\u001b[1;34m(model_name_hf, description, X_train, y_train, X_test, y_test, train_evaluate_func)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m val_loader: \u001b[38;5;66;03m# Solo desempaquetamos 2 valores\u001b[39;00m\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;66;03m# La llamada a model(X_batch) ejecuta BertClassifierWrapper.forward()\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     46\u001b[0m         predicted \u001b[38;5;241m=\u001b[39m (y_pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat() \n\u001b[0;32m     47\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\js834\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\js834\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[15], line 39\u001b[0m, in \u001b[0;36mBertClassifierWrapper.forward\u001b[1;34m(self, input_ids)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids):\n\u001b[0;32m     38\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m (input_ids \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m---> 39\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[1;32mc:\\Users\\js834\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\js834\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\js834\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1482\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1475\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1479\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1480\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1482\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1494\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1496\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\js834\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\js834\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\js834\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:936\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    934\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m--> 936\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    945\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((batch_size, seq_length \u001b[38;5;241m+\u001b[39m past_key_values_length), device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\js834\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\js834\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\js834\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:179\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    176\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    182\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
      "File \u001b[1;32mc:\\Users\\js834\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\js834\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\js834\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\js834\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "# Usaremos estos dos modelos de Hugging Face\n",
    "MODELOS_CONTEXTUALES = {\n",
    "    \"BERT_Multilingual\": \"bert-base-multilingual-cased\",\n",
    "    \"DistilBERT_Uncased\": \"distilbert-base-uncased\"\n",
    "}\n",
    "TEXT_COLUMN = 'text_str'\n",
    "LABEL_COLUMN = 'toxic'\n",
    "\n",
    "X_train = train_df[TEXT_COLUMN]\n",
    "y_train = train_df[LABEL_COLUMN]\n",
    "\n",
    "X_test = val_df[TEXT_COLUMN]\n",
    "y_test = val_df[LABEL_COLUMN]\n",
    "resultados_contextuales_pt = {}\n",
    "\n",
    "model_distilbert_uncased = None\n",
    "model_bert_base_cased = None\n",
    "\n",
    "for desc, model_hf in MODELOS_CONTEXTUALES.items():\n",
    "    current_model, acc = execute_and_evaluate_pytorch_hf(\n",
    "        model_hf, \n",
    "        desc, \n",
    "        X_train, \n",
    "        y_train, \n",
    "        X_test, \n",
    "        y_test, \n",
    "        train_evaluate \n",
    "    )\n",
    "    # Para guardar los modelos\n",
    "    if desc == \"DistilBERT_Uncased\":\n",
    "        model_distilbert_uncased = current_model\n",
    "    else:\n",
    "        model_bert_base_cased = current_model\n",
    "    \n",
    "    resultados_contextuales_pt[desc] = {'Accuracy': acc}\n",
    "\n",
    "# --- RESUMEN FINAL ---\n",
    "resultados_df = pd.DataFrame(resultados_contextuales_pt).T\n",
    "print(resultados_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b680677",
   "metadata": {},
   "source": [
    "## 8. Análisis de Resultados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f3dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "resultados = {\n",
    "    'Modelo': [\n",
    "        'BoW + Logistic Reg.', \n",
    "        'TF-IDF + SVM', \n",
    "        'CNN (Scratch)', \n",
    "        'CNN (W2V Frozen)', \n",
    "        'CNN (W2V Fine-Tuned)', \n",
    "        'CNN (GloVe Fine-Tuned)',\n",
    "        'BERT Multilingual',\n",
    "        'DistilBERT Uncased'\n",
    "    ],\n",
    "    'Accuracy': [\n",
    "        0.5117,  # BoW + Logistic Regression\n",
    "        0.5021,  # TF-IDF + SVM\n",
    "        0.8708,  # CNN Scratch\n",
    "        0.6515,  # CNN Frozen \n",
    "        0.9057,  # CNN W2V Fine-Tuned\n",
    "        0.9269,  # CNN GloVe \n",
    "        0.8792,  # BERT Multilingual\n",
    "        0.9025   # DistilBERT Uncased\n",
    "    ],\n",
    "    'Tipo': ['Clásico', 'Clásico', 'Deep Learning', 'Deep Learning', 'Deep Learning', 'Deep Learning', 'Contextual', 'Contextual']\n",
    "}\n",
    "\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "display(df_resultados.sort_values(by='Accuracy', ascending=False))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_resultados, x='Modelo', y='Accuracy', hue='Tipo', palette='viridis', legend=False)\n",
    "plt.ylim(0.5, 1.0) \n",
    "plt.title('Comparativa de Precisión (Accuracy) entre Modelos', fontsize=15)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fa0734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis del mejor modelo - CNN (GloVe Fine-Tuned)\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluar_modelo_final(model, test_loader):\n",
    "\n",
    "    model.eval()  \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for X_batch, y_batch in test_loader:\n",
    "            \n",
    "            # La salida del modelo ya es un tensor de probabilidad gracias al Wrapper\n",
    "            outputs = model(X_batch)\n",
    "            \n",
    "            # Convertir probabilidad a clase (0 o 1)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            \n",
    "            # Guardar resultados (móviles de vuelta a la CPU para numpy/sklearn)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # 3. Calcular métricas con Scikit-Learn\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.2%}\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=['Neutral', 'Tóxico']))\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Neutral', 'Tóxico'], yticklabels=['Neutral', 'Tóxico'])\n",
    "    plt.ylabel('Realidad')\n",
    "    plt.xlabel('Predicción')\n",
    "    plt.title('Matriz de Confusión - Test Set: CNN (GloVe Fine-Tuned)')\n",
    "    plt.show()\n",
    "    \n",
    "    return acc\n",
    "\n",
    "acc_distilbert = evaluar_modelo_final(model_glove_finetuned, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d0590e",
   "metadata": {},
   "source": [
    "## 9. Conclusiones\n",
    "\n",
    "Tras evaluar las diferentes estrategias diferentes, hemos llegado a las siguientes conclusiones.\n",
    "\n",
    "### 1. El Ganador: Transfer Learning (CNN + GloVe)\n",
    "El mejor rendimiento se obtuvo con **CNN + GloVe Fine-Tuned**, alcanzando una precisión del **92.69%**.\n",
    "Esto demuestra el poder del **Transfer Learning**. Al utilizar embeddings pre-entrenados en millones de textos globales y ajustarlos a nuestro problema, el modelo logró captar matices que el entrenamiento solo con nuestro dataset pequeño no permitía.\n",
    "\n",
    "### 2. Los Modelos Clásicos\n",
    "Los modelos base (TF-IDF + SVM y Regresión Logística) mostraron un rendimiento muy pobre, con precisiones cercanas a 0.50 (50.0%), que es practicamente adivinar al azar, esto indica que los modelos clásicos no lograron generalizar.\n",
    "\n",
    "### 3. La Importancia Vital del Fine-Tuning\n",
    "El hallazgo más crítico del experimento está en la comparación de Word2Vec:\n",
    "* **W2V Frozen (65.15%):** Rendimiento muy pobre. Los embeddings estáticos no se adaptaron bien a la red convolucional.\n",
    "* **W2V Fine-Tuned (90.57%):** Al permitir que la red ajustara los pesos de los embeddings, el rendimiento se disparó más de un **25%**.\n",
    "* **Conclusión:** Para tareas específicas de NLP, **es obligatorio descongelar (unfreeze)** los embeddings para permitir que el modelo adapte el significado de las palabras al contexto específico.\n",
    "\n",
    "### 4. La Sensibilidad Extrema de los Modelos Contextuales\n",
    "La estrategia final, basada en Fine-Tuning de Transformadores (BERT/DistilBERT), no logró el liderazgo, a pesar de su complejidad y coste computacional. Al contar con un conjunto de datos limitado y habiendo una clara componente léxica en la clasificación, BERT no fue capaz de aportar una ventaja significativa.\n",
    "\n",
    "### Resumen Final\n",
    "Hemos logrado pasar de una línea base del **50%** (Regresión Logística) a un modelo de Deep Learning robusto del **92.69%** (CNN+GloVe), demostrando que la combinación de redes neuronales con conocimiento pre-entrenado es la estrategia óptima para resolver este problema."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
