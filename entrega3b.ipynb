{
 "cells": [
  {
   "cell_type": "code",
   "id": "9b4539e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:05:10.806947Z",
     "start_time": "2025-11-24T16:05:10.766574Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import ast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuración\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_LEN = 25  # Basado en tu análisis anterior (99% < 16 palabras)\n",
    "\n",
    "print(f\"=== INICIO TAREA B: DETOXIFICACIÓN CON ATENCIÓN ===\")\n",
    "print(f\"Dispositivo: {device}\")\n",
    "\n",
    "# 1. Cargar Datos (Solo ParaDetox)\n",
    "df_detox = pd.read_csv(\"datos/english_paradetox_preprocessed_non_lemmatized.csv\")\n",
    "\n",
    "# Convertir strings a listas\n",
    "df_detox['toxic_sentence_tokens'] = df_detox['toxic_sentence_tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df_detox['neutral_sentence_tokens'] = df_detox['neutral_sentence_tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "print(f\"Total de pares tóxico-neutral: {len(df_detox)}\")\n",
    "\n",
    "# 2. Vocabulario (Tokens especiales + Palabras)\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2index = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.index2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.n_words = 4\n",
    "\n",
    "    def add_sentence(self, tokens):\n",
    "        for word in tokens:\n",
    "            if word not in self.word2index:\n",
    "                self.word2index[word] = self.n_words\n",
    "                self.index2word[self.n_words] = word\n",
    "                self.n_words += 1\n",
    "\n",
    "# Crear vocabulario compartido\n",
    "vocab = Vocabulary()\n",
    "for tokens in df_detox['toxic_sentence_tokens']: vocab.add_sentence(tokens)\n",
    "for tokens in df_detox['neutral_sentence_tokens']: vocab.add_sentence(tokens)\n",
    "\n",
    "print(f\"Tamaño del vocabulario: {vocab.n_words}\")\n",
    "\n",
    "# 3. Dataset y DataLoader\n",
    "class DetoxDataset(Dataset):\n",
    "    def __init__(self, df, vocab, max_len=25):\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def text_to_indices(self, tokens):\n",
    "        indices = [self.vocab.word2index.get(t, 3) for t in tokens] # 3=<UNK>\n",
    "        indices.append(2) # <EOS> al final\n",
    "        return indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_tokens = self.df.iloc[idx]['toxic_sentence_tokens']\n",
    "        trg_tokens = self.df.iloc[idx]['neutral_sentence_tokens']\n",
    "\n",
    "        src_indices = self.text_to_indices(src_tokens)\n",
    "        trg_indices = self.text_to_indices(trg_tokens)\n",
    "        \n",
    "        # Padding Manual\n",
    "        if len(src_indices) < self.max_len:\n",
    "            src_indices += [0] * (self.max_len - len(src_indices))\n",
    "        else:\n",
    "            src_indices = src_indices[:self.max_len]\n",
    "            \n",
    "        if len(trg_indices) < self.max_len:\n",
    "            trg_indices += [0] * (self.max_len - len(trg_indices))\n",
    "        else:\n",
    "            trg_indices = trg_indices[:self.max_len]\n",
    "\n",
    "        return torch.tensor(src_indices, dtype=torch.long), torch.tensor(trg_indices, dtype=torch.long)\n",
    "\n",
    "# Split Train/Test\n",
    "train_data, test_data = train_test_split(df_detox, test_size=0.1, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(DetoxDataset(train_data, vocab), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(DetoxDataset(test_data, vocab), batch_size=32, shuffle=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIO TAREA B: DETOXIFICACIÓN CON ATENCIÓN ===\n",
      "Dispositivo: cuda\n",
      "Total de pares tóxico-neutral: 400\n",
      "Tamaño del vocabulario: 1391\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "29ce6fa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:05:10.977325Z",
     "start_time": "2025-11-24T16:05:10.963206Z"
    }
   },
   "source": [
    "# --- 1. ENCODER ---\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        # Usamos GRU bidireccional para captar mejor el contexto\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hid_dim * 2, hid_dim) # Colapsar bidireccional a unidireccional\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src = [batch size, src len]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        \n",
    "        # outputs: [batch, src len, hid dim * 2] -> Contiene info de todos los tokens\n",
    "        # hidden: [2, batch, hid dim] -> Último estado oculto (forward y backward)\n",
    "        \n",
    "        # Concatenamos el hidden forward y backward para pasarle uno solo al decoder\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
    "        \n",
    "        return outputs, hidden\n",
    "\n",
    "# --- 2. CAPA DE ATENCIÓN ---\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        # Aprende a puntuar qué tan bien encajan el estado del decoder y el del encoder\n",
    "        self.attn = nn.Linear((hid_dim * 2) + hid_dim, hid_dim)\n",
    "        self.v = nn.Linear(hid_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: [batch, hid dim] (Estado actual del decoder)\n",
    "        # encoder_outputs: [batch, src len, hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        # Repetir hidden src_len veces para compararlo con cada output del encoder\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        # Calcular energía (scores)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        return torch.softmax(attention, dim=1) # Pesos que suman 1\n",
    "\n",
    "# --- 3. DECODER CON ATENCIÓN ---\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((hid_dim * 2) + emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear((hid_dim * 2) + hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # input = [batch size] (una palabra)\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        # Calcular pesos de atención (a)\n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        # Calcular contexto (suma ponderada de outputs del encoder)\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        # Input del GRU: Embedding de la palabra actual + Contexto de atención\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        # Predicción final\n",
    "        prediction = self.fc_out(torch.cat((output.squeeze(1), weighted.squeeze(1), embedded.squeeze(1)), dim=1))\n",
    "        return prediction, hidden.squeeze(0)\n",
    "\n",
    "# --- 4. MODELO SEQ2SEQ COMPLETO ---\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # Codificar\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        # Primera entrada <SOS>\n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            # Decodificar usando atención\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            outputs[:, t, :] = output\n",
    "            \n",
    "            top1 = output.argmax(1) \n",
    "            input = trg[:, t] if random.random() < teacher_forcing_ratio else top1\n",
    "            \n",
    "        return outputs"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "204f7080",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:05:38.097542Z",
     "start_time": "2025-11-24T16:05:11.026349Z"
    }
   },
   "source": [
    "# Hiperparámetros\n",
    "INPUT_DIM = vocab.n_words\n",
    "OUTPUT_DIM = vocab.n_words\n",
    "ENC_EMB_DIM = 128\n",
    "DEC_EMB_DIM = 128\n",
    "HID_DIM = 256\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "# Instanciar\n",
    "attn = Attention(HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT, attn)\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "# Inicializar pesos\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name: nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else: nn.init.constant_(param.data, 0)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0) # Ignorar <PAD>\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        \n",
    "        # Reshape\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "print(\"--- Entrenando Seq2Seq con Atención ---\")\n",
    "for epoch in range(20):\n",
    "    loss = train(model, train_loader, optimizer, criterion, 1)\n",
    "    if (epoch+1) % 2 == 0: # Imprimir cada 2 épocas\n",
    "        print(f'Epoch: {epoch+1} | Loss: {loss:.4f}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Entrenando Seq2Seq con Atención ---\n",
      "Epoch: 2 | Loss: 5.9880\n",
      "Epoch: 4 | Loss: 5.7142\n",
      "Epoch: 6 | Loss: 5.5119\n",
      "Epoch: 8 | Loss: 5.3271\n",
      "Epoch: 10 | Loss: 5.1623\n",
      "Epoch: 12 | Loss: 4.9644\n",
      "Epoch: 14 | Loss: 4.7005\n",
      "Epoch: 16 | Loss: 4.4468\n",
      "Epoch: 18 | Loss: 4.2270\n",
      "Epoch: 20 | Loss: 4.0427\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "28889a55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:05:41.585094Z",
     "start_time": "2025-11-24T16:05:41.501924Z"
    }
   },
   "source": [
    "def detoxify_sentence(sentence, model, vocab, device, max_len=25):\n",
    "    model.eval()\n",
    "    # Tokenizar (simple)\n",
    "    tokens = sentence.lower().split()\n",
    "    # Convertir a índices\n",
    "    indices = [vocab.word2index.get(t, 3) for t in tokens] # 3=UNK\n",
    "    indices.append(2) # EOS\n",
    "    \n",
    "    # Padding\n",
    "    if len(indices) < max_len: indices += [0] * (max_len - len(indices))\n",
    "    else: indices = indices[:max_len]\n",
    "    \n",
    "    # Tensores\n",
    "    src_tensor = torch.LongTensor(indices).unsqueeze(0).to(device)\n",
    "    # Dummy target (solo necesitamos el <SOS> inicial para empezar)\n",
    "    trg_tensor = torch.zeros((1, max_len), dtype=torch.long).to(device)\n",
    "    trg_tensor[0, 0] = 1 # SOS\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Forzamos teacher_forcing=0 para que genere libremente\n",
    "        output = model(src_tensor, trg_tensor, teacher_forcing_ratio=0)\n",
    "        \n",
    "        # Obtener tokens predichos\n",
    "        pred_token_ids = output.argmax(2).squeeze().tolist()\n",
    "    \n",
    "    # Convertir índices a palabras\n",
    "    pred_words = []\n",
    "    for idx in pred_token_ids:\n",
    "        if idx == 2: break # EOS\n",
    "        if idx not in [0, 1, 2, 3]: # Ignorar especiales en la salida\n",
    "            pred_words.append(vocab.index2word[idx])\n",
    "            \n",
    "    return \" \".join(pred_words)\n",
    "\n",
    "# --- PRUEBA FINAL ---\n",
    "test_sentences = [\n",
    "    \"you are a stupid idiot\",\n",
    "    \"shut up and listen to me\",\n",
    "    \"this is fucking garbage\",\n",
    "    \"women should not vote\"\n",
    "]\n",
    "\n",
    "print(\"\\n--- RESULTADOS DE DETOXIFICACIÓN ---\")\n",
    "for s in test_sentences:\n",
    "    res = detoxify_sentence(s, model, vocab, device)\n",
    "    print(f\"Tóxico:  {s}\")\n",
    "    print(f\"Neutral: {res}\")\n",
    "    print(\"-\" * 20)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESULTADOS DE DETOXIFICACIÓN ---\n",
      "Tóxico:  you are a stupid idiot\n",
      "Neutral: n't really good\n",
      "--------------------\n",
      "Tóxico:  shut up and listen to me\n",
      "Neutral: masculine get would away\n",
      "--------------------\n",
      "Tóxico:  this is fucking garbage\n",
      "Neutral: one\n",
      "--------------------\n",
      "Tóxico:  women should not vote\n",
      "Neutral: n't n't n't\n",
      "--------------------\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "3aff1b49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:05:41.842200Z",
     "start_time": "2025-11-24T16:05:41.701806Z"
    }
   },
   "source": [
    "frases_faciles = [\n",
    "    # --- GRUPO 1: Eliminación de \"Adverbios\" tóxicos ---\n",
    "    \"What the hell is this?\",\n",
    "    \"That is fucking wrong.\",\n",
    "    \n",
    "    # --- GRUPO 2: Insultos básicos (Tu modelo usará 'bad' o 'wrong') ---\n",
    "    \"You are stupid.\",\n",
    "    \"This idea is garbage.\",\n",
    "    \"He is a liar.\",\n",
    "    \n",
    "    # --- GRUPO 3: Órdenes agresivas ---\n",
    "    \"Shut your mouth.\",\n",
    "    \"Get out of here.\",\n",
    "    \n",
    "    # --- GRUPO 4: Odio simple ---\n",
    "    \"I hate this game.\",\n",
    "    \"You are ugly.\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== PRUEBA DE 'SANITY CHECK' (FRASES FÁCILES) ===\")\n",
    "for frase in frases_faciles:\n",
    "    traduccion = detoxify_sentence(frase, model, vocab, device)\n",
    "    print(f\"Tóxico:  {frase}\")\n",
    "    print(f\"Neutral: {traduccion}\")\n",
    "    print(\"-\" * 20)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PRUEBA DE 'SANITY CHECK' (FRASES FÁCILES) ===\n",
      "Tóxico:  What the hell is this?\n",
      "Neutral: n't really bad\n",
      "--------------------\n",
      "Tóxico:  That is fucking wrong.\n",
      "Neutral: one bad\n",
      "--------------------\n",
      "Tóxico:  You are stupid.\n",
      "Neutral: n't bad\n",
      "--------------------\n",
      "Tóxico:  This idea is garbage.\n",
      "Neutral: n't really n't\n",
      "--------------------\n",
      "Tóxico:  He is a liar.\n",
      "Neutral: n't really bad\n",
      "--------------------\n",
      "Tóxico:  Shut your mouth.\n",
      "Neutral: n't really\n",
      "--------------------\n",
      "Tóxico:  Get out of here.\n",
      "Neutral: n't really bad\n",
      "--------------------\n",
      "Tóxico:  I hate this game.\n",
      "Neutral: n't really bad\n",
      "--------------------\n",
      "Tóxico:  You are ugly.\n",
      "Neutral: n't bad\n",
      "--------------------\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
