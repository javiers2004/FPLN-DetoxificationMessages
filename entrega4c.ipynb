{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4db773c5108c54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T16:41:23.359381508Z",
     "start_time": "2026-01-11T16:41:20.945924102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TAREA EXTRA: ZERO-SHOT CLASSIFICATION CON PARADETOX ---\n",
      "Objetivo: Detectar toxicidad usando un CSV específico.\n",
      "\n",
      "Cargando modelo (esto puede tardar un poco)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando las primeras 5 filas del archivo 'datos/english_paradetox_test_set_preprocessed_non_lemmatized.csv'...\n",
      "\n",
      "Fila 0:\n",
      "Texto: 'mandated  and \" right fucking now \" would be good .'\n",
      "Ganador: threat (Confianza: 89.00%)\n",
      "------------------------------\n",
      "Fila 1:\n",
      "Texto: '&gt i wouldn 't care how ignorant you are in you weren 't pretending to know shit .'\n",
      "Ganador: threat (Confianza: 49.58%)\n",
      "------------------------------\n",
      "Fila 2:\n",
      "Texto: '6. are you fucking insane ?'\n",
      "Ganador: insult (Confianza: 33.75%)\n",
      "------------------------------\n",
      "Fila 3:\n",
      "Texto: 'after all its hard to get a job if your stupid .'\n",
      "Ganador: threat (Confianza: 43.37%)\n",
      "------------------------------\n",
      "Fila 4:\n",
      "Texto: 'after all most of these dorks are millionares and can pay their own way .'\n",
      "Ganador: threat (Confianza: 47.58%)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"--- TAREA EXTRA: ZERO-SHOT CLASSIFICATION CON PARADETOX ---\")\n",
    "print(\"Objetivo: Detectar toxicidad usando un CSV específico.\\n\")\n",
    "\n",
    "# 1. Configuración del archivo y columna\n",
    "archivo_csv = \"datos/english_paradetox_test_set_preprocessed_non_lemmatized.csv\"\n",
    "columna_texto = \"text\"  # La columna que solicitaste\n",
    "\n",
    "# 2. Cargamos el pipeline\n",
    "print(\"Cargando modelo (esto puede tardar un poco)...\")\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# 3. Etiquetas candidatas\n",
    "candidate_labels = [\"insult\", \"threat\", \"hate speech\", \"constructive criticism\", \"toxic\"]\n",
    "\n",
    "# 4. Carga y Procesamiento del CSV\n",
    "if os.path.exists(archivo_csv):\n",
    "    try:\n",
    "        # Leemos el CSV\n",
    "        df = pd.read_csv(archivo_csv)\n",
    "\n",
    "        if columna_texto not in df.columns:\n",
    "            raise ValueError(f\"La columna '{columna_texto}' no existe en el CSV. Las columnas encontradas son: {list(df.columns)}\")\n",
    "\n",
    "        # IMPORTANTE: Tomamos solo las primeras 5 filas para la prueba.\n",
    "        subset_df = df.head(5)\n",
    "\n",
    "        print(f\"\\nProcesando las primeras {len(subset_df)} filas del archivo '{archivo_csv}'...\\n\")\n",
    "\n",
    "        for index, row in subset_df.iterrows():\n",
    "            texto_actual = row[columna_texto]\n",
    "\n",
    "            # Verificación básica de que hay texto\n",
    "            if pd.isna(texto_actual) or str(texto_actual).strip() == \"\":\n",
    "                print(f\"Fila {index}: Texto vacío, saltando...\")\n",
    "                continue\n",
    "\n",
    "            # Clasificación\n",
    "            result = classifier(str(texto_actual), candidate_labels)\n",
    "\n",
    "            # Resultados\n",
    "            top_label = result['labels'][0]\n",
    "            top_score = result['scores'][0]\n",
    "\n",
    "            print(f\"Fila {index}:\")\n",
    "            print(f\"Texto: '{texto_actual}'\")\n",
    "            print(f\"Ganador: {top_label} (Confianza: {top_score:.2%})\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error al procesar el CSV: {e}\")\n",
    "else:\n",
    "    print(f\"ERROR: No encuentro el archivo '{archivo_csv}' en esta carpeta.\")\n",
    "    print(\"Asegúrate de subir el archivo al mismo directorio donde corre este script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677f3f71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T16:45:47.056961697Z",
     "start_time": "2026-01-11T16:45:46.072148109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CLASIFICACIÓN DE EMOCIONES (EJEMPLOS ALEATORIOS) ---\n",
      "Cargando modelo de emociones...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analizando 5 frases ALEATORIAS del dataset...\n",
      "\n",
      "Fila 569:\n",
      "Texto: 'Dont really like the liberals either, Too much lefty rubbish leaves us all poor.'\n",
      "Emoción predominante: SADNESS (Confianza: 0.44)\n",
      "------------------------------\n",
      "Fila 62:\n",
      "Texto: 'dont you ever change one of my edits , or i will come to your house , and i will cut you .'\n",
      "Emoción predominante: ANGER (Confianza: 0.77)\n",
      "------------------------------\n",
      "Fila 253:\n",
      "Texto: 'shit , i d love an number pm sunset .'\n",
      "Emoción predominante: DISGUST (Confianza: 0.25)\n",
      "------------------------------\n",
      "Fila 67:\n",
      "Texto: 'either way , how the fuck do you forget your child in the backseat .'\n",
      "Emoción predominante: ANGER (Confianza: 0.81)\n",
      "------------------------------\n",
      "Fila 281:\n",
      "Texto: 'that s a lot of work for something that shouldn t take any fucking work at all .'\n",
      "Emoción predominante: ANGER (Confianza: 0.79)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"--- CLASIFICACIÓN DE EMOCIONES (EJEMPLOS ALEATORIOS) ---\")\n",
    "\n",
    "# 1. Configuración\n",
    "archivo_csv = \"datos/english_paradetox_test_set_preprocessed_non_lemmatized.csv\"\n",
    "columna_texto = \"text\"\n",
    "cantidad_ejemplos = 5 \n",
    "\n",
    "# 2. Cargar pipeline de emociones\n",
    "print(\"Cargando modelo de emociones...\")\n",
    "classifier_emotion = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", top_k=None)\n",
    "\n",
    "# 3. Cargar y procesar datos\n",
    "if os.path.exists(archivo_csv):\n",
    "    try:\n",
    "        df = pd.read_csv(archivo_csv)\n",
    "\n",
    "        # Verificar columna\n",
    "        if columna_texto not in df.columns:\n",
    "            raise ValueError(f\"No existe la columna '{columna_texto}'\")\n",
    "\n",
    "        n_muestras = min(cantidad_ejemplos, len(df))\n",
    "        subset_df = df.sample(n=n_muestras)\n",
    "\n",
    "        print(f\"\\nAnalizando {n_muestras} frases ALEATORIAS del dataset...\\n\")\n",
    "\n",
    "        for index, row in subset_df.iterrows():\n",
    "            texto = row[columna_texto]\n",
    "\n",
    "            # Saltamos vacíos\n",
    "            if pd.isna(texto) or str(texto).strip() == \"\":\n",
    "                continue\n",
    "\n",
    "            # Clasificar\n",
    "            resultado = classifier_emotion(str(texto))\n",
    "\n",
    "            # Obtener la emoción ganadora (la primera de la lista ordenada)\n",
    "            emocion = resultado[0][0]['label']\n",
    "            confianza = resultado[0][0]['score']\n",
    "\n",
    "            print(f\"Fila {index}:\")\n",
    "            print(f\"Texto: '{texto}'\")\n",
    "            print(f\"Emoción predominante: {emocion.upper()} (Confianza: {confianza:.2f})\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(f\"No se encuentra el archivo: {archivo_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7993a531",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T16:47:37.930037895Z",
     "start_time": "2026-01-11T16:47:37.284428905Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\js834\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ANÁLISIS DE OBJETIVOS (NER) ---\n",
      "Objetivo: Identificar a QUÉ o QUIÉN se ataca (Personas, Organizaciones, Países).\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\js834\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase: 'I hate John because he is stupid'\n",
      "  └── Entidad detectada: John (Tipo: PER, Confianza: 1.00)\n",
      "------------------------------\n",
      "Frase: 'Google is the worst company ever'\n",
      "  └── Entidad detectada: Google (Tipo: ORG, Confianza: 1.00)\n",
      "------------------------------\n",
      "Frase: 'I will destroy Paris and all its people'\n",
      "  └── Entidad detectada: Paris (Tipo: LOC, Confianza: 1.00)\n",
      "------------------------------\n",
      "Frase: 'Americans are always so loud'\n",
      "  └── Entidad detectada: Americans (Tipo: MISC, Confianza: 1.00)\n",
      "------------------------------\n",
      "Frase: 'Kill Elon Musk right now'\n",
      "  └── Entidad detectada: El (Tipo: PER, Confianza: 0.92)\n",
      "  └── Entidad detectada: ##on Musk (Tipo: PER, Confianza: 0.86)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "print(\"--- ANÁLISIS DE OBJETIVOS (NER) ---\")\n",
    "print(\"Objetivo: Identificar a QUÉ o QUIÉN se ataca (Personas, Organizaciones, Países).\\n\")\n",
    "\n",
    "# 1. Cargar el pipeline de NER (Named Entity Recognition)\n",
    "# aggregation_strategy=\"simple\" es CLAVE: junta los tokens (\"J\", \"##ohn\") en palabras completas (\"John\")\n",
    "ner_classifier = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n",
    "\n",
    "# 2. Ejemplos con entidades claras\n",
    "ejemplos_ner = [\n",
    "    \"I hate John because he is stupid\",              # Ataque a Persona\n",
    "    \"Google is the worst company ever\",              # Ataque a Organización\n",
    "    \"I will destroy Paris and all its people\",       # Ataque a Lugar\n",
    "    \"Americans are always so loud\",                  # Ataque a Grupo (Gentilicio)\n",
    "    \"Kill Elon Musk right now\"                       # Amenaza directa a Persona\n",
    "]\n",
    "\n",
    "# 3. Procesar y mostrar resultados\n",
    "for texto in ejemplos_ner:\n",
    "    entidades = ner_classifier(texto)\n",
    "\n",
    "    print(f\"Frase: '{texto}'\")\n",
    "    if len(entidades) > 0:\n",
    "        for ent in entidades:\n",
    "            print(f\"  └── Entidad detectada: {ent['word']} (Tipo: {ent['entity_group']}, Confianza: {ent['score']:.2f})\")\n",
    "    else:\n",
    "        print(\"  └── No se detectaron nombres propios específicos.\")\n",
    "    print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
