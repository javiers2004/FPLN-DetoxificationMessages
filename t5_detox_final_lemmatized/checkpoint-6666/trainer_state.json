{
  "best_global_step": 3333,
  "best_metric": 1.0932188034057617,
  "best_model_checkpoint": "./t5_detox_final_lemmatized\\checkpoint-3333",
  "epoch": 6.0,
  "eval_steps": 500,
  "global_step": 6666,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.045004500450045004,
      "grad_norm": 4.22779655456543,
      "learning_rate": 0.00029937893789378937,
      "loss": 2.1595,
      "step": 50
    },
    {
      "epoch": 0.09000900090009001,
      "grad_norm": 7.407798767089844,
      "learning_rate": 0.0002987038703870387,
      "loss": 1.6743,
      "step": 100
    },
    {
      "epoch": 0.135013501350135,
      "grad_norm": 3.4701004028320312,
      "learning_rate": 0.000298028802880288,
      "loss": 1.5844,
      "step": 150
    },
    {
      "epoch": 0.18001800180018002,
      "grad_norm": 4.83553409576416,
      "learning_rate": 0.00029735373537353733,
      "loss": 1.5052,
      "step": 200
    },
    {
      "epoch": 0.22502250225022502,
      "grad_norm": 2.9473633766174316,
      "learning_rate": 0.00029667866786678665,
      "loss": 1.4251,
      "step": 250
    },
    {
      "epoch": 0.27002700270027,
      "grad_norm": 2.9381744861602783,
      "learning_rate": 0.00029600360036003597,
      "loss": 1.5434,
      "step": 300
    },
    {
      "epoch": 0.31503150315031503,
      "grad_norm": 2.991704225540161,
      "learning_rate": 0.0002953285328532853,
      "loss": 1.4239,
      "step": 350
    },
    {
      "epoch": 0.36003600360036003,
      "grad_norm": 2.782235860824585,
      "learning_rate": 0.0002946534653465346,
      "loss": 1.4642,
      "step": 400
    },
    {
      "epoch": 0.40504050405040504,
      "grad_norm": 3.901827573776245,
      "learning_rate": 0.0002939783978397839,
      "loss": 1.4101,
      "step": 450
    },
    {
      "epoch": 0.45004500450045004,
      "grad_norm": 4.861331462860107,
      "learning_rate": 0.0002933033303330333,
      "loss": 1.4546,
      "step": 500
    },
    {
      "epoch": 0.49504950495049505,
      "grad_norm": 3.1640422344207764,
      "learning_rate": 0.0002926282628262826,
      "loss": 1.397,
      "step": 550
    },
    {
      "epoch": 0.54005400540054,
      "grad_norm": 2.9586007595062256,
      "learning_rate": 0.00029195319531953194,
      "loss": 1.383,
      "step": 600
    },
    {
      "epoch": 0.585058505850585,
      "grad_norm": 3.2433526515960693,
      "learning_rate": 0.00029127812781278126,
      "loss": 1.2517,
      "step": 650
    },
    {
      "epoch": 0.6300630063006301,
      "grad_norm": 2.081737995147705,
      "learning_rate": 0.0002906030603060306,
      "loss": 1.353,
      "step": 700
    },
    {
      "epoch": 0.6750675067506751,
      "grad_norm": 1.9211229085922241,
      "learning_rate": 0.0002899279927992799,
      "loss": 1.3795,
      "step": 750
    },
    {
      "epoch": 0.7200720072007201,
      "grad_norm": 3.3233232498168945,
      "learning_rate": 0.0002892529252925292,
      "loss": 1.3169,
      "step": 800
    },
    {
      "epoch": 0.7650765076507651,
      "grad_norm": 2.1794209480285645,
      "learning_rate": 0.00028857785778577854,
      "loss": 1.3144,
      "step": 850
    },
    {
      "epoch": 0.8100810081008101,
      "grad_norm": 2.522399663925171,
      "learning_rate": 0.00028790279027902786,
      "loss": 1.3314,
      "step": 900
    },
    {
      "epoch": 0.8550855085508551,
      "grad_norm": 2.7546398639678955,
      "learning_rate": 0.0002872277227722772,
      "loss": 1.2226,
      "step": 950
    },
    {
      "epoch": 0.9000900090009001,
      "grad_norm": 2.740161895751953,
      "learning_rate": 0.00028655265526552655,
      "loss": 1.3323,
      "step": 1000
    },
    {
      "epoch": 0.9450945094509451,
      "grad_norm": 3.323164463043213,
      "learning_rate": 0.00028587758775877587,
      "loss": 1.3281,
      "step": 1050
    },
    {
      "epoch": 0.9900990099009901,
      "grad_norm": 2.8762054443359375,
      "learning_rate": 0.0002852025202520252,
      "loss": 1.2592,
      "step": 1100
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.162684679031372,
      "eval_runtime": 17.0214,
      "eval_samples_per_second": 116.031,
      "eval_steps_per_second": 14.511,
      "step": 1111
    },
    {
      "epoch": 1.035103510351035,
      "grad_norm": 2.227172613143921,
      "learning_rate": 0.0002845409540954095,
      "loss": 1.1924,
      "step": 1150
    },
    {
      "epoch": 1.08010801080108,
      "grad_norm": 2.4418575763702393,
      "learning_rate": 0.00028386588658865883,
      "loss": 1.1925,
      "step": 1200
    },
    {
      "epoch": 1.125112511251125,
      "grad_norm": 2.806009531021118,
      "learning_rate": 0.00028319081908190815,
      "loss": 1.1995,
      "step": 1250
    },
    {
      "epoch": 1.17011701170117,
      "grad_norm": 1.624711275100708,
      "learning_rate": 0.00028251575157515747,
      "loss": 1.2282,
      "step": 1300
    },
    {
      "epoch": 1.215121512151215,
      "grad_norm": 1.3886096477508545,
      "learning_rate": 0.00028184068406840684,
      "loss": 1.182,
      "step": 1350
    },
    {
      "epoch": 1.2601260126012601,
      "grad_norm": 1.825409173965454,
      "learning_rate": 0.00028116561656165616,
      "loss": 1.1966,
      "step": 1400
    },
    {
      "epoch": 1.3051305130513051,
      "grad_norm": 1.8563785552978516,
      "learning_rate": 0.0002804905490549055,
      "loss": 1.1828,
      "step": 1450
    },
    {
      "epoch": 1.3501350135013501,
      "grad_norm": 2.1386327743530273,
      "learning_rate": 0.0002798154815481548,
      "loss": 1.2048,
      "step": 1500
    },
    {
      "epoch": 1.3951395139513951,
      "grad_norm": 3.7054951190948486,
      "learning_rate": 0.0002791404140414041,
      "loss": 1.2013,
      "step": 1550
    },
    {
      "epoch": 1.4401440144014401,
      "grad_norm": 2.141422748565674,
      "learning_rate": 0.00027846534653465344,
      "loss": 1.2207,
      "step": 1600
    },
    {
      "epoch": 1.4851485148514851,
      "grad_norm": 3.362666130065918,
      "learning_rate": 0.00027779027902790276,
      "loss": 1.1077,
      "step": 1650
    },
    {
      "epoch": 1.5301530153015301,
      "grad_norm": 2.403219223022461,
      "learning_rate": 0.0002771152115211521,
      "loss": 1.172,
      "step": 1700
    },
    {
      "epoch": 1.5751575157515751,
      "grad_norm": 2.3860764503479004,
      "learning_rate": 0.0002764401440144014,
      "loss": 1.2222,
      "step": 1750
    },
    {
      "epoch": 1.6201620162016201,
      "grad_norm": 2.1680209636688232,
      "learning_rate": 0.0002757650765076507,
      "loss": 1.1696,
      "step": 1800
    },
    {
      "epoch": 1.6651665166516652,
      "grad_norm": 1.7584130764007568,
      "learning_rate": 0.0002750900090009001,
      "loss": 1.0923,
      "step": 1850
    },
    {
      "epoch": 1.7101710171017102,
      "grad_norm": 2.8453662395477295,
      "learning_rate": 0.0002744149414941494,
      "loss": 1.116,
      "step": 1900
    },
    {
      "epoch": 1.7551755175517552,
      "grad_norm": 1.7271430492401123,
      "learning_rate": 0.00027373987398739873,
      "loss": 1.116,
      "step": 1950
    },
    {
      "epoch": 1.8001800180018002,
      "grad_norm": 2.5790958404541016,
      "learning_rate": 0.00027306480648064805,
      "loss": 1.105,
      "step": 2000
    },
    {
      "epoch": 1.8451845184518452,
      "grad_norm": 1.976914644241333,
      "learning_rate": 0.00027238973897389737,
      "loss": 1.1561,
      "step": 2050
    },
    {
      "epoch": 1.8901890189018902,
      "grad_norm": 2.4423511028289795,
      "learning_rate": 0.0002717146714671467,
      "loss": 1.1488,
      "step": 2100
    },
    {
      "epoch": 1.9351935193519352,
      "grad_norm": 2.1403427124023438,
      "learning_rate": 0.000271039603960396,
      "loss": 1.161,
      "step": 2150
    },
    {
      "epoch": 1.9801980198019802,
      "grad_norm": 2.142746925354004,
      "learning_rate": 0.00027036453645364533,
      "loss": 1.1165,
      "step": 2200
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.1146275997161865,
      "eval_runtime": 19.8437,
      "eval_samples_per_second": 99.528,
      "eval_steps_per_second": 12.447,
      "step": 2222
    },
    {
      "epoch": 2.025202520252025,
      "grad_norm": 1.5345548391342163,
      "learning_rate": 0.0002697029702970297,
      "loss": 1.0724,
      "step": 2250
    },
    {
      "epoch": 2.07020702070207,
      "grad_norm": 2.8992397785186768,
      "learning_rate": 0.00026902790279027903,
      "loss": 1.0789,
      "step": 2300
    },
    {
      "epoch": 2.115211521152115,
      "grad_norm": 3.9025959968566895,
      "learning_rate": 0.00026835283528352835,
      "loss": 0.9988,
      "step": 2350
    },
    {
      "epoch": 2.16021602160216,
      "grad_norm": 2.3023388385772705,
      "learning_rate": 0.00026767776777677767,
      "loss": 1.0398,
      "step": 2400
    },
    {
      "epoch": 2.205220522052205,
      "grad_norm": 2.6235239505767822,
      "learning_rate": 0.000267002700270027,
      "loss": 1.0991,
      "step": 2450
    },
    {
      "epoch": 2.25022502250225,
      "grad_norm": 2.3599252700805664,
      "learning_rate": 0.0002663276327632763,
      "loss": 0.9874,
      "step": 2500
    },
    {
      "epoch": 2.295229522952295,
      "grad_norm": 2.7450270652770996,
      "learning_rate": 0.0002656525652565256,
      "loss": 1.0723,
      "step": 2550
    },
    {
      "epoch": 2.34023402340234,
      "grad_norm": 2.1953232288360596,
      "learning_rate": 0.00026497749774977494,
      "loss": 0.9815,
      "step": 2600
    },
    {
      "epoch": 2.385238523852385,
      "grad_norm": 2.1155526638031006,
      "learning_rate": 0.00026430243024302426,
      "loss": 1.0979,
      "step": 2650
    },
    {
      "epoch": 2.43024302430243,
      "grad_norm": 1.9900481700897217,
      "learning_rate": 0.00026362736273627364,
      "loss": 1.0143,
      "step": 2700
    },
    {
      "epoch": 2.4752475247524752,
      "grad_norm": 2.2242507934570312,
      "learning_rate": 0.00026295229522952296,
      "loss": 1.0548,
      "step": 2750
    },
    {
      "epoch": 2.5202520252025202,
      "grad_norm": 1.681702971458435,
      "learning_rate": 0.0002622772277227723,
      "loss": 1.0455,
      "step": 2800
    },
    {
      "epoch": 2.5652565256525652,
      "grad_norm": 1.4635658264160156,
      "learning_rate": 0.0002616021602160216,
      "loss": 1.0838,
      "step": 2850
    },
    {
      "epoch": 2.6102610261026102,
      "grad_norm": 1.9666850566864014,
      "learning_rate": 0.0002609270927092709,
      "loss": 1.0051,
      "step": 2900
    },
    {
      "epoch": 2.6552655265526552,
      "grad_norm": 2.4581809043884277,
      "learning_rate": 0.00026025202520252023,
      "loss": 0.9836,
      "step": 2950
    },
    {
      "epoch": 2.7002700270027002,
      "grad_norm": 2.2639222145080566,
      "learning_rate": 0.00025957695769576955,
      "loss": 1.0513,
      "step": 3000
    },
    {
      "epoch": 2.7452745274527453,
      "grad_norm": 1.763593316078186,
      "learning_rate": 0.0002589018901890189,
      "loss": 1.0296,
      "step": 3050
    },
    {
      "epoch": 2.7902790279027903,
      "grad_norm": 2.311603307723999,
      "learning_rate": 0.0002582268226822682,
      "loss": 1.0348,
      "step": 3100
    },
    {
      "epoch": 2.8352835283528353,
      "grad_norm": 1.909830093383789,
      "learning_rate": 0.0002575517551755175,
      "loss": 1.0634,
      "step": 3150
    },
    {
      "epoch": 2.8802880288028803,
      "grad_norm": 1.5871962308883667,
      "learning_rate": 0.0002568766876687669,
      "loss": 1.0913,
      "step": 3200
    },
    {
      "epoch": 2.9252925292529253,
      "grad_norm": 2.114760160446167,
      "learning_rate": 0.0002562016201620162,
      "loss": 1.0569,
      "step": 3250
    },
    {
      "epoch": 2.9702970297029703,
      "grad_norm": 1.7258362770080566,
      "learning_rate": 0.0002555265526552655,
      "loss": 1.0123,
      "step": 3300
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.0932188034057617,
      "eval_runtime": 17.7983,
      "eval_samples_per_second": 110.965,
      "eval_steps_per_second": 13.878,
      "step": 3333
    },
    {
      "epoch": 3.0153015301530153,
      "grad_norm": 1.8029067516326904,
      "learning_rate": 0.00025485148514851484,
      "loss": 1.0354,
      "step": 3350
    },
    {
      "epoch": 3.0603060306030603,
      "grad_norm": 1.6881794929504395,
      "learning_rate": 0.00025417641764176416,
      "loss": 0.8996,
      "step": 3400
    },
    {
      "epoch": 3.1053105310531053,
      "grad_norm": 1.5808275938034058,
      "learning_rate": 0.0002535013501350135,
      "loss": 0.9826,
      "step": 3450
    },
    {
      "epoch": 3.1503150315031503,
      "grad_norm": 1.3413804769515991,
      "learning_rate": 0.0002528262826282628,
      "loss": 0.9102,
      "step": 3500
    },
    {
      "epoch": 3.1953195319531953,
      "grad_norm": 1.760858178138733,
      "learning_rate": 0.0002521512151215121,
      "loss": 0.9238,
      "step": 3550
    },
    {
      "epoch": 3.2403240324032403,
      "grad_norm": 1.883609414100647,
      "learning_rate": 0.00025147614761476144,
      "loss": 0.9345,
      "step": 3600
    },
    {
      "epoch": 3.2853285328532853,
      "grad_norm": 1.4730490446090698,
      "learning_rate": 0.00025080108010801076,
      "loss": 0.918,
      "step": 3650
    },
    {
      "epoch": 3.3303330333033303,
      "grad_norm": 2.204347848892212,
      "learning_rate": 0.00025012601260126013,
      "loss": 0.9149,
      "step": 3700
    },
    {
      "epoch": 3.3753375337533753,
      "grad_norm": 2.3538479804992676,
      "learning_rate": 0.00024945094509450945,
      "loss": 0.9476,
      "step": 3750
    },
    {
      "epoch": 3.4203420342034203,
      "grad_norm": 2.4431397914886475,
      "learning_rate": 0.00024877587758775877,
      "loss": 0.9085,
      "step": 3800
    },
    {
      "epoch": 3.4653465346534653,
      "grad_norm": 1.850260615348816,
      "learning_rate": 0.0002481008100810081,
      "loss": 0.9917,
      "step": 3850
    },
    {
      "epoch": 3.5103510351035103,
      "grad_norm": 3.1103177070617676,
      "learning_rate": 0.0002474257425742574,
      "loss": 0.9718,
      "step": 3900
    },
    {
      "epoch": 3.5553555355535553,
      "grad_norm": 1.8415731191635132,
      "learning_rate": 0.00024675067506750673,
      "loss": 0.9347,
      "step": 3950
    },
    {
      "epoch": 3.6003600360036003,
      "grad_norm": 2.244765520095825,
      "learning_rate": 0.00024607560756075605,
      "loss": 0.9498,
      "step": 4000
    },
    {
      "epoch": 3.6453645364536453,
      "grad_norm": 1.9126873016357422,
      "learning_rate": 0.00024540054005400537,
      "loss": 1.045,
      "step": 4050
    },
    {
      "epoch": 3.6903690369036903,
      "grad_norm": 2.046250581741333,
      "learning_rate": 0.0002447254725472547,
      "loss": 0.9491,
      "step": 4100
    },
    {
      "epoch": 3.7353735373537353,
      "grad_norm": 1.8414621353149414,
      "learning_rate": 0.000244050405040504,
      "loss": 0.9862,
      "step": 4150
    },
    {
      "epoch": 3.7803780378037803,
      "grad_norm": 1.856997013092041,
      "learning_rate": 0.00024337533753375335,
      "loss": 0.9727,
      "step": 4200
    },
    {
      "epoch": 3.8253825382538253,
      "grad_norm": 2.0284268856048584,
      "learning_rate": 0.00024270027002700267,
      "loss": 0.9349,
      "step": 4250
    },
    {
      "epoch": 3.8703870387038704,
      "grad_norm": 2.0827462673187256,
      "learning_rate": 0.000242025202520252,
      "loss": 0.9328,
      "step": 4300
    },
    {
      "epoch": 3.9153915391539154,
      "grad_norm": 2.533712863922119,
      "learning_rate": 0.0002413501350135013,
      "loss": 0.9089,
      "step": 4350
    },
    {
      "epoch": 3.9603960396039604,
      "grad_norm": 1.9662569761276245,
      "learning_rate": 0.00024067506750675063,
      "loss": 1.0228,
      "step": 4400
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.109764575958252,
      "eval_runtime": 18.4243,
      "eval_samples_per_second": 107.195,
      "eval_steps_per_second": 13.406,
      "step": 4444
    },
    {
      "epoch": 4.005400540054006,
      "grad_norm": 1.8997981548309326,
      "learning_rate": 0.00023999999999999998,
      "loss": 0.9618,
      "step": 4450
    },
    {
      "epoch": 4.05040504050405,
      "grad_norm": 2.149925708770752,
      "learning_rate": 0.0002393249324932493,
      "loss": 0.8491,
      "step": 4500
    },
    {
      "epoch": 4.095409540954096,
      "grad_norm": 2.0053632259368896,
      "learning_rate": 0.00023864986498649862,
      "loss": 0.846,
      "step": 4550
    },
    {
      "epoch": 4.14041404140414,
      "grad_norm": 2.8919947147369385,
      "learning_rate": 0.00023797479747974794,
      "loss": 0.8902,
      "step": 4600
    },
    {
      "epoch": 4.185418541854186,
      "grad_norm": 1.5749467611312866,
      "learning_rate": 0.00023729972997299726,
      "loss": 0.8797,
      "step": 4650
    },
    {
      "epoch": 4.23042304230423,
      "grad_norm": 1.4524136781692505,
      "learning_rate": 0.00023662466246624663,
      "loss": 0.8643,
      "step": 4700
    },
    {
      "epoch": 4.275427542754276,
      "grad_norm": 1.6303606033325195,
      "learning_rate": 0.00023594959495949592,
      "loss": 0.8836,
      "step": 4750
    },
    {
      "epoch": 4.32043204320432,
      "grad_norm": 1.4881571531295776,
      "learning_rate": 0.00023527452745274524,
      "loss": 0.8894,
      "step": 4800
    },
    {
      "epoch": 4.365436543654366,
      "grad_norm": 1.6643109321594238,
      "learning_rate": 0.00023459945994599456,
      "loss": 0.8198,
      "step": 4850
    },
    {
      "epoch": 4.41044104410441,
      "grad_norm": 2.300196886062622,
      "learning_rate": 0.00023392439243924388,
      "loss": 0.9554,
      "step": 4900
    },
    {
      "epoch": 4.455445544554456,
      "grad_norm": 2.159531593322754,
      "learning_rate": 0.00023324932493249325,
      "loss": 0.8689,
      "step": 4950
    },
    {
      "epoch": 4.5004500450045,
      "grad_norm": 2.024304151535034,
      "learning_rate": 0.00023257425742574257,
      "loss": 0.8629,
      "step": 5000
    },
    {
      "epoch": 4.545454545454545,
      "grad_norm": 1.9307622909545898,
      "learning_rate": 0.00023189918991899187,
      "loss": 0.9134,
      "step": 5050
    },
    {
      "epoch": 4.59045904590459,
      "grad_norm": 1.7660043239593506,
      "learning_rate": 0.00023122412241224119,
      "loss": 0.8754,
      "step": 5100
    },
    {
      "epoch": 4.635463546354636,
      "grad_norm": 1.9499094486236572,
      "learning_rate": 0.0002305490549054905,
      "loss": 0.8818,
      "step": 5150
    },
    {
      "epoch": 4.68046804680468,
      "grad_norm": 2.1171376705169678,
      "learning_rate": 0.00022987398739873988,
      "loss": 0.7599,
      "step": 5200
    },
    {
      "epoch": 4.725472547254725,
      "grad_norm": 3.176443099975586,
      "learning_rate": 0.0002291989198919892,
      "loss": 0.8984,
      "step": 5250
    },
    {
      "epoch": 4.77047704770477,
      "grad_norm": 2.4072389602661133,
      "learning_rate": 0.00022852385238523852,
      "loss": 0.8756,
      "step": 5300
    },
    {
      "epoch": 4.815481548154816,
      "grad_norm": 2.237421751022339,
      "learning_rate": 0.0002278487848784878,
      "loss": 0.9111,
      "step": 5350
    },
    {
      "epoch": 4.86048604860486,
      "grad_norm": 1.6630072593688965,
      "learning_rate": 0.00022717371737173713,
      "loss": 0.8877,
      "step": 5400
    },
    {
      "epoch": 4.905490549054905,
      "grad_norm": 2.0838875770568848,
      "learning_rate": 0.0002264986498649865,
      "loss": 0.8554,
      "step": 5450
    },
    {
      "epoch": 4.9504950495049505,
      "grad_norm": 1.6119924783706665,
      "learning_rate": 0.00022582358235823582,
      "loss": 0.9312,
      "step": 5500
    },
    {
      "epoch": 4.995499549954996,
      "grad_norm": 1.941253662109375,
      "learning_rate": 0.00022514851485148514,
      "loss": 0.9042,
      "step": 5550
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.1405024528503418,
      "eval_runtime": 21.5264,
      "eval_samples_per_second": 91.748,
      "eval_steps_per_second": 11.474,
      "step": 5555
    },
    {
      "epoch": 5.0405040504050405,
      "grad_norm": 1.6557526588439941,
      "learning_rate": 0.00022448694869486947,
      "loss": 0.7899,
      "step": 5600
    },
    {
      "epoch": 5.085508550855086,
      "grad_norm": 2.2657580375671387,
      "learning_rate": 0.00022381188118811879,
      "loss": 0.8221,
      "step": 5650
    },
    {
      "epoch": 5.1305130513051305,
      "grad_norm": 2.178126335144043,
      "learning_rate": 0.0002231368136813681,
      "loss": 0.8078,
      "step": 5700
    },
    {
      "epoch": 5.175517551755176,
      "grad_norm": 2.6863369941711426,
      "learning_rate": 0.00022246174617461743,
      "loss": 0.7642,
      "step": 5750
    },
    {
      "epoch": 5.2205220522052205,
      "grad_norm": 1.7328277826309204,
      "learning_rate": 0.00022178667866786677,
      "loss": 0.849,
      "step": 5800
    },
    {
      "epoch": 5.265526552655266,
      "grad_norm": 1.9479116201400757,
      "learning_rate": 0.0002211116111611161,
      "loss": 0.7653,
      "step": 5850
    },
    {
      "epoch": 5.3105310531053105,
      "grad_norm": 1.9429197311401367,
      "learning_rate": 0.0002204365436543654,
      "loss": 0.9184,
      "step": 5900
    },
    {
      "epoch": 5.355535553555356,
      "grad_norm": 2.261141777038574,
      "learning_rate": 0.00021976147614761473,
      "loss": 0.7954,
      "step": 5950
    },
    {
      "epoch": 5.4005400540054005,
      "grad_norm": 1.5438563823699951,
      "learning_rate": 0.00021908640864086405,
      "loss": 0.7522,
      "step": 6000
    },
    {
      "epoch": 5.445544554455446,
      "grad_norm": 2.6697168350219727,
      "learning_rate": 0.0002184113411341134,
      "loss": 0.802,
      "step": 6050
    },
    {
      "epoch": 5.4905490549054905,
      "grad_norm": 1.7634353637695312,
      "learning_rate": 0.00021773627362736272,
      "loss": 0.8197,
      "step": 6100
    },
    {
      "epoch": 5.535553555355536,
      "grad_norm": 2.576089859008789,
      "learning_rate": 0.00021706120612061203,
      "loss": 0.7943,
      "step": 6150
    },
    {
      "epoch": 5.5805580558055805,
      "grad_norm": 2.0108094215393066,
      "learning_rate": 0.00021638613861386135,
      "loss": 0.7867,
      "step": 6200
    },
    {
      "epoch": 5.625562556255625,
      "grad_norm": 2.243553876876831,
      "learning_rate": 0.00021571107110711067,
      "loss": 0.8277,
      "step": 6250
    },
    {
      "epoch": 5.6705670567056705,
      "grad_norm": 2.4239232540130615,
      "learning_rate": 0.00021503600360036002,
      "loss": 0.8862,
      "step": 6300
    },
    {
      "epoch": 5.715571557155716,
      "grad_norm": 2.723362922668457,
      "learning_rate": 0.00021436093609360934,
      "loss": 0.8073,
      "step": 6350
    },
    {
      "epoch": 5.7605760576057605,
      "grad_norm": 2.644684314727783,
      "learning_rate": 0.00021368586858685866,
      "loss": 0.8696,
      "step": 6400
    },
    {
      "epoch": 5.805580558055805,
      "grad_norm": 2.234663963317871,
      "learning_rate": 0.00021301080108010798,
      "loss": 0.8088,
      "step": 6450
    },
    {
      "epoch": 5.8505850585058505,
      "grad_norm": 2.6431376934051514,
      "learning_rate": 0.0002123357335733573,
      "loss": 0.7957,
      "step": 6500
    },
    {
      "epoch": 5.895589558955896,
      "grad_norm": 2.1601755619049072,
      "learning_rate": 0.00021166066606660664,
      "loss": 0.8049,
      "step": 6550
    },
    {
      "epoch": 5.9405940594059405,
      "grad_norm": 3.6003150939941406,
      "learning_rate": 0.00021098559855985596,
      "loss": 0.8193,
      "step": 6600
    },
    {
      "epoch": 5.985598559855985,
      "grad_norm": 2.346968412399292,
      "learning_rate": 0.00021031053105310528,
      "loss": 0.8644,
      "step": 6650
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.1502680778503418,
      "eval_runtime": 23.1201,
      "eval_samples_per_second": 85.423,
      "eval_steps_per_second": 10.683,
      "step": 6666
    }
  ],
  "logging_steps": 50,
  "max_steps": 22220,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 3,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 3
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 586761691987968.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
